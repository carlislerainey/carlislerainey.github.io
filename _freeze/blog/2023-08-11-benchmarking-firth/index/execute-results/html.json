{
  "hash": "d540f82290d3134cffbc7989f7bea17f",
  "result": {
    "markdown": "---\ntitle: \"Benchmarking Firth's Logit: {brglm2} versus {logistf}\"\nsubtitle: \"The M2 Is Really Fast\"\nauthor: \"Carlisle Rainey\"\ndate: \"2023-08-11\"\ncategories: [logistic regression, small samples, Firth, computing, R]\ndescription: \"In this post, I benchmark the brglm2 and logistf packages for fitting logistic regression models with Firth's penalty\"\ntoc: false\ncode-annotations: hover\ndraft: false\nbibliography: references.bib\n---\n\n\n## Firth's Logit\n\nI like Firth's logistic regression model [@firth1993]. I talk about that in @rainey2021 and [this Twitter thread](https://twitter.com/carlislerainey/status/1686389777225113601). @kosmidis2021 offer an excellent, recent follow-up, as well.\n\nI'll refer you to the papers for a careful discussion of the benefits, but Firth's penalty reduces the bias *and variance* of the logit coefficients.\n\n## Goals for Benchmarking\n\nIn this post, I want to do two things:\n\n1.  Compare the brglm2 and logistf packages. Which fits logistic regression models with Firth's penalty the fastest?\n2.  Comparean older Intel Mac with a newer M2. How much faster is the M2, if at all.\n\nModern computers fit these models really fast, but large simulations require us to pay attention to marginal speed increases. Here, I benchmark the two packages for fitting logistic regression models with Firth's penalty in a *small sample*--the results might not generalize to a larger sample. The data set comes from @weisiger2014 (see `?crdata::weisiger2014`). It has only 35 observations.\n\nYou can find the benchmarking code as a [GitHub Gist](https://gist.github.com/carlislerainey/8bf23a322252bead64d0a07391f7383d).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install crdata package to egt weisiger2014 data set\nremotes::install_github(\"carlislerainey/crdata\")\n\n# load packages\nlibrary(tidyverse)\nlibrary(brglm2)\nlibrary(logistf)\nlibrary(microbenchmark)\n\n# load data\nweis <- crdata::weisiger2014\n\n# rescale weisiger2014 explanatory variables using arm::rescale()\nrs_weis <- weis %>%\n  mutate(across(polity_conq:coord, arm::rescale)) \n\n# create functions to fit models\nf <- resist ~ polity_conq + lndist + terrain + soldperterr + gdppc2 + coord\nf1 <- function() {\n  glm(f, data = rs_weis, family = \"binomial\")\n}\nf2 <- function() {\n  glm(f, data = rs_weis, family = \"binomial\", method = brglmFit)\n}\nf3 <- function() {\n  logistf(f, data = rs_weis)\n}\n\n# do benchmarking\nmicrobenchmark(\"regular glm()\" = f1(), \n                    \"brglm2\" = f2(), \n                    \"logistf\" = f3(), \n                    times = 100)\n```\n:::\n\n\n## 2018 Intel iMac\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsystem(\"sysctl -n machdep.cpu.brand_string\", intern = TRUE)\n```\n:::\n\n\n```         \n[1] \"Intel(R) Core(TM) i7-7700K CPU @ 4.20GHz\"\n```\n\nFor the older Intel Mac, the brglm2 package takes about 4.8 times as long as the vanilla `glm()` logit and the logistf package takes about 5.8 times as long.\n\nMost importantly, for our purposes, the logistf package takes about 20% longer. That means six hours rather than five for a large batch of simulations.\n\n```         \nUnit: milliseconds\n          expr      min       lq     mean   median       uq      max neval\n regular glm() 1.117625 1.194022 1.827576 1.256566 1.331332 18.95785   100\n        brglm2 5.461556 5.833547 6.333050 6.012908 6.503606 22.78004   100\n       logistf 6.544562 6.901007 7.577122 7.208252 7.661631 22.74872   100\n```\n\n## M2 Max Macbook Pro\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsystem(\"sysctl -n machdep.cpu.brand_string\", intern = TRUE)\n```\n:::\n\n\n```         \n[1] \"Apple M2 Max\"\n```\n\nPerhaps most surpisingly, the M2 is a lot faster.\n\n-   For the vanilla `glm()`, the simulations take only 43% as long.\n-   For brglm2, the simulations take only 42% as long.\n-   For logistf, the simulations take only 47% as long.\n\nThese are big, big speedups.\n\n```    \nUnit: milliseconds\n          expr      min       lq      mean    median       uq       max neval\n regular glm() 0.505489 0.537510 0.5577767 0.5518395 0.566948  0.688431   100\n        brglm2 2.405265 2.479803 2.7853514 2.5521680 2.643926 13.737542   100\n       logistf 3.249045 3.351955 3.5532765 3.4383625 3.533564 13.867061   100\n```\n\n# Summary\n\n-   logistf is about 20% slower than brglm2 on the Intel iMac and about 34% slower on the M2.\n-   The M2 is more than twice as fast as my old Intel iMac.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}