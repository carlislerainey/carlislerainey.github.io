{
  "hash": "2e42532dcb05da97a8a7f80944a52322",
  "result": {
    "markdown": "---\ntitle: \"Statistical Power from Pilot Data\"\nauthor: \"Carlisle Rainey\"\ndate: \"2024-06-03\"\ncategories: [statistical power, hypothesis tests, power analysis, methodology]\ndescription: \"In this post, I discuss how pilot data can be used to predict the standard error (but should not be used to estimate the treatment effect).\"\nreference-location: margin\ntoc: false\ntwitter-card:\n  card-style: summary_large_image\n  image: \"twitter-card.png\"\nopen-graph:   \n  image: \"twitter-card.png\"\nbibliography: references.bib\ndraft: true\n---\n\n\n\n\n-----\n\nWe can think of statistical power as determined by the ratio $\\frac{\\tau}{SE}$, where $\\tau$ is the treatment effect and SE is the standard error of the estimate.[^1] To reason about statistical power, one needs to make assumptions or predictions about the treatment effect and the standard error.\n\n[^1]: @bloom1995 has a really beautiful paper on this idea. It's one of my favorites.\n\nIn this post, I discuss ways that pilot data should and should *not* be used as part of a power analysis. I make two points:\n\n1.  Pilot data are not usually useful to estimate the treatment effect.\n2.  Pilot data can be useful to predict the standard error.\n\nWith a predicted standard error in hand, we can obtain a prediction for the minimum detectable effect, the statistical power, or the required sample size.\n\n## Pilot data should not usually be used to estimate the treatment effect\n\nTo compute statistical power, researchers need to make an assumption about the size of the treatment effect. It's easy to feel lost without any guidance on what effects are reasonable to look for, so we might feel tempted to use a small pilot study to estimate the treatment effect and then use that estimate in our power analysis. This is a bad idea because the estimate of the treatment effect from a pilot study is too uncertain for a power analysis.[^2] @leon2011 and @albers2018 discuss this problem in more detail.[^3]\n\n[^2]: The estimate of the treatment effect from a well-powered study is too noisy as well.\n\n[^3]: @perugini2014 offer a potential solution if it's important to estimate the treatment effect from pilot data, though their approach is data-hungry and very conservative.\n\n::: callout-warning\nDo not use a small pilot study to estimate the treatment effect and then use that estimate in a power analysis.\n:::\n\n## Pilot data *can* be used to predict the standard error\n\nWhile pilot data might not be useful for estimating the treatment effect, **pilot data are useful for estimating the standard error of the planned study.** Given that power is a function of the ratio of the treatment effect and the standard error, it's important to have a good prediction of the standard error. Further, the noisiness of this estimated standard error is predictable, so it's easy to nudge the estimate slightly to obtain a *conservative* prediction.\n\nIn political science, it's common to run pilot studies with, say, 100-200 respondents before a full-sized study of, say, 1,000 respondents. It can be very helpful to use these pilot data to confirm any preliminary power calculations.\n\nHere are two helpful rules:\n\n2.  We can use pilot data to predict the standard error of the estimated treatment effect in a planned study. Conservatively, the standard error will be about $\\sqrt{\\frac{n^{pilot}}{n^{planned}}}\\ \\left\\lbrack \\left( \\sqrt{\\frac{1}{n^{pilot}}} + 1 \\right) \\cdot {\\widehat{SE}}_{\\widehat{\\tau}}^{pilot} \\right\\rbrack$, where $n^{pilot}$ is the number of respondents per condition in the pilot data, $SE_{\\widehat{\\tau}}^{pilot}$ is the estimated standard error using the pilot data, and $n^{planned}$ is the number of respondents per condition in the planned study.\n3.  We can use pilot data to conservatively predict the sample size we will need in a planned study. For 80% power to detect the treatment effect $\\widetilde{\\tau}$, we will (conservatively) need about $n^{pilot} \\cdot \\left\\lbrack \\frac{2.5}{\\widetilde{\\tau}} \\cdot \\left( \\sqrt{\\frac{1}{n^{pilot}}} + 1 \\right) \\cdot {\\widehat{SE}}_{\\widehat{\\tau}}^{pilot} \\right\\rbrack^{2}$ respondents per condition, where $n^{pilot}$ is the number of respondents per condition in the pilot data and $SE_{\\widehat{\\tau}}^{pilot}$ is the estimated standard error using the pilot data.\n\nNote that the factor $\\sqrt{\\frac{1}{n^{pilot}}}$ nudges the predicted standard error in a conservative direction. See [this working paper](https://github.com/carlislerainey/power-rules/blob/main/power-rules.pdf) for more details.\n\nWe can use the predicted standard error to find the minimum detectable effect (for 80% power) or the power (for a given treatment effect).  Or we can use the pilot data to estimate the required sample size (for 80% power to detect a given treatment effect.)\n\n## An illustrative simulation\n\nHere's a simulation to illustrate the idea.\n\n### The setting\n\nLet's imagine a setting where a study with 1,000 respondents has 80% power to detect an average treatment effect of 1 unit. I'm imagining that we're using linear regression with robust standard errors to test the hypothesis that the average treatment effect is positive (aka Welch's *t*-test). There's just one treatment group and one control group with 500 respondents each, for 1,000 respondents total.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set the treatment effect, SE, and sample size\ntau <- 1  # treatment effect\nse  <- tau/(qnorm(0.95) + qnorm(0.80))  # standard error for 80% power\nn_planned <- 500  # sample size per condition in planned full study\n\n# calculate required standard deviation to yield 80% power\nsigma <- se*sqrt(2*n_planned)/2\n```\n:::\n\n\nLet's confirm that this setting does indeed give us 80% power.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nres_list <- NULL # a container to collect results\nfor (i in 1:1000) {\n      \n  # simulate study\n  y0 <- rnorm(2*n_planned, sd = sigma)\n  y1 <- y0 + tau\n  d <- sample(rep(0:1, length.out = 2*n_planned))\n  y <- ifelse(d == 1, y1, y0)\n  data <- data.frame(y, d)\n  \n  # fit model and get standard error and p-value\n  fit <- lm(y ~ d, data = data)\n  tau_hat <- as.numeric(coef(fit)[\"d\"])\n  se_hat <- as.numeric(sqrt(diag(sandwich::vcovHC(fit, type = \"HC2\")))[\"d\"])\n  p_value <- pnorm(tau_hat/se_hat, lower.tail = FALSE)\n\n  # collect results\n  res_list[[i]] <- data.frame(tau_hat, se_hat, p_value)\n  }\n\n# compute power (and monte carlo error)\nres_list |>\n  bind_rows() |>\n  summarize(power = mean(p_value < 0.05), \n            mc_error = sqrt(power*(1 - power))/sqrt(n()), \n            lwr = power - 2*mc_error, \n            upr = power + 2*mc_error)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  power   mc_error       lwr       upr\n1   0.8 0.01264911 0.7747018 0.8252982\n```\n:::\n:::\n\n\nNailed it!\n\n### The Pilot Studies\n\nNow let's simulate a 1,000 pilot studies with 10, 30, 60, 90, and 150 respondents per condition. I'm going to grab the standard error from each but throw the estimates of the treatment effects right into the trash.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# sample size per condition in pilot study\nn_pilot_values   <- c(10, 30, 60, 90, 150) \n\nres_list <- NULL  # a container to collect results\niter <- 1  # counter to index the collection\nfor (i in 1:1000) {\n  for (j in 1:length(n_pilot_values)) {\n    \n    # set respondents per condition in the pilot study\n    n_pilot <- n_pilot_values[j]\n    \n    # simulate pilot study\n    y0 <- rnorm(2*n_pilot, sd = sigma)\n    y1 <- y0 + tau\n    d <- sample(rep(0:1, length.out = 2*n_pilot))\n    y <- ifelse(d == 1, y1, y0)\n    pilot_data <- data.frame(y, d)\n    \n    # fit model and get standard error\n    fit_pilot <- lm(y ~ d, data = pilot_data)\n    tau_hat <- as.numeric(coef(fit_pilot)[\"d\"])\n    pilot_se_hat <- as.numeric(sqrt(diag(sandwich::vcovHC(fit_pilot, type = \"HC2\")))[\"d\"])\n    \n    # collect standard errors \n    res_list[[iter]] <- data.frame(pilot_se_hat, n_pilot)\n    iter <- iter + 1 # update counter\n  }\n}\n\n# combine collected results in a data frame\nres <- bind_rows(res_list) |>\n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 5,000\nColumns: 2\n$ pilot_se_hat <dbl> 2.4498312, 1.5249611, 1.2713570, 0.9729555, 0.6930375, 2.…\n$ n_pilot      <dbl> 10, 30, 60, 90, 150, 10, 30, 60, 90, 150, 10, 30, 60, 90,…\n```\n:::\n:::\n\n\nNow let's take a look a these standard errors from the simulated pilot studies. Notice that the standard errors are all larger than the standard error in the full study. And the smaller the pilot, the larger the standard error. This makes sense.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(res, aes(x = pilot_se_hat)) + \n  geom_histogram() + \n  facet_wrap(vars(n_pilot)) + \n  geom_vline(xintercept = se)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nHowever, we can translate the standard error from the pilot studies into predictions for the standard errors in the full studies by multiplying the pilot standard error times $\\sqrt{\\frac{n^{pilot}}{n^{planned}}}$.^[In this setting, we're planning on 500 respondents per conditions.]\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(res, aes(x = sqrt(n_pilot/n_planned)*pilot_se_hat)) + \n  geom_histogram() + \n  facet_wrap(vars(n_pilot)) + \n  geom_vline(xintercept = se)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nThat's spot on! However, notice that we sometimes *substantially* underestimate the standard error. When we underestimate the standard error, we will *under*estimate the power.\n\nAs a solution, we can gently nudge the pilot standard error up by a factor of $\\left( \\sqrt{\\frac{1}{n^{pilot}}} + 1 \\right)$, which will make \"almost all\" of the standard errors *over*-estimates or \"conservative\" (details [here](https://github.com/carlislerainey/power-rules/blob/main/power-rules.pdf)).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(res, aes(x = sqrt(n_pilot/n_planned)*(sqrt(1/n_pilot) + 1)*pilot_se_hat)) + \n  geom_histogram() + \n  facet_wrap(vars(n_pilot)) + \n  geom_vline(xintercept = se)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nThis works super well.\n\nBut how should we use this predicted standard error to evaluate or choose a sample size?\n\n## How to use the predicted standard error\n\nWe can use these conservative standard errors to compute any of the following (conservatively, as well):\n\n1.  the minimum detectable effect with 80% power\n2.  the statistical power for a given treatment effect\n3.  the sample size required to obtain 80% power for a given treatment effect\n\n### The Minimum Detectable Effect\n\nFirst, we can compute the minimum detectable effect with 80% power. This is about 2.5 times the standard error.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# compute minimum detectable effect\nmde <- res %>%\n  mutate(pred_se_cons = sqrt(n_pilot/n_planned)*(sqrt(1/n_pilot) + 1)*pilot_se_hat, \n         mde_cons = 2.5*pred_se_cons) %>%\n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 5,000\nColumns: 4\n$ pilot_se_hat <dbl> 2.4498312, 1.5249611, 1.2713570, 0.9729555, 0.6930375, 2.…\n$ n_pilot      <dbl> 10, 30, 60, 90, 150, 10, 30, 60, 90, 150, 10, 30, 60, 90,…\n$ pred_se_cons <dbl> 0.4560182, 0.4417360, 0.4972678, 0.4563020, 0.4105858, 0.…\n$ mde_cons     <dbl> 1.140046, 1.104340, 1.243169, 1.140755, 1.026465, 1.34938…\n```\n:::\n\n```{.r .cell-code}\n# plot minimum detectable effect\nggplot(mde, aes(x = mde_cons)) + \n  geom_histogram() + \n  facet_wrap(vars(n_pilot)) + \n  geom_vline(xintercept = tau)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n### Statistical Power\n\nSecond, we can compute the statistical power for given treatment effect. Power equals $1 - \\Phi_{std}\\left(1.64 - \\frac{\\tau}{SE} \\right)$, where $\\Phi_{std}(z)$ is `pnorm()`, $SE$ is the standard error of the estimated treatment effect, and $\\tau$ is the treatment effect.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# compute the power\npwr <- res %>%\n  mutate(pred_se_cons = sqrt(n_pilot/n_planned)*(sqrt(1/n_pilot) + 1)*pilot_se_hat, \n         power_cons = 1 - pnorm(1.64 - tau/pred_se_cons)) %>%\n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 5,000\nColumns: 4\n$ pilot_se_hat <dbl> 2.4498312, 1.5249611, 1.2713570, 0.9729555, 0.6930375, 2.…\n$ n_pilot      <dbl> 10, 30, 60, 90, 150, 10, 30, 60, 90, 150, 10, 30, 60, 90,…\n$ pred_se_cons <dbl> 0.4560182, 0.4417360, 0.4972678, 0.4563020, 0.4105858, 0.…\n$ power_cons   <dbl> 0.7098323, 0.7336191, 0.6446771, 0.7093652, 0.7868515, 0.…\n```\n:::\n\n```{.r .cell-code}\n# plot the power\nggplot(pwr, aes(x = power_cons)) + \n  geom_histogram() + \n  facet_wrap(vars(n_pilot)) + \n  geom_vline(xintercept = .8)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n### Required Sample Size\n\nFinally, we can compute the required sample size to obtain 80% power to detect a certain treatment effect. \n\nAs I described above, for 80% power to detect the treatment effect $\\widetilde{\\tau}$, we will (conservatively) need about $n^{pilot} \\cdot \\left\\lbrack \\frac{2.5}{\\widetilde{\\tau}} \\cdot \\left( \\sqrt{\\frac{1}{n^{pilot}}} + 1 \\right) \\cdot {\\widehat{SE}}_{\\widehat{\\tau}}^{pilot} \\right\\rbrack^{2}$ respondents per condition, where $n^{pilot}$ is the number of respondents per condition in the pilot data and $SE_{\\widehat{\\tau}}^{pilot}$ is the estimated standard error using the pilot data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# compute the required sample size\nss <- res %>%\n  mutate(ss_cons = n_pilot*((2.5/tau)*(sqrt(1/n_pilot) + 1)*pilot_se_hat)^2) %>%\n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 5,000\nColumns: 3\n$ pilot_se_hat <dbl> 2.4498312, 1.5249611, 1.2713570, 0.9729555, 0.6930375, 2.…\n$ n_pilot      <dbl> 10, 30, 60, 90, 150, 10, 30, 60, 90, 150, 10, 30, 60, 90,…\n$ ss_cons      <dbl> 649.8520, 609.7834, 772.7352, 650.6609, 526.8148, 910.418…\n```\n:::\n\n```{.r .cell-code}\n# plot the required sample size\nggplot(ss, aes(x = ss_cons)) + \n  geom_histogram() + \n  facet_wrap(vars(n_pilot)) + \n  geom_vline(xintercept = n_planned)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nSample size is an especially helpful metric, because it is the constraint and cost that researchers face most directly. Because these required sample sizes are *conservative*, they tend to be too large---but by how much? For pilots with 60 respondents per condition, the sample sizes tend to be about 30% too large. This means that researchers could have obtained 80% power with 1,000 respondents but instead used 1,300 respondents. \n\nIn my view, this 30% waste is not particularly concerning. It's relatively small and the statistical power will still be less than 90% even if the sample size is increased by 30%.\n\nBut most importantly, almost all of the sample sizes *exceed* what we need for 80% power.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# compute features of the sample sizes\nss %>%\n  group_by(n_pilot) %>%\n  mutate(waste = ss_cons/n_planned - 1) %>%\n  summarize(avg_waste = scales::percent(mean(waste), accuracy = 1), \n            pct_too_small = scales::percent(mean(ss_cons < 500), accuracy = 1)) %>%\n  rename(`Respondents per condition in pilot study` = n_pilot,\n         `Average waste (needed 1,000 and used 1,300 means waste is 30%)` = avg_waste, \n         `Percent of sample sizes that produce less than 80% power` = pct_too_small) %>%\n  tinytable::tt()\n```\n\n::: {.cell-output-display}\n```{=html}\n<!DOCTYPE html> \n<html lang=\"en\">\n  <head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>tinytable_ww6mb2fv1qgf79mem3gu</title>\n    <style>\n.table td.tinytable_css_m9x3hbv5xd1ka9v9a2c7, .table th.tinytable_css_m9x3hbv5xd1ka9v9a2c7 {    border-bottom: solid 0.1em #d3d8dc; }\n    </style>\n    <script src=\"https://polyfill.io/v3/polyfill.min.js?features=es6\"></script>\n    <script id=\"MathJax-script\" async src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"></script>\n    <script>\n    MathJax = {\n      tex: {\n        inlineMath: [['$', '$'], ['\\\\(', '\\\\)']]\n      },\n      svg: {\n        fontCache: 'global'\n      }\n    };\n    </script>\n  </head>\n\n  <body>\n    <div class=\"container\">\n      <table class=\"table table-borderless\" id=\"tinytable_ww6mb2fv1qgf79mem3gu\" style=\"width: auto; margin-left: auto; margin-right: auto;\" data-quarto-disable-processing='true'>\n        <thead>\n        \n              <tr>\n                <th scope=\"col\">Respondents per condition in pilot study</th>\n                <th scope=\"col\">Average waste (needed 1,000 and used 1,300 means waste is 30%)</th>\n                <th scope=\"col\">Percent of sample sizes that produce less than 80% power</th>\n              </tr>\n        </thead>\n        \n        <tbody>\n                <tr>\n                  <td> 10</td>\n                  <td>76%</td>\n                  <td>8%</td>\n                </tr>\n                <tr>\n                  <td> 30</td>\n                  <td>42%</td>\n                  <td>4%</td>\n                </tr>\n                <tr>\n                  <td> 60</td>\n                  <td>29%</td>\n                  <td>3%</td>\n                </tr>\n                <tr>\n                  <td> 90</td>\n                  <td>23%</td>\n                  <td>3%</td>\n                </tr>\n                <tr>\n                  <td>150</td>\n                  <td>19%</td>\n                  <td>1%</td>\n                </tr>\n        </tbody>\n      </table>\n    </div>\n\n    <script>\n      function styleCell_tinytable_zurp7tde1n3od9t8f0er(i, j, css_id) {\n        var table = document.getElementById(\"tinytable_ww6mb2fv1qgf79mem3gu\");\n        table.rows[i].cells[j].classList.add(css_id);\n      }\n      function insertSpanRow(i, colspan, content) {\n        var table = document.getElementById('tinytable_ww6mb2fv1qgf79mem3gu');\n        var newRow = table.insertRow(i);\n        var newCell = newRow.insertCell(0);\n        newCell.setAttribute(\"colspan\", colspan);\n        // newCell.innerText = content;\n        // this may be unsafe, but innerText does not interpret <br>\n        newCell.innerHTML = content;\n      }\n      function spanCell_tinytable_zurp7tde1n3od9t8f0er(i, j, rowspan, colspan) {\n        var table = document.getElementById(\"tinytable_ww6mb2fv1qgf79mem3gu\");\n        const targetRow = table.rows[i];\n        const targetCell = targetRow.cells[j];\n        for (let r = 0; r < rowspan; r++) {\n          // Only start deleting cells to the right for the first row (r == 0)\n          if (r === 0) {\n            // Delete cells to the right of the target cell in the first row\n            for (let c = colspan - 1; c > 0; c--) {\n              if (table.rows[i + r].cells[j + c]) {\n                table.rows[i + r].deleteCell(j + c);\n              }\n            }\n          }\n          // For rows below the first, delete starting from the target column\n          if (r > 0) {\n            for (let c = colspan - 1; c >= 0; c--) {\n              if (table.rows[i + r] && table.rows[i + r].cells[j]) {\n                table.rows[i + r].deleteCell(j);\n              }\n            }\n          }\n        }\n        // Set rowspan and colspan of the target cell\n        targetCell.rowSpan = rowspan;\n        targetCell.colSpan = colspan;\n      }\n\nwindow.addEventListener('load', function () { styleCell_tinytable_zurp7tde1n3od9t8f0er(0, 0, 'tinytable_css_m9x3hbv5xd1ka9v9a2c7') })\nwindow.addEventListener('load', function () { styleCell_tinytable_zurp7tde1n3od9t8f0er(0, 1, 'tinytable_css_m9x3hbv5xd1ka9v9a2c7') })\nwindow.addEventListener('load', function () { styleCell_tinytable_zurp7tde1n3od9t8f0er(0, 2, 'tinytable_css_m9x3hbv5xd1ka9v9a2c7') })\n    </script>\n\n  </body>\n\n</html>\n```\n:::\n:::\n\n\n## Summary\n\nWe think of statistical power as determined by the ratio $\\frac{\\tau}{SE}$, where $\\tau$ is the treatment effect and SE is the standard error of the estimate. To reason about statistical power, one needs to make assumptions or predictions about the treatment effect and the standard error.\n\nI make two points in this post:\n\n1.  Pilot data are not usually useful to estimate the treatment effect.\n2.  Pilot data can be useful to predict the standard error.\n\nWith a predicted standard error in hand, we can obtain a prediction for the minimum detectable effect, the statistical power, or the required sample size.\n\nYou can find more details in [this paper](https://github.com/carlislerainey/power-rules/blob/main/power-rules.pdf).",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}