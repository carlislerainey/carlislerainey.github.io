[
  {
    "objectID": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html",
    "href": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html",
    "title": "Power, Part I: Power Is for You, Not for Reviewer Two",
    "section": "",
    "text": "There’s been some really good work lately on statistical power. I’ll point you to two really great papers.\n\nArel-Bundock, Vincent, Ryan C. Briggs, Hristos Doucouliagos, Marco Mendoza Aviña, and T.D. Stanley. 2022. “Quantitative Political Science Research Is Greatly Underpowered.” OSF Preprints. July 5. doi: 10.31219/osf.io/7vy2f.\nKane, John V. 2023. “More Than Meets the ITT: A Guide for Investigating Null Results .” APSA Preprints. doi: 10.33774/apsa-2023-h4p0q-v2.\n\nI’ve been long interested in statistical power (see Rainey 20141 and Rainey 20152), and these new papers have me thinking even more about the importance of power.1 Rainey, Carlisle. 2014. “Arguing for a Negligible Effect.” American Journal of Political Science 58(4): 1083-1091.2 McCaskey, Kelly and Carlisle Rainey. 2015. “Substantive Importance and the Veil of Statistical Significance.” Statistics, Politics, and Policy 6(1-2): 77-96.\nIn this post, I argue that statistical power isn’t something ancillary. Power is primary. I also argue that power isn’t something you–the researcher–build to satisfy an especially cranky Reviewer 2, it’s something you do for yourself, to make sure that your study succeeds."
  },
  {
    "objectID": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#background",
    "href": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#background",
    "title": "Power, Part I: Power Is for You, Not for Reviewer Two",
    "section": "",
    "text": "There’s been some really good work lately on statistical power. I’ll point you to two really great papers.\n\nArel-Bundock, Vincent, Ryan C. Briggs, Hristos Doucouliagos, Marco Mendoza Aviña, and T.D. Stanley. 2022. “Quantitative Political Science Research Is Greatly Underpowered.” OSF Preprints. July 5. doi: 10.31219/osf.io/7vy2f.\nKane, John V. 2023. “More Than Meets the ITT: A Guide for Investigating Null Results .” APSA Preprints. doi: 10.33774/apsa-2023-h4p0q-v2.\n\nI’ve been long interested in statistical power (see Rainey 20141 and Rainey 20152), and these new papers have me thinking even more about the importance of power.1 Rainey, Carlisle. 2014. “Arguing for a Negligible Effect.” American Journal of Political Science 58(4): 1083-1091.2 McCaskey, Kelly and Carlisle Rainey. 2015. “Substantive Importance and the Veil of Statistical Significance.” Statistics, Politics, and Policy 6(1-2): 77-96.\nIn this post, I argue that statistical power isn’t something ancillary. Power is primary. I also argue that power isn’t something you–the researcher–build to satisfy an especially cranky Reviewer 2, it’s something you do for yourself, to make sure that your study succeeds."
  },
  {
    "objectID": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#the-hypothesis-testing-framework",
    "href": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#the-hypothesis-testing-framework",
    "title": "Power, Part I: Power Is for You, Not for Reviewer Two",
    "section": "The Hypothesis Testing Framework",
    "text": "The Hypothesis Testing Framework\nIn the hypothesis testing framework, you consider two hypotheses: the null hypothesis and the alternative hypothesis.\nThe hypothesis test is all about arguing against the null hypothesis \\(H_0\\) (leaving the alternative \\(H_A\\) as the only remaining possibility). You will (try to) show that your data would be “unusual” if the null hypothesis were correct.33 When hypothesizing about the average treatment effect (ATE), this can take a variety of forms. The form doesn’t really matter.\nIf the data would NOT be unusual under the null hypothesis, then you do not reject the null hypothesis."
  },
  {
    "objectID": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#intepreting-a-failure-to-reject",
    "href": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#intepreting-a-failure-to-reject",
    "title": "Power, Part I: Power Is for You, Not for Reviewer Two",
    "section": "Intepreting a Failure to Reject",
    "text": "Intepreting a Failure to Reject\nA failure to reject means that the data “would not be unusual under the null hypothesis.” This does not imply that you should conclude the data are only consistent with the null. Indeed, there is a sharp asymmetry in hypothesis testing. I describe this in my 2014 AJPS:\n\nPolitical scientists commonly interpret a lack of statistical significance (i.e., a failure to reject the null) as evidence for a negligible effect (Gill 1999), but this approach acts as a broken compass… If the sample size is too small, the researcher often concludes that the effect is negligible even though the data are also consistent with large, meaningful effects. This occurs because the small sample leads to a large confidence interval, which is likely to contain both “no effect” and large effects.\n\nGill (1999)4 describes this more forcefully:4 Gill, Jeff. 1999. “The Insignificance of Null Hypothesis Significance Testing.” Political Research Quarterly 52(3): 647-674.\n\nWe teach graduate students to be very careful when describing the occurrence of not rejecting the null hypothesis. This is because failing to reject the null hypothesis does not rule out an infinite number of other competing research hypotheses. Null hypothesis significance testing is asymmetric: if the test statistic is sufficiently atypical given the null hypothesis then the null hypothesis is rejected, but if the test statistic is insufficiently atypical given the null hypothesis then the null hypothesis is not accepted. This is a double standard: H1 is held innocent until proven guilty and Ho is held guilty until proven innocent (Rozeboom 1960)…\n\n\nThere are two problems that develop as a result of asymmetry. The first is a misinterpretation of the asymmetry to assert that finding a non-statistically significant difference or effect is evidence that it is equal to zero or nearly zero. Regarding the impact of this acceptance error Schmidt (1996: 126) asserts that this: “belief held by many researchers is the most devastating of all to the research enterprise.” This acceptance of the null hypothesis is damaging because it inhibits the exploration of competing research hypotheses. The second problem pertains to the correct interpretation of failing to reject the null hypotheses. Failing to reject the null hypothesis essentially provides almost no information about the state of the world. It simply means that given the evidence at hand one cannot make an assertion about some relationship: all you can conclude is that you can’t conclude that the null was false (Cohen 1962).\n\nThere are many incorrect, but somewhat innocent interpretations of p-values. Interpreting a lack of statistical significance as evidence for the null is incorrect and wildly misleading in many cases.\n\n\n\n\n\n\nImportant Point\n\n\n\nA non-statistically significant difference is not evidence that an effect is equal to zero or nearly zero. Interpreting a non-statistically significant effect otherwise is “devastating.”"
  },
  {
    "objectID": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#the-implication-of-a-non-conclusion",
    "href": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#the-implication-of-a-non-conclusion",
    "title": "Power, Part I: Power Is for You, Not for Reviewer Two",
    "section": "The Implication of a Non-Conclusion",
    "text": "The Implication of a Non-Conclusion\nIf you cannot draw a conclusion then, what exactly has happened? Obtaining \\(p &gt; 0.05\\) will not be an “error” because you won’t make a strong claim that the research hypothesis is wrong. Instead, you will simply admit that you failed to uncover evidence against the null. Failing to uncover evidence isn’t an error.\nIndeed, Jones and Tukey (2000)5 write:5 Jones, Lyle V., and John W. Tukey. 2000. “A Sensible Formulation of the Significance Test.” Psychological Methods 5(4): 411-414.\n\nA conclusion is in error only when it is “a reversal,” when it asserts one direction while the (unknown) truth is the other direction. Asserting that the direction is not yet established may constitute a wasted opportunity, but it is not an error.\n\n\n\n\n\n\n\nImportant Point\n\n\n\nFailing to uncover evidence isn’t an “error,” it is a “wasted effort.”\n\n\nThis is worth emphasizing in a different way. Tests are not magical tools that tell you which hypothesis is correct. Instead, tests summarize the evidence against the null. There are two critical pieces to “evidence against the null”: (1) the amount of evidence and (2) whether the evidence is against the null. If you buy your own argument that the null is false (surely you do!), then (2) is taken care of. Only the amount of evidence remains, and you–the researcher–choose the amount of evidence to supply."
  },
  {
    "objectID": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#the-implication-for-power-calculations",
    "href": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#the-implication-for-power-calculations",
    "title": "Power, Part I: Power Is for You, Not for Reviewer Two",
    "section": "The Implication for Power Calculations",
    "text": "The Implication for Power Calculations\nThis perspective helps motivate power calculations. By their design, tests control the error rate in certain situations (when then null is correct). You do not need to worry about Type I errors. First, the test controls the error rate under the null. Second, you are pretty sure the null is wrong (see your theory section).\n\n\n\n\n\n\nImportant Point\n\n\n\nThe hypothesis test takes care of the the Type I error rate. If you choose a properly-sized test, you don’t need to worry about those errors any more.\n\n\nIf you aren’t worried about Type I errors, what are you worried about? They only thing left to worry about is wasting your time and money. Statistical power is the chance not of wasting your time and money.\nPower isn’t a secondary quantity that you compute for thoroughness or in anticipation of a comment from Reviewer 2. Power is something that you build for yourself.\nStatisticians talk a lot about Type I errors because that’s their contribution. It’s your job to bring the power.\nAnd importantly, power is under your control. Kane provides a rich summary of ways to increase the power of your experiment. At a minimum, you have brute force control through sample size.\nPower isn’t an ancillary concern, it’s the entire game from the very beginning of the planning stage. It should be at the forefront of the researchers mind from the very beginning. You should want the power as high as possible.66 I hear that 80% is the standard, but I’m pretty uncomfortable spending dozens of hours and thousands of dollars running for a 1 in 5 chance of wasting my time. I want that chance as close to zero as I can get it. I want power close to 100%. 99% power and 80% power might both seem “high” or “acceptable,” but these are not the same. 80% power means 1 in 5 studies fail. 99% power means that 1 in 100 studies fail.\nYou have to supply a test overwhelming evidence to consistently reject the null. Careful power calculations help you make sure you succeed in this war against the null.\nPower isn’t about Type S and M errors (Gelman and Carlin 2014)7. Power is about you protecting yourself from a failed study. And that seems like a protection worth pursuing carefully.87 Gelman, Andrew, and John Carlin. “Beyond Power Calculations: Assessing Type S (Sign) and Type M (Magnitude) Errors.” Perspectives on Psychological Science 9(6): 641-651.8 Of course it’s also about Type S and M errors, but those are discipline-level concerns. I’m talking about your incentives as a researcher."
  },
  {
    "objectID": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#summary",
    "href": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#summary",
    "title": "Power, Part I: Power Is for You, Not for Reviewer Two",
    "section": "Summary",
    "text": "Summary\nHere are the takeaways:\n\nStatistical power is the chance of using your time and money productively (i.e., not wasting it).\nStatistical power is under your control (see Kane).\nYour power might be (much) lower than you think–you should check (see Arel-Bundock et al.).\nPower should be a primary concern throughout the design. The researcher should care deeply about power, perhaps more than anything else.\n\n\n\n\n\n\n\nImportant Point\n\n\n\nThe hypothesis test is no oracle. It will not consistently reject the null (even when the null is wrong) unless you supply overwhelming evidence. In experimental design, that’s not a task, that’s the task."
  },
  {
    "objectID": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html",
    "href": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html",
    "title": "Power, Part II: What Do Confidence Intervals from High-Powered Studies Look Like?",
    "section": "",
    "text": "In this post, I address confidence intervals that are nestled right up against zero.1 These intervals indicate that an estimate is “barely” significant. I want to be clear: “barely significant” is still significant, so you should still reject the null hypothesis.21 This is the second post in a series. In my previous post, I mentioned two new papers that have me thinking about power: Arel-Bundock et al.’s “Quantitative Political Science Research Is Greatly Underpowered” and Kane’s “More Than Meets the ITT: A Guide for Investigating Null Results”. Go check out that post and those papers if you haven’t.2 I’m focusing on confidence intervals here because inference from confidence intervals is a bit more intuitive (see Rainey 2014 and Rainey 2015. In the cases I discuss, whether one checks whether the p-value is less than 0.05 or checks that confidence interval contains zero are equivalent.\nBut I want to address a feeling that can come along with a confidence interval nestled right up against zero. A feeling of victory. It seems like a perfectly designed study. You rejected the null and collected just enough data to do it.\nBut instead, it should feel like a near-miss. Like an accident narrowly avoided. A confidence interval nestled right up against zero indicates that one of two things has happened: either you were (1) unlucky or (2) under-powered.\nBecause “unlucky” is always a possibility, we can’t learn much from a particular confidence interval, but we can learn a lot from a literature. A literature with well-powered studies produces confidence intervals that often fall far from zero. A well-powered literature does not produce confidence intervals that consistently nestle up against zero. Under-powered studies, though, do tend to produce confidence intervals that nestle right up against zero."
  },
  {
    "objectID": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#background",
    "href": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#background",
    "title": "Power, Part II: What Do Confidence Intervals from High-Powered Studies Look Like?",
    "section": "",
    "text": "In this post, I address confidence intervals that are nestled right up against zero.1 These intervals indicate that an estimate is “barely” significant. I want to be clear: “barely significant” is still significant, so you should still reject the null hypothesis.21 This is the second post in a series. In my previous post, I mentioned two new papers that have me thinking about power: Arel-Bundock et al.’s “Quantitative Political Science Research Is Greatly Underpowered” and Kane’s “More Than Meets the ITT: A Guide for Investigating Null Results”. Go check out that post and those papers if you haven’t.2 I’m focusing on confidence intervals here because inference from confidence intervals is a bit more intuitive (see Rainey 2014 and Rainey 2015. In the cases I discuss, whether one checks whether the p-value is less than 0.05 or checks that confidence interval contains zero are equivalent.\nBut I want to address a feeling that can come along with a confidence interval nestled right up against zero. A feeling of victory. It seems like a perfectly designed study. You rejected the null and collected just enough data to do it.\nBut instead, it should feel like a near-miss. Like an accident narrowly avoided. A confidence interval nestled right up against zero indicates that one of two things has happened: either you were (1) unlucky or (2) under-powered.\nBecause “unlucky” is always a possibility, we can’t learn much from a particular confidence interval, but we can learn a lot from a literature. A literature with well-powered studies produces confidence intervals that often fall far from zero. A well-powered literature does not produce confidence intervals that consistently nestle up against zero. Under-powered studies, though, do tend to produce confidence intervals that nestle right up against zero."
  },
  {
    "objectID": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#a-simulation",
    "href": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#a-simulation",
    "title": "Power, Part II: What Do Confidence Intervals from High-Powered Studies Look Like?",
    "section": "A Simulation",
    "text": "A Simulation\nI’m going to explore the behavior of confidence intervals with a little simulation. In this simulation, I’m going to assert a standard error rather than create the standard error endogenously through sample size, etc. I use a true effect size of 0.5 and standard errors of 0.5, 0.3, 0.2, and 0.15 to create studies with 25%, 50%, 80%, and 95% power, respectively.3 I think of 80% as “minimally-powered” and 95% as “well-powered.”3 I’m ignoring how to choose the true effect, estimate the standard error, and compute power. For now, I’m placing all this behind the curtain. See Daniël Lakens’ book [Improving Your Statistical Inferences] for discussion (h/t Bermond Scoggins).\nI’m using a one-sided test (hypothesizing a positive effect), so I’ll use 90% confidence intervals with arms that are 1.64 standard errors wide. Let’s simulate some estimates from each of our four studies and compute their confidence intervals. I simulate 5,000 confidence intervals to explore below.\n\n\nCode\n# load packages\nlibrary(tidyverse)\n\n# create a parameter for the true effect\ntrue_effect &lt;- 0.5 # just assumed by me\n\n# create a data frame of standard errors (with approximate power)\nse_df &lt;- tribble(\n  ~se,    ~pwr,\n  0.5,    \"about 25% power\",\n  0.3,    \"about 50% power\",\n  0.2,    \"about 80% power\",\n  0.15,   \"about 95% power\"\n)\n\n# create function to simulate estimates for each standard error\nsimulate_estimates &lt;- function(se, pwr) {\n  tibble(\n    est = rnorm(n_cis, mean = true_effect, sd = se),\n    se = se,\n    pwr = pwr\n  )\n}\n\n# simulate the estimates, compute the confidence intervals, and wrangle\nn_cis &lt;- 5000  # the number of cis to create\nci_df &lt;- se_df %&gt;% \n  # simulate estimates\n  pmap_dfr(simulate_estimates) %&gt;%\n  # compute confidence intervals\n  mutate(lwr = est - 1.64*se, \n         upr = est + 1.64*se) %&gt;%\n  # summarize the location of the confidence interval\n  mutate(result = case_when(lwr &lt; 0 ~ \"Not significant\",\n                            lwr &lt; se ~ \"Nestled against zero\",\n                            lwr &gt;= se~ \"Not nestled against zero\"))\n\n\nNow let’s quickly confirm my power calculations by computing the proportion of confidence intervals to the right of zero. These are about right. In a later post, I’ll describe how I think about computing these quantities.\n\n\nCode\n# confirm power calculations\nci_df %&gt;%\n  group_by(se, pwr) %&gt;%\n  summarize(sim_pwr = 1 - mean(result == \"Not significant\"),\n            sim_pwr = scales::percent(sim_pwr, accuracy = 1)) %&gt;%\n    select(SE = se, \n         Power = pwr,\n         `Percent Significant` = sim_pwr) %&gt;%\n  kableExtra::kable(format = \"markdown\")\n\n\n\n\n\nSE\nPower\nPercent Significant\n\n\n\n\n0.15\nabout 95% power\n95%\n\n\n0.20\nabout 80% power\n80%\n\n\n0.30\nabout 50% power\n51%\n\n\n0.50\nabout 25% power\n26%"
  },
  {
    "objectID": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#what-do-confidence-intervals-from-well-powered-studies-look-like",
    "href": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#what-do-confidence-intervals-from-well-powered-studies-look-like",
    "title": "Power, Part II: What Do Confidence Intervals from High-Powered Studies Look Like?",
    "section": "What Do Confidence Intervals from Well-Powered Studies Look Like?",
    "text": "What Do Confidence Intervals from Well-Powered Studies Look Like?\nNow let’s see what these confidence intervals look like. 5,000 is too many to plot, so I sample 25. But I applied the statistical significance filter first. This mimics the publication process and makes the plots a little easier to compare. My argument doesn’t depend on this filter, though.\nI plotted these 100 intervals below44 4 studies x 25 simulated intervals per study = 100 intervals.\nThere are three important vertical lines in these plots.\n\nThe solid line indicates zero. All confidence intervals are above zero because I applied the significance filter.\nThe dotted line indicates one standard error above zero. This varies across panels because the standard error varies across panels.\nThe dashed line indicates the true effect of 0.5. Because I applied the significance filter, the lower-powered studies are consistently over-estimating the true effect.\n\nThe intervals are green when the lower bound of the 90% confidence interval falls within one standard error of zero—that’s my definition of “nestled up against zero.” The intervals are orange when the lower bound falls further than one standard error above zero.\nNotice how low-powered studies tend to nestle their confidence intervals right up against zero. Almost all of the confidence intervals from the study with 25% power are nestled right up against zero. Very few of the confidence intervals from the study with 95% power are nestled up against zero.\nAgain, you should apply this standard to a literature. You should not apply this standard to a particular study because even well-powered studies sometimes produce confidence intervals that nestle up against zero. But when you start to see confidence intervals consistently falling close to zero, you should start to assume that the literature uses under-powered studies and that the estimates in that literature are inflated due to Type M errors (Gelman and Stern 2014).\n\n\nCode\ngg_df &lt;- ci_df %&gt;%\n  filter(lwr &gt; 0) %&gt;% # apply significance filter \n  # sample 25 intervals (from those that are significant)\n  group_by(se, pwr) %&gt;%\n  sample_n(25) %&gt;%\n  # create id (ordered by estimate value)\n  group_by(se, pwr) %&gt;%\n  arrange(est) %&gt;%\n  mutate(ci_id = 1:n())\n  \nggplot(gg_df, aes(x = est, xmin = lwr, xmax = upr, y = ci_id,\n                    color = result)) + \n  facet_wrap(vars(pwr), ncol = 1, scales = \"free_x\") + \n  geom_vline(data = se_df, aes(xintercept = se), linetype = \"dotted\") +\n  geom_vline(xintercept = 0) + \n  geom_vline(xintercept = true_effect, linetype = \"dashed\") + \n  geom_errorbarh(height = 0) + \n  geom_point() + \n  scale_color_brewer(type = \"qual\", palette = 2) + \n  theme_bw() + \n  labs(x = \"Estimate and 90% CI\",\n       y = NULL,\n       color = \"Result\")"
  },
  {
    "objectID": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#showing-this-another-way-density-of-the-lower-bounds",
    "href": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#showing-this-another-way-density-of-the-lower-bounds",
    "title": "Power, Part II: What Do Confidence Intervals from High-Powered Studies Look Like?",
    "section": "Showing This Another Way: Density of the Lower Bounds",
    "text": "Showing This Another Way: Density of the Lower Bounds\nWe can also plot the density of the lower bounds of these 5,000 intervals. This approach shows the “nestling” most clearly. The plots below show that the lower bounds of confidence intervals tend to nestle close to zero when the power is low, and lie further from zero when the power is high.\n\n\nCode\ngg_df &lt;- ci_df %&gt;%\n  filter(lwr &gt; 0) # apply significance filter \nggplot(gg_df, aes(x = lwr)) + \n  facet_wrap(vars(pwr), scales = \"free_x\") + \n  geom_density(fill = \"grey50\") + \n  geom_vline(data = se_df, aes(xintercept = se), linetype = \"dotted\") + \n  theme_bw() + \n  labs(x = \"Location of Lower Bound of 90% CI\",\n       y = \"Density\",\n       color = \"Power\")"
  },
  {
    "objectID": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#showing-this-another-way-frequency-of-nestling",
    "href": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#showing-this-another-way-frequency-of-nestling",
    "title": "Power, Part II: What Do Confidence Intervals from High-Powered Studies Look Like?",
    "section": "Showing This Another Way: Frequency of Nestling",
    "text": "Showing This Another Way: Frequency of Nestling\nLastly, I compute the percent of confidence intervals that are nestled right up against zero. For a well-powered study with 95% power, only about 1 in 5 confidence intervals nestle up against zero. For a poorly-powered study with 25% power, about 4 in 5 of confidence intervals nestle up against zero (among those that are above zero). The table below shows the remaining frequencies.\n\n\nCode\nci_df %&gt;%\n  group_by(se, pwr, result) %&gt;%\n  summarize(frac = n()/n_cis, .groups = \"drop\") %&gt;%\n  pivot_wider(names_from = result, values_from = frac) %&gt;%\n  mutate(`Nestled, given significant` = `Nestled against zero`/(1 - `Not significant`),\n         `Not nestled, given significant` = `Not nestled against zero`/(1 - `Not significant`)) %&gt;%\n  select(SE = se, \n         Power = pwr,\n         `Not significant`,\n         `Nestled against zero`,\n         `Not nestled against zero`,\n         `Nestled, given significant`,\n         `Not nestled, given significant`) %&gt;%\n  mutate(across(`Not significant`:`Not nestled, given significant`, ~ scales::percent(., accuracy = 1))) %&gt;%\n  kableExtra::kable()\n\n\n\n\n\nSE\nPower\nNot significant\nNestled against zero\nNot nestled against zero\nNestled, given significant\nNot nestled, given significant\n\n\n\n\n0.15\nabout 95% power\n5%\n19%\n76%\n20%\n80%\n\n\n0.20\nabout 80% power\n20%\n36%\n45%\n44%\n56%\n\n\n0.30\nabout 50% power\n49%\n35%\n16%\n69%\n31%\n\n\n0.50\nabout 25% power\n74%\n21%\n5%\n81%\n19%"
  },
  {
    "objectID": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#summary",
    "href": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#summary",
    "title": "Power, Part II: What Do Confidence Intervals from High-Powered Studies Look Like?",
    "section": "Summary",
    "text": "Summary\nIn this post, I address confidence intervals that are nestled right up against zero. These intervals can suggest a perfectly powered study—not too much, not too little. But instead, a confidence interval nestled right up against zero indicates that one of two things has happened: either you were (1) unlucky or (2) under-powered.\nBecause “unlucky” is always a possibility, we can’t learn much from a particular confidence interval, but we can learn a lot from a literature. A literature with well-powered studies produces confidence intervals that often fall far from zero. A well-powered literature does not produce confidence intervals that consistently nestle up against zero. Under-powered studies, though, do tend to produce confidence intervals that nestle right up against zero.\n\n\n\n\n\n\nKey Takeaway\n\n\n\nUnder-powered studies tend to produce confidence intervals that are nestled right up against zero. Well-powered studies tend to produce confidence intervals that fall further away. A literature that produces confidence intervals that consistently nestle right up against zero is likely a collection of under-powered studies."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\nMay 25, 2023\n\n\nPower, Part II: What Do Confidence Intervals from High-Powered Studies Look Like?\n\n\n9 min\n\n\n\n\nMay 21, 2023\n\n\nPower, Part I: Power Is for You, Not for Reviewer Two\n\n\n8 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Carlisle Rainey",
    "section": "",
    "text": "email\n  \n  \n    \n     CV\n  \n\n  \n  \nI’m an Associate Professor in the Department of Political Science at Florida State University. My research has appeared in the American Political Science Review, the American Journal of Political Science, Political Analysis, and other peer-reviewed journals. I teach courses in American politics, comparative politics, and political methodology.\n\nConnect\nIf want to stay up-to-date on my work, please join my email list. Each semester, I send around a brief summary of things I’m working on, finding useful, and thinking about. You can also subscribe to blog posts and working papers, if you want.\n\n  \n      Get Updates!\n  \n\n\nYou can also subscribe to blog posts via RSS. I’m on Twitter and Mastodon. I publicly version-control many of research projects and courses on GitHub. I upload my talks to Speaker Deck. I’m on Google Scholar, ORCID and OSF."
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Research",
    "section": "",
    "text": "Rainey, Carlisle. “Hypothesis Tests Under Separation.” Conditionally accepted at Political Analysis. [Paper][GitHub] [OSF] [Dataverse]\nClifford, Scott, Thomas Leeper, and Carlisle Rainey. “Generalizing Survey Experiments Using Topic Sampling: Theory, Evaluations, and Extensions.” [Draft]\nRainey, Carlisle, Harley Roe, and Qing Wang. “A Latent Measure of Dissent.” [GitHub]"
  },
  {
    "objectID": "research/index.html#unpublished-papers",
    "href": "research/index.html#unpublished-papers",
    "title": "Research",
    "section": "",
    "text": "Rainey, Carlisle. “Hypothesis Tests Under Separation.” Conditionally accepted at Political Analysis. [Paper][GitHub] [OSF] [Dataverse]\nClifford, Scott, Thomas Leeper, and Carlisle Rainey. “Generalizing Survey Experiments Using Topic Sampling: Theory, Evaluations, and Extensions.” [Draft]\nRainey, Carlisle, Harley Roe, and Qing Wang. “A Latent Measure of Dissent.” [GitHub]"
  },
  {
    "objectID": "research/index.html#published-papers",
    "href": "research/index.html#published-papers",
    "title": "Research",
    "section": "Published Papers",
    "text": "Published Papers\n\nForthcoming\n\nClifford, Scott, Thomas Leeper, and Carlisle Rainey. “Generalizing Survey Experiments Using Topic Sampling: An Application to Party Cues.” Forthcoming in Political Behavior. [Draft]\nRainey, Carlisle. “A Careful Consideration of CLARIFY: Simulation-Induced Bias in Point Estimates of Quantities of Interest.” Forthcoming in Political Science Research and Methods. [GitHub]\n\n\n\n2021\n\nMcCaskey, Kelly and Carlisle Rainey. 2021 “Estimating Logit Models with Small Samples.” Political Science Research and Methods 9(3) 549-564. [Paper] [GitHub]\n\n\n\n2020\n\nBaissa, Daniel K. and Carlisle Rainey. 2020. “When BLUE Is Not Best: Non-Normal Errors and the Linear Model.” Political Science Research and Methods 8(1) 136-148. [Paper] [GitHub]\n\n\n\n2018\n\nRainey, Carlisle and Robert Jackson. “Unreliable Inferences about Unobservable Processes: A Critique of Partial Observability Models.” Political Science Research and Methods 6(2): 381-391. [Paper] [Journal [GitHub]\n\n\n\n2017\n\nRainey, Carlisle. 2017. “Transformation-Induced Bias: Unbiased Coefficients Do Not Imply Unbiased Quantities of Interest.” Political Analysis 25(3): 402-409. [Paper] [Journal] [Dataverse] [GitHub]\n\n\n\n2016\n\nRainey, Carlisle. 2016. “Dealing with Separation in Logistic Regression Models.” Political Analysis. 24(3): 339-355. [Paper] [Journal] [Slides] [Appendix] [GitHub] [Software]\nRainey, Carlisle. 2016. “Compression and Conditional Effects: A Product Term Is Essential When Using Logistic Regression to Test for Interaction.” Political Science Research and Methods. 4(3): 621-639. [Paper] [Appendix] [Journal] [GitHub]\nRainey, Carlisle. 2016. “Does District Magnitude Matter: The Case of Taiwan.” Electoral Studies. 41: 202-212. [Paper] [Journal] [Dataverse] [GitHub]\n\n\n\n2015\n\nMcCaskey, Kelly and Carlisle Rainey. 2015. “Substantive Importance and the Veil of Statistical Significance.” Statistics, Politics, and Policy 6(1-2): 77-96. [Paper] [Journal] [GitHub]\nClifford, Scott, Jennifer Jerit, Matt Motyl, and Carlisle Rainey. 2015. “Moral Concerns and Culture War Attitudes: Investigating the Influence of Elite Rhetoric.” Political Communication. 32(2): 229-248. [Paper] [Journal]\nRainey, Carlisle. 2015. “Strategic Mobilization: Why Proportional Representation Decreases Voter Mobilization.” Electoral Studies. 37(1): 86-98. [Paper] [Appendix] [Journal] [Dataverse] [GitHub]\n\n\n\n2014\n\nRainey, Carlisle. 2014. “Arguing for a Negligible Effect.” American Journal of Political Science 58(4): 1083-1091. [Paper] [Appendix] [Journal] [Dataverse] [GitHub]\nBarabas, Jason, Jennifer Jerit, William Pollock, and Carlisle Rainey. 2014. “The Question(s) of Political Knowledge.” American Political Science Review 108(4): 840-855. [Paper] [Journal] [Dataverse] [GitHub]\nBarrilleaux, Charles and Carlisle Rainey. 2014. “The Politics of Need: Examining Governors’ Decisions to Oppose the ‘Obamacare’ Medicaid Expansion.” State Politics and Policy Quarterly. 14(4): 437-460. [Paper] [Appendix] [Journal] [Dataverse] [GitHub]"
  },
  {
    "objectID": "talks/2023-topic-sampling/index.html",
    "href": "talks/2023-topic-sampling/index.html",
    "title": "Topic Sampling @ EPOVB 2023",
    "section": "",
    "text": "The two papers are below.\n\nClifford, Scott, Thomas Leeper, and Carlisle Rainey. “Generalizing Survey Experiments Using Topic Sampling: An Application to Party Cues.” Forthcoming in Political Behavior. [Draft]\nClifford, Scott, Thomas Leeper, and Carlisle Rainey. “Generalizing Survey Experiments Using Topic Sampling: Theory, Evaluations, and Extensions.” [Draft]\n\nSlides, with transitions [Dropbox] and without [Dropbox]"
  },
  {
    "objectID": "talks/index.html",
    "href": "talks/index.html",
    "title": "Talks",
    "section": "",
    "text": "Topic Sampling @ EPOVB 2023\n\n\nA talk at the 2023 meeting of the Election, Public Opinion, and Voting Behavior section of APSA.\n\n\n\nCarlisle Rainey\n\n\nMar 4, 2023\n\n\n\n\n\n\n\n\nNo matching items\n\n\nFor older talks, see Speaker Deck."
  },
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Teaching",
    "section": "",
    "text": "Advanced Quantitative Methods"
  },
  {
    "objectID": "teaching/index.html#current-classes",
    "href": "teaching/index.html#current-classes",
    "title": "Teaching",
    "section": "",
    "text": "Advanced Quantitative Methods"
  },
  {
    "objectID": "teaching/index.html#archived-classes",
    "href": "teaching/index.html#archived-classes",
    "title": "Teaching",
    "section": "Archived Classes",
    "text": "Archived Classes\n\nPOS 3713: Introduction to Political Science Research Methods\nPOS 5737: Introduction to Data Analysis\nPOLS 209 at TAMU: Research Methods"
  },
  {
    "objectID": "teaching/pols-209/index.html",
    "href": "teaching/pols-209/index.html",
    "title": "POLS 209",
    "section": "",
    "text": "syllabus [pdf]\nassigned exercises from FPP [pdf]\ndata sets [zip]\n\nWriting Assignment 1 [Dropbox]\n\nRubric [Dropbox]\n\nWriting Assignment 2 [Dropbox] [checklist]\n\n\n\nAllen 2051, MW, 11am-12pm. Please reserve your slot here.\n\n\n\nTasks (bullets) below the date should be completed before the next class (unless a due-date is listed).\nAugust 30: Introduction\n. Read the syllabus carefully.\n. Install R and RStudio (complete by Sep 6).\n. Order textbook (have by Sep 11).\n. Obtain a pocket calculator (have by Sep 11).\nSep 1: Questions\n. Review Notes on Questions [pdf]. Complete exercises.\nSep 4: Models\n. Review Notes on Models [pdf] (and slides from lecture [pdf]). Complete exercises.\n. Install R and RStudio.\nSep 6: Model-Building Exercise\nSep 8: Computing in R\nSep 11: Computing in R, part 2\n. Read Notes on Computing in R [pdf]. Complete exercises.\nSep 13: Loading Data in R . Review your notes from my Lecture on Data Frames [pdf].\n. Review Notes on Loading Data in R [pdf]. Complete exercises.\n. Complete Computing Assignment 1 [pdf].\nSep 15: Causal Inference and Histograms\n. Have textbook and calculator.\n. Read chs. 1-2 of FPP. Complete assigned exercises (remember that assigned exercises are [at the top]).\n. Review your notes from my Lecture on Causal Inference [pdf]. Complete exercises at the end.\n. Read ch. 3 of FPP. Complete assigned exercises.\nSep 18: Histograms in R\n. Review Notes on Histograms in R [pdf]. Complete exercises.\n. Begin to work on Computing Assignment 2 [pdf] (due Sep 22).\nSep 20: Average and SD\n. Read ch. 4 of FPP. Complete assigned exercises.\n. Submit Computing Assignment 2 [pdf].\nSep 22: Catch-Up Day\n. Begin working on Writing Assignment 1 [Dropbox]. Come prepared with questions.\nSep 25: Average and SD in R\n. Review your notes from my Lecture on Average and SD in R [pdf].\n. Review Notes on Average and SD in R [pdf]. Complete exercises.\n. Begin working on Computing Assignment 3 [pdf].\nSep 27: Normal Approximation\n. Read ch. 5 of FPP. Complete assigned exercises.\n. Finish the leadership extremity exercise we began in class [pdf].\n. Submit Computing Assignment 3 [pdf].\nSep 29: Measurement\n. Read ch. 6 of FPP. Complete assigned exercises.\n. Prepare for Exam 1. Focus on the review exercises from notes, slides, and textbook.\nOct 2: Review for Exam 1\n. Study Guide [Google Doc]\nOct 4: Exam 1 (bring pencil, pocket calculator, and small green Scantron)\n. Read “Politics and the English Language” [pdf]. Expect a reading quiz.\nOct 6: Discussion of “Politics and the English Language”Scatterplots and Correlation, Part 1\n. Read chs. 7-8 of FPP. Complete assigned exercises.\n. See this sheet for p. 137, #9(a) [Google Sheet].\nOct 9: Measurement, Part 2\n. Review your notes from my Lecture on Measurement [pdf]. Complete exercises at the end.\nOct 11: Scatterplots and Correlation in R\n. Review Notes on Scatterplots and Correlation in R [pdf]. Complete exercises.\n. Read ch. 9 of FPP. Complete assigned exercises.\n. Play this game [web] and track your performance.\n. Don’t forget to submit Writing Assignment 1.\nOct 13: Regression, Part 1\n. Read ch. 10 of FPP. Complete assigned exercises.\nOct 16: Regression, Part 2\n. Read ch. 11 of FPP. Complete assigned exercises.\n. Note that I accidentally assigned ch. 11 on the 13th as well. I meant to assign ch. 10. Make sure you’ve finished both ch. 10 and 11.\n. Begin Computing Assignment 4 [pdf].\nOct 18: Regression, Part 3\n. Read ch. 12 of FPP. Complete assigned exercises.\n. Submit Computing Assignment 4 [pdf].\n. Submit peer review for Writing Assignment 1. Details on eCampus.\nOct 20: Regression in R\n. Review Notes on Regression in R [pdf]. Complete exercises.\nOct 23: Multiple Regression, Part 1\nOct 25: Multiple Regression, Part 2\n. Review your notes on my lecture on econometric notation. Make sure you can explain the similarities and differences between FPP’s simple notation and the more complicated econometric notation. What are the two advantages of econometric notation?\n. Review your notes on my lecture [pdf] on regression for prediction.\n. Read these notes [pdf] for more detail on prediction and BIC.\n. Complete Computing Assignment 5 [pdf].\n. Read “5 Steps toward Constructing a Better Sentence” [web].\n. Read “5 Steps toward Writing an Effective Paragraph” [web].\n. Use this example response memo [pdf] when writing your own response memo.\nOct 27: “Breakfast with Ben”\nOct 30: Exam 2 Review\n. Study Guide [Google Doc]\nNov 1: Exam 2 (bring pencil, pocket calculator, and small green Scantron)\n. Final submission of Writing Assignment 1.\nNov 3: Probability, Part 1\n. We’ll look at the Federalist papers [web] in class.\n. Read ch. 13 of FPP. Complete assigned exercises.\nNov 6: Probability, Part 2\n. Read ch. 14 of FPP. Complete assigned exercises.\n. Submit peer review by noon on July 26.\nNov 8: Law of Averages\n. Read ch. 16 of FPP. Complete assigned exercises.\nNov 10: Expected Value and Standard Error\n. Read ch. 17 of FPP. Complete assigned exercises.\n. Begin Writing Assignment 2 [Dropbox]\nNov 13: Normal Approximation for Probability Histograms\n. Read ch. 18 of FPP. Complete assigned exercises. . In class, fill in this table [Google Sheet].\n. In class, use these slides as needed [Google Slides].\nNov 15: Sample Surveys, Part 1\n. Read ch. 19 of FPP. Complete assigned exercises.\nNov 17: Sample Surveys, Part 2\n. Read ch. 20 of FPP. Complete assigned exercises.\nNov 20: FSAB Panel Day\n. Catch-up on any review exercises you haven’t done.\n. Writing Assignment 2 due (postponed to Tuesday, Nov. 28).\nNov 22: No Class (Reading Day)\nNov 24: No Class (Thanksgiving Holiday)\nNov 27: Catch-up Day\n. Make sure you’ve read through ch. 20 of FPP and completed assigned exercises.\nNov 29: The Accuracy of Percentages\n. Read ch. 21 of FPP. Complete assigned exercises.\nDec 1: The Accuracy of Averages\n. Read ch. 23 of FPP. Complete assigned exercises.\nDec 4: Hypothesis Tests . Read ch. 26 of FPP. Complete assigned exercises.\nDec 6: Final Exam Review\n. Example problem for hypothesis test and 95% CI for percent [Dropbox].\n. Final submission of Writing Assignment 2 due.\n. Study Guide [Google Doc]\nDec 8 or 11: Final Exam (bring pencil, pocket calculator, and small green Scantron) . For 901 (8:35-9:25am), 10am-12pm on Dec 8\n. For 902 (9:45-10:35am), 8-10am on Dec 11"
  },
  {
    "objectID": "teaching/pols-209/index.html#office-hours",
    "href": "teaching/pols-209/index.html#office-hours",
    "title": "POLS 209",
    "section": "",
    "text": "Allen 2051, MW, 11am-12pm. Please reserve your slot here."
  },
  {
    "objectID": "teaching/pols-209/index.html#schedule",
    "href": "teaching/pols-209/index.html#schedule",
    "title": "POLS 209",
    "section": "",
    "text": "Tasks (bullets) below the date should be completed before the next class (unless a due-date is listed).\nAugust 30: Introduction\n. Read the syllabus carefully.\n. Install R and RStudio (complete by Sep 6).\n. Order textbook (have by Sep 11).\n. Obtain a pocket calculator (have by Sep 11).\nSep 1: Questions\n. Review Notes on Questions [pdf]. Complete exercises.\nSep 4: Models\n. Review Notes on Models [pdf] (and slides from lecture [pdf]). Complete exercises.\n. Install R and RStudio.\nSep 6: Model-Building Exercise\nSep 8: Computing in R\nSep 11: Computing in R, part 2\n. Read Notes on Computing in R [pdf]. Complete exercises.\nSep 13: Loading Data in R . Review your notes from my Lecture on Data Frames [pdf].\n. Review Notes on Loading Data in R [pdf]. Complete exercises.\n. Complete Computing Assignment 1 [pdf].\nSep 15: Causal Inference and Histograms\n. Have textbook and calculator.\n. Read chs. 1-2 of FPP. Complete assigned exercises (remember that assigned exercises are [at the top]).\n. Review your notes from my Lecture on Causal Inference [pdf]. Complete exercises at the end.\n. Read ch. 3 of FPP. Complete assigned exercises.\nSep 18: Histograms in R\n. Review Notes on Histograms in R [pdf]. Complete exercises.\n. Begin to work on Computing Assignment 2 [pdf] (due Sep 22).\nSep 20: Average and SD\n. Read ch. 4 of FPP. Complete assigned exercises.\n. Submit Computing Assignment 2 [pdf].\nSep 22: Catch-Up Day\n. Begin working on Writing Assignment 1 [Dropbox]. Come prepared with questions.\nSep 25: Average and SD in R\n. Review your notes from my Lecture on Average and SD in R [pdf].\n. Review Notes on Average and SD in R [pdf]. Complete exercises.\n. Begin working on Computing Assignment 3 [pdf].\nSep 27: Normal Approximation\n. Read ch. 5 of FPP. Complete assigned exercises.\n. Finish the leadership extremity exercise we began in class [pdf].\n. Submit Computing Assignment 3 [pdf].\nSep 29: Measurement\n. Read ch. 6 of FPP. Complete assigned exercises.\n. Prepare for Exam 1. Focus on the review exercises from notes, slides, and textbook.\nOct 2: Review for Exam 1\n. Study Guide [Google Doc]\nOct 4: Exam 1 (bring pencil, pocket calculator, and small green Scantron)\n. Read “Politics and the English Language” [pdf]. Expect a reading quiz.\nOct 6: Discussion of “Politics and the English Language”Scatterplots and Correlation, Part 1\n. Read chs. 7-8 of FPP. Complete assigned exercises.\n. See this sheet for p. 137, #9(a) [Google Sheet].\nOct 9: Measurement, Part 2\n. Review your notes from my Lecture on Measurement [pdf]. Complete exercises at the end.\nOct 11: Scatterplots and Correlation in R\n. Review Notes on Scatterplots and Correlation in R [pdf]. Complete exercises.\n. Read ch. 9 of FPP. Complete assigned exercises.\n. Play this game [web] and track your performance.\n. Don’t forget to submit Writing Assignment 1.\nOct 13: Regression, Part 1\n. Read ch. 10 of FPP. Complete assigned exercises.\nOct 16: Regression, Part 2\n. Read ch. 11 of FPP. Complete assigned exercises.\n. Note that I accidentally assigned ch. 11 on the 13th as well. I meant to assign ch. 10. Make sure you’ve finished both ch. 10 and 11.\n. Begin Computing Assignment 4 [pdf].\nOct 18: Regression, Part 3\n. Read ch. 12 of FPP. Complete assigned exercises.\n. Submit Computing Assignment 4 [pdf].\n. Submit peer review for Writing Assignment 1. Details on eCampus.\nOct 20: Regression in R\n. Review Notes on Regression in R [pdf]. Complete exercises.\nOct 23: Multiple Regression, Part 1\nOct 25: Multiple Regression, Part 2\n. Review your notes on my lecture on econometric notation. Make sure you can explain the similarities and differences between FPP’s simple notation and the more complicated econometric notation. What are the two advantages of econometric notation?\n. Review your notes on my lecture [pdf] on regression for prediction.\n. Read these notes [pdf] for more detail on prediction and BIC.\n. Complete Computing Assignment 5 [pdf].\n. Read “5 Steps toward Constructing a Better Sentence” [web].\n. Read “5 Steps toward Writing an Effective Paragraph” [web].\n. Use this example response memo [pdf] when writing your own response memo.\nOct 27: “Breakfast with Ben”\nOct 30: Exam 2 Review\n. Study Guide [Google Doc]\nNov 1: Exam 2 (bring pencil, pocket calculator, and small green Scantron)\n. Final submission of Writing Assignment 1.\nNov 3: Probability, Part 1\n. We’ll look at the Federalist papers [web] in class.\n. Read ch. 13 of FPP. Complete assigned exercises.\nNov 6: Probability, Part 2\n. Read ch. 14 of FPP. Complete assigned exercises.\n. Submit peer review by noon on July 26.\nNov 8: Law of Averages\n. Read ch. 16 of FPP. Complete assigned exercises.\nNov 10: Expected Value and Standard Error\n. Read ch. 17 of FPP. Complete assigned exercises.\n. Begin Writing Assignment 2 [Dropbox]\nNov 13: Normal Approximation for Probability Histograms\n. Read ch. 18 of FPP. Complete assigned exercises. . In class, fill in this table [Google Sheet].\n. In class, use these slides as needed [Google Slides].\nNov 15: Sample Surveys, Part 1\n. Read ch. 19 of FPP. Complete assigned exercises.\nNov 17: Sample Surveys, Part 2\n. Read ch. 20 of FPP. Complete assigned exercises.\nNov 20: FSAB Panel Day\n. Catch-up on any review exercises you haven’t done.\n. Writing Assignment 2 due (postponed to Tuesday, Nov. 28).\nNov 22: No Class (Reading Day)\nNov 24: No Class (Thanksgiving Holiday)\nNov 27: Catch-up Day\n. Make sure you’ve read through ch. 20 of FPP and completed assigned exercises.\nNov 29: The Accuracy of Percentages\n. Read ch. 21 of FPP. Complete assigned exercises.\nDec 1: The Accuracy of Averages\n. Read ch. 23 of FPP. Complete assigned exercises.\nDec 4: Hypothesis Tests . Read ch. 26 of FPP. Complete assigned exercises.\nDec 6: Final Exam Review\n. Example problem for hypothesis test and 95% CI for percent [Dropbox].\n. Final submission of Writing Assignment 2 due.\n. Study Guide [Google Doc]\nDec 8 or 11: Final Exam (bring pencil, pocket calculator, and small green Scantron) . For 901 (8:35-9:25am), 10am-12pm on Dec 8\n. For 902 (9:45-10:35am), 8-10am on Dec 11"
  }
]