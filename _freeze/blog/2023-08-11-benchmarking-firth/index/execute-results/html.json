{
  "hash": "402cb87dbd8459a481734aa285fde628",
  "result": {
    "markdown": "---\ntitle: \"Benchmarking Firth's Logit: {brglm2} versus {logistf}\"\nsubtitle: \"logistf() is really fast\"\nauthor: \"Carlisle Rainey\"\ndate: \"2023-08-11\"\ncategories: [logistic regression, small samples, Firth, computing, R]\ndescription: \"In this post, I benchmark the brglm2 and logistf packages for fitting logistic regression models with Firth's penalty. \"\ntoc: false\ncode-annotations: hover\nwebsite:\n  twitter-card:\n    image: \"/profile-tw.png\"\n    card-style: \"summary\"\ndraft: false\nbibliography: references.bib\n---\n\n\n## Firth's Logit\n\nI like Firth's logistic regression model [@firth1993]. I talk about that in @rainey2021 and [this Twitter thread](https://twitter.com/carlislerainey/status/1686389777225113601). @kosmidis2021 offer an excellent, recent follow-up as well.\n\nI'll refer you to the papers for a careful discussion of the benefits, but Firth's penalty reduces the bias *and variance* of the logit coefficients.\n\n## Goals for Benchmarking\n\nIn this post, I want to compare the brglm2 and logistf packages. Which fits logistic regression models with Firth's penalty the fastest?\n\nThese packages both fit the models almost instantly, so there is no practical difference when fitting just one model. But large Monte Carlo simulations (or perhaps bootstraps), small differences might add up to a substantial time difference.\n\nHere, I benchmark the two packages for fitting logistic regression models with Firth's penalty in a *small sample*--the results might not generalize to a larger sample. The data set comes from @weisiger2014 (see `?crdata::weisiger2014`). It has only 35 observations.\n\nYou can find the benchmarking code as a [GitHub Gist](https://gist.github.com/carlislerainey/8bf23a322252bead64d0a07391f7383d).\n\n## Benchmarking\n\nI benchmark four methods here.\n\n1. A vanilla `glm()` logit model.\n1. A Firth's logit via brglm2 by supplying `method = brglm2::brglmFit` to `glm()`.\n1. A Firth's logit via logistf via `logistf()` using the default settings. \n1. A Firth's logit via logistf via `logistf()` with the argument `pl = FALSE`. This argument is important because it skips hypothesis testing using profile likelihoods, which are computationally costly.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install crdata package to egt weisiger2014 data set\nremotes::install_github(\"carlislerainey/crdata\")\n\n# load packages\nlibrary(tidyverse)\nlibrary(brglm2)\nlibrary(logistf)\nlibrary(microbenchmark)\n\n\n# load data\nweis <- crdata::weisiger2014\n\n# rescale weisiger2014 explanatory variables using arm::rescale()\nrs_weis <- weis %>%\n  mutate(across(polity_conq:coord, arm::rescale)) \n\n# create functions to fit models\nf <- resist ~ polity_conq + lndist + terrain + soldperterr + gdppc2 + coord\nf1 <- function() {\n  glm(f, data = rs_weis, family = \"binomial\")\n}\nf2 <- function() {\n  glm(f, data = rs_weis, family = \"binomial\", method = brglmFit)\n}\nf3 <- function() {\n  logistf(f, data = rs_weis)\n}\nf4 <- function() {\n  logistf(f, data = rs_weis, pl = FALSE,\n          control= logistf.control(lconv=1e-06))\n}\n\n# do benchmarking\nbm <- microbenchmark(\"regular glm()\" = f1(), \n               \"brglm2\" = f2(), \n               \"logistf (default)\" = f3(),\n               \"logistf (w/ pl = FALSE)\" = f4(),\n               times = 100) \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in microbenchmark(`regular glm()` = f1(), brglm2 = f2(), `logistf\n(default)` = f3(), : less accurate nanosecond times to avoid potential integer\noverflows\n```\n:::\n\n```{.r .cell-code}\nprint(bm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUnit: microseconds\n                    expr      min       lq      mean    median        uq\n           regular glm()  500.446  536.198  559.1264  548.6005  566.9685\n                  brglm2 2412.768 2489.520 2598.0650 2528.2445 2574.1440\n       logistf (default) 3291.152 3366.162 3863.3787 3414.3775 3497.4230\n logistf (w/ pl = FALSE)  522.668  549.605  639.5291  563.2170  578.5920\n       max neval\n  1056.980   100\n  7455.153   100\n 12975.434   100\n  7377.663   100\n```\n:::\n:::\n\n\nIn short, logistf is slower than brglm2, but only because it computes the profile likelihood *p*-values by default. Once we skip those calculations using `pl = FALSE`, logistf is *much* faster. On average, it's faster than `glm()`, because `glm()` has the occasional *really* slow computation. \n\nHere's a plot showing the computation times of the four fits. Remember that all of these are computed practically instantly, so it only makes a difference when the fits are done thousands of times, like in a Monte Carlo simulation. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot times\nbm %>%\n  group_by(expr) %>%\n  summarize(avg_time = mean(time)*10e-5) %>%  # convert to milliseconds\n  ggplot(aes(x = fct_rev(expr), y = avg_time)) + \n  geom_col() + \n  labs(x = \"Method\", \n       y = \"Avg. Time (in milliseconds)\") + \n  coord_flip()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n# Follow-Up Notes\n\nThe models return *slightly* different estimates. Maybe they are using slightly different convergence tolerances. I didn't investigate this beyond noticing it.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncbind(coef(f2()), coef(f4()))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                  [,1]       [,2]\n(Intercept) -0.4771934 -0.4771935\npolity_conq -2.2771109 -2.2771117\nlndist       3.4020241  3.4020215\nterrain      1.1018696  1.1018713\nsoldperterr -0.5952096 -0.5952110\ngdppc2      -1.1542010 -1.1541998\ncoord        3.0514481  3.0514473\n```\n:::\n:::\n\n\nIoannis Kosmidis made me aware of two things. \n\n1. logistf has a C++ backend (thus explaining the speed).\n1. brglm2 is written entirely in R. He noted that a new version is coming out soon that might be substantially faster. (brglm2 is also more general; it supports a variety of models and corrections).\n\n## Computer\n\nHere's the info on my machine. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsystem(\"sysctl -n machdep.cpu.brand_string\", intern = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Apple M2 Max\"\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}