{
  "hash": "4bf56f0bf0d3db7cc992b6db008c008a",
  "result": {
    "markdown": "---\ntitle: \"Benchmarking Firth's Logit: {brglm2} versus {logistf}\"\nsubtitle: \"logistf() is really fast\"\nauthor: \"Carlisle Rainey\"\ndate: \"2023-08-11\"\ncategories: [logistic regression, small samples, Firth, computing, R]\ndescription: \"In this post, I benchmark the brglm2 and logistf packages for fitting logistic regression models with Firth's penalty. \"\ntoc: false\ncode-annotations: hover\ndraft: false\nbibliography: references.bib\n---\n\n\n## Firth's Logit\n\nI like Firth's logistic regression model [@firth1993]. I talk about that in @rainey2021 and [this Twitter thread](https://twitter.com/carlislerainey/status/1686389777225113601). @kosmidis2021 offer an excellent, recent follow-up as well.\n\nI'll refer you to the papers for a careful discussion of the benefits, but Firth's penalty reduces the bias *and variance* of the logit coefficients.\n\n## Goals for Benchmarking\n\nIn this post, I want to compare the brglm2 and logistf packages. Which fits logistic regression models with Firth's penalty the fastest?\n\nThese packages both fit the models almost instantly, so there is no practical difference when fitting just one model. But large Monte Carlo simulations (or perhaps bootstraps), small differences might add up to a substantial time difference.\n\nHere, I benchmark the two packages for fitting logistic regression models with Firth's penalty in a *small sample*--the results might not generalize to a larger sample. The data set comes from @weisiger2014 (see `?crdata::weisiger2014`). It has only 35 observations.\n\nYou can find the benchmarking code as a [GitHub Gist](https://gist.github.com/carlislerainey/8bf23a322252bead64d0a07391f7383d).\n\n## Computer\n\nHere's the info on my machine. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsystem(\"sysctl -n machdep.cpu.brand_string\", intern = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Apple M2 Max\"\n```\n:::\n:::\n\n\n## Benchmarking\n\nI benchmark four methods here.\n\n1. A vanilla `glm()` logit model.\n1. A Firth's logit via brglm2 by supplying `method = brglm2::brglmFit` to `glm()`.\n1. A Firth's logit via logistf via `logistf()` using the default settings. \n1. A Firth's logit via logistf via `logistf()` with the argument `pl = FALSE`. This argument is important because it skips hypothesis testing using profile likelihoods, which are computationally costly.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install crdata package to egt weisiger2014 data set\nremotes::install_github(\"carlislerainey/crdata\")\n\n# load packages\nlibrary(tidyverse)\nlibrary(brglm2)\nlibrary(logistf)\nlibrary(microbenchmark)\n\n\n# load data\nweis <- crdata::weisiger2014\n\n# rescale weisiger2014 explanatory variables using arm::rescale()\nrs_weis <- weis %>%\n  mutate(across(polity_conq:coord, arm::rescale)) \n\n# create functions to fit models\nf <- resist ~ polity_conq + lndist + terrain + soldperterr + gdppc2 + coord\nf1 <- function() {\n  glm(f, data = rs_weis, family = \"binomial\")\n}\nf2 <- function() {\n  glm(f, data = rs_weis, family = \"binomial\", method = brglmFit)\n}\nf3 <- function() {\n  logistf(f, data = rs_weis)\n}\nf4 <- function() {\n  logistf(f, data = rs_weis, pl = FALSE,\n          control= logistf.control(lconv=1e-06))\n}\n\n# do benchmarking\nbm <- microbenchmark(\"regular glm()\" = f1(), \n               \"brglm2\" = f2(), \n               \"logistf (default)\" = f3(),\n               \"logistf (w/ pl = FALSE)\" = f4(),\n               times = 100) \n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in microbenchmark(`regular glm()` = f1(), brglm2 = f2(), `logistf\n(default)` = f3(), : less accurate nanosecond times to avoid potential integer\noverflows\n```\n:::\n\n```{.r .cell-code}\nprint(bm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUnit: microseconds\n                    expr      min       lq      mean   median        uq\n           regular glm()  509.589  536.936  579.8331  549.523  564.2625\n                  brglm2 2422.731 2494.010 2676.4173 2525.662 2620.5765\n       logistf (default) 3226.495 3333.382 3775.0389 3396.215 3466.6730\n logistf (w/ pl = FALSE)  517.297  547.719  573.9143  562.438  583.4300\n       max neval\n  2536.383   100\n 10647.003   100\n 13501.218   100\n  1144.925   100\n```\n:::\n:::\n\n\nIn short, logistf is slower than brglm2, but only because it computes the profile likelihood *p*-values by default. Once we skip those calculations using `pl = FALSE`, logistf is *much* faster. On average, it's faster than `glm()`, because `glm()` has the occasional *really* slow computation. \n\nHere's a plot showing the computation times of the four fits. Remember that all of these are computed practically instantly, so it only makes a difference when the fits are done thousands of times, like in a Monte Carlo simulation. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot times\nbm %>%\n  group_by(expr) %>%\n  summarize(avg_time = mean(time)*10e-5) %>%  # convert to milliseconds\n  ggplot(aes(x = fct_rev(expr), y = avg_time)) + \n  geom_col() + \n  labs(x = \"Method\", \n       y = \"Avg. Time (in milliseconds)\") + \n  coord_flip()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n# Follow-Up Notes\n\nIoannis Kosmidis made me aware of two things. \n\n1. logistf has a C++ backend (thus explaining the speed).\n1. brglm2 is written entirely in R. He noted that a new version is coming out soon that might be substantially faster. (brglm2 is also more general; it supports a variety of models and corrections).\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}