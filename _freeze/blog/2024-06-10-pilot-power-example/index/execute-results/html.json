{
  "hash": "f5f81d904d3dcdb18d892d08bfd8dc26",
  "result": {
    "markdown": "---\ntitle: \"Statistical Power from Pilot Data: An Example\"\nauthor: \"Carlisle Rainey\"\ndate: \"2024-06-10\"\ncategories: [statistical power, hypothesis tests, power analysis, methodology]\ndescription: \"In this post, I provide an example of how pilot data can be used to predict the standard error in a planned study.\"\nreference-location: margin\ntoc: false\ntwitter-card:\n  card-style: summary_large_image\n  image: \"twitter-card.png\"\nopen-graph:   \n  image: \"twitter-card.png\"\nbibliography: references.bib\ndraft: false\n---\n\n\n\n\n------------------------------------------------------------------------\n\nWe can think of statistical power as determined by the ratio $\\frac{\\tau}{SE}$, where $\\tau$ is the treatment effect and SE is the standard error of the estimate. To reason about statistical power, one needs to make assumptions or predictions about the treatment effect and the standard error.\n\nAnd as data-oriented researchers, we often want to use data to inform these predictions and assumptions. We might want to use *pilot* data.[^1]\n\n[^1]: Here's how @leon2011 describe the purpose of a pilot study. \"The fundamental purpose of conducting a pilot study is to examine the feasibility of an approach that is intended to ultimately be used in a larger scale study. This applies to all types of research studies. Here we use the randomized controlled clinical trial (RCT) for illustration. Prior to initiating a full scale RCT an investigator may choose to conduct a pilot study in order to evaluate the feasibility of recruitment, randomization, retention, assessment procedures, new methods, and/or implementation of the novel intervention. A pilot study, however, is not used for hypothesis testing. Instead it serves as an earlier-phase developmental function that will enhance the probability of success in the larger subsequent RCTs that are anticipated.\"\n\nUsually:\n\n1.  Pilot data are *not* useful to predict the treatment effect.\n2.  Pilot data are useful to predict the standard error.\n\nWith a predicted standard error in hand, we can predict the minimum detectable effect, the statistical power, or the required sample size in the planned study.\n\nIn this post, I give an example of how this can work.\n\n## Predicting the SE from pilot data\n\nHere's how I suggest we use pilot data to predict the standard error in the planned study:\n\n::: callout-tip\n## Predicting the SE in the planned study using pilot data\n\nConservatively, the standard error will be about $\\sqrt{\\frac{n^{pilot}}{n^{planned}}} \\cdot \\left\\lbrack \\left( \\sqrt{\\frac{1}{n^{pilot}}} + 1 \\right) \\cdot {\\widehat{SE}}_{\\widehat{\\tau}}^{pilot} \\right\\rbrack$, where $n^{pilot}$ is the number of respondents per condition in the pilot data, $SE_{\\widehat{\\tau}}^{pilot}$ is the estimated standard error using the pilot data, and $n^{planned}$ is the number of respondents per condition in the planned study.\n:::\n\nThe factor $\\left( \\sqrt{\\frac{1}{n^{pilot}}} + 1 \\right)$ nudges the standard error from the pilot study in a conservative direction, since it might be an under-estimate of the actual standard error.[^2] For the details, see [this early paper](https://github.com/carlislerainey/power-rules/blob/main/power-rules.pdf), but this conservative standard error estimate is approximately the upper bound of a 95% confidence interval for the *standard error* using the pilot data.\n\n[^2]: More generally, we can use a bootstrap to conservatively estimate the standard error, without relying on this analytical approximation.\n\n## Example\n\n### The Robbins et al. study\n\nAs an example, let's use half of the experiment conducted by @robbins2024.\n\nRobbins et al. use a 2x2 factorial vignette design, randomly assigning each respondent to read one of four vignettes. The vignette describes a hypothetical covert operation ordered by the president that ends in either success or failure. Then, the vignette describes a whistleblower coming forward and describes the president's opposition in Congress as either amplifying or ignoring the whistleblower.\n\n\n```{=html}\n<!DOCTYPE html>\n<html>\n<head>\n    <style>\n        table {\n            width: 100%;\n            border-collapse: collapse;\n        }\n        table, th, td {\n            border: 1px solid black;\n        }\n        th, td {\n            padding: 15px;\n            text-align: center;\n        }\n        th {\n            background-color: #f2f2f2;\n        }\n    </style>\n</head>\n<body>\n\n<table>\n    <thead>\n        <tr>\n            <th>President's Opposition in Congress</th>\n            <th colspan=\"2\">Outcome of Operation</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <th></th>\n            <th>Success</th>\n            <th>Failure</th>\n        </tr>\n        <tr>\n            <td>Amplifies Whistleblower</td>\n            <td>Vignette 1: Success & Amplify</td>\n            <td>Vignette 2: Failure & Amplify</td>\n        </tr>\n        <tr>\n            <td>Ignores Whistleblower</td>\n            <td>Vignette 3: Success & Ignore</td>\n            <td>Vignette 4: Failure & Ignore</td>\n        </tr>\n    </tbody>\n</table>\n\n</body>\n</html>\n```\n\nAfter the vignette, the respondent is asked whether they approve of the opposition in Congress' actions on a seven-point Likert scale from strongly approve to strongly disapprove.\n\nFor a simple example, let's **focus on the effect of amplifying the whistleblower when the operation succeeds**. That is, let's compare responses after Vignette 1 and Vignette 3. How much does amplifying a whistleblower increase approval *when the opperation succeeds*? We expect a small effect here, so we should pay careful attention to power.\n\n## The task\n\n**We hoped to detect an effect as small as 0.35 points on the seven-point scale and had tentatively planned on 250 respondents per condition.** To test the survey instrument and data provider, we conducted a small pilot with about 75 respondents per condition. Let's use those pilot data to check whether 250 respondents seem sufficient.\n\n## The data\n\nIn the [{crdata} package](https://www.carlislerainey.com/crdata/) on GitHub, you can find the the pilot data we collected leading up to the main study.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# download the {crdata} package from github\nremotes::install_github(\"carlislerainey/crdata\")\n```\n:::\n\n\nNow let's load the pilot data. To focus on observations where the operation succeeds, we're going to keep only the observations where the vignette describes a successful observation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load pilot data and keep only success condition\nrobbins2_pilot <- crdata::robbins_pilot |>\n  subset(failure == \"Success\") |>\n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 147\nColumns: 5\n$ cong_overall <dbl> 3, 1, -2, 0, -2, -1, 0, -1, 0, 0, 0, 2, -1, 3, -3, 0, 0, …\n$ failure      <fct> Success, Success, Success, Success, Success, Success, Suc…\n$ amplify      <fct> Ignore, Ignore, Ignore, Amplify, Ignore, Ignore, Ignore, …\n$ pid7         <fct> Strong Democrat, Not very strong Republican, Strong Democ…\n$ pid_strength <dbl> 3, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 1, 0, 3, 3, 3, 3, 1, 1, …\n```\n:::\n:::\n\n\n`cong_overall` is the respondent's approval of Congress' actions on a seven-point scale and `amplify` indicates whether Congress amplified the whistleblower (i.e., criticized the president).\n\n### Analyzing the pilot data\n\nNow let's analyze the pilot data as we plan to analyze the main data set that we plan to collect later. We're interested in the average response in the `Amplify` and `Ignore` conditions, so let's use a t-test.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# t test\nfit_pilot <- t.test(cong_overall ~ amplify, data = robbins2_pilot)\n```\n:::\n\n\n::: callout-caution\n## Ignore the Estimated Treatment Effect\n\nIt can be really tempting to look at the estimated treatment effect. In this pilot study, it's *actually statistically significant.* I intentionally don't show the estimated treatment effect (or quantities requiring it, like *p*-values). If we looked at these, we might make one of the following mistakes:\n\n1.   \"The pilot got significant results, therefore even the pilot is sufficiently powered.\"\n2.  \"The estimate from the pilot is significant, therefore we can use the estimated treatment effect in the power analysis.\"\n\nBoth of these claims are misleading. The estimated treatment effect is very noisy, so ignore the estimated treatment effect.\n\n:::\n\n### Predicting the SE in the main study\n\nTo predict the standard error in the main study, we need two pieces of information from this pilot:\n\n1.  the sample size per condition and\n2.  the estimated standard error.\n\nWe can get the number of observations per condition using `table()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create a table showing the observations per condition\ntable(robbins2_pilot$amplify)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n Ignore Amplify \n     70      77 \n```\n:::\n\n```{.r .cell-code}\n# sample size per condition\nn_pilot <- mean(table(robbins2_pilot$amplify))\nn_pilot\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 73.5\n```\n:::\n:::\n\n\nAnd then we need the estimated standard error, which is computed by `t.test()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# get estimated standard error from pilot\nse_hat_pilot <- fit_pilot$stderr\nse_hat_pilot\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.2761011\n```\n:::\n:::\n\n\nNow we can predict the standard error in the planned study.\n\nFor the main study, we planned on about 250 respondents per condition.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn_planned <- 250\n```\n:::\n\n\nThe we can conservatively predict the standard error in the full study as $\\sqrt{\\frac{n^{pilot}}{n^{planned}}} \\cdot \\left\\lbrack \\left( \\sqrt{\\frac{1}{n^{pilot}}} + 1 \\right) \\cdot {\\widehat{SE}}_{\\widehat{\\tau}}^{pilot} \\right\\rbrack$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_se_cons <- sqrt(n_pilot/n_planned)*((sqrt(1/n_pilot) + 1)*se_hat_pilot)\npred_se_cons\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1671691\n```\n:::\n:::\n\n\nBut is this standard error small enough?\n\n### Evaluating the predicted SE in the main study\n\nWe can convert the standard error to the minimum detectable effect with 80% power using $2.5 \\times SE$.[^3]\n\n[^3]: See @bloom1995 for an excellent discussion of this rule. I also write about it [here](https://www.carlislerainey.com/blog/2023-06-12-power-3-rule-of-364/).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# compute conservative minimum detectable effect\n2.5*pred_se_cons\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.4179227\n```\n:::\n:::\n\n\nWe hoped to detect an effect as small as 0.35 points on the seven-point scale, so we're going to need more than 250 respondents per condition!\n\nWe can also compute the power to detect an effect of 0.35 points on the seven-point scale.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# compute power as a percent\n1 - pnorm(1.64 - 0.35/pred_se_cons)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.6749736\n```\n:::\n:::\n\n\nNote that these are *conservative* estimates of the minimum detectable effect and statistical power.\n\nHere's what things look like if we remove the conservative nudge $\\left( \\sqrt{\\frac{1}{n^{pilot}}} + 1 \\right)$ and predict the standard error as $\\sqrt{\\frac{n^{pilot}}{n^{planned}}} \\cdot {\\widehat{SE}}_{\\widehat{\\tau}}^{pilot}$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# without the conservative nudge\npred_se <- sqrt(n_pilot/n_planned)*se_hat_pilot\n2.5*pred_se  # best guess of minimum detectable effect\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3742673\n```\n:::\n\n```{.r .cell-code}\n1 - pnorm(1.64 - 0.35/pred_se)  # best guess of power\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.7573806\n```\n:::\n:::\n\n\nAs you can see, the minimum detectable effect and power are a little too low. We need more respondents!\n\n### Adjusting the Sample Size\n\nOur plan of 250 respondents per condition seems too low. If we want, we can predict the sample size we need to get to 80% power using the following rule:\n\n::: callout-tip\n## Predicting the required sample size in the planned study using pilot data\n\nFor 80% power to detect the treatment effect $\\widetilde{\\tau}$, we will (conservatively) need about $n^{pilot} \\cdot \\left\\lbrack \\frac{2.5}{\\widetilde{\\tau}} \\cdot \\left( \\sqrt{\\frac{1}{n^{pilot}}} + 1 \\right) \\cdot {\\widehat{SE}}_{\\widehat{\\tau}}^{pilot} \\right\\rbrack^{2}$ respondents per condition, where $n^{pilot}$ is the number of respondents per condition in the pilot data and $SE_{\\widehat{\\tau}}^{pilot}$ is the estimated standard error using the pilot data.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn_pilot*((2.5/0.35)*((sqrt(1/n_pilot) + 1)*se_hat_pilot))^2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 356.4477\n```\n:::\n:::\n\n\nThus to get 80% power, the pilot data suggest that we (conservatively) need about 360 respondents per condition. We used 367 in the full study. Here are the conservative predictions for 367 respondents per condition.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn_planned <- 367\npred_se_cons <- sqrt(n_pilot/n_planned)*((sqrt(1/n_pilot) + 1)*se_hat_pilot)\npred_se_cons\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1379726\n```\n:::\n\n```{.r .cell-code}\n2.5*pred_se_cons  # conservative minimum detectable effect\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3449315\n```\n:::\n\n```{.r .cell-code}\n1 - pnorm(1.64 - 0.35/pred_se_cons)  # conservative power\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8150699\n```\n:::\n:::\n\n\n## How did we do?\n\nWe ran the full study.[^4]\n\n[^4]: See @robbins2024 for the full results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrobbins2_main <- crdata::robbins_main |>\n  subset(failure == \"Success\") |>\n  glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 735\nColumns: 5\n$ cong_overall <dbl> 2, -2, -1, -3, 0, -1, -2, -1, 1, 1, 0, -3, -2, 2, 2, -3, …\n$ failure      <fct> Success, Success, Success, Success, Success, Success, Suc…\n$ amplify      <fct> Ignore, Amplify, Amplify, Amplify, Ignore, Ignore, Amplif…\n$ pid7         <fct> Not very strong Republican, Not very strong Republican, S…\n$ pid_strength <dbl> 2, 2, 3, 3, 3, 2, 0, 2, 2, 1, 2, 0, 3, 1, 2, 3, 2, 3, 2, …\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_main <- t.test(cong_overall ~ amplify, data = robbins2_main)\nfit_main$stderr\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1322618\n```\n:::\n\n```{.r .cell-code}\n1 - pnorm(1.64 - 0.35/fit_main$stderr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8428563\n```\n:::\n:::\n\n\nAs you can see, the pilot data gave us a good, slightly conservative prediction. We conservatively predicted a standard error of 0.138 in the planned study and we estimated a standard error of 0.132 after running the study. We conservatively predicted our power would be about 82% to detected an effect of 0.35 on the seven-point scale, but after running the study, it seems like we had about 84% power.\n\n## A bootstrap alternative\n\nWe can also use the bootstrap as an alternative. There are a few ways one might approach it.\n\nHere's one:\n\n1.  Treat the pilot data as a population. Create a data set with the planned sample size by sampling with replacement from the pilot data.\n2.  Perform the planned analysis on each resampled data set.\n3.  Store the estimated standard error from each analysis.\n\nRepeat the process above many times. For each standard error estimate, compute the implied statistical power. This gives a distribution of power estimates. Find a value near the bottom of this distribution. The factor we used above---The factor $\\left( \\sqrt{\\frac{1}{n^{pilot}}} + 1 \\right)$---nudges the standard error to about the 2.5th percentile, so we can use that here, too.\n\n\n::: {.cell hash='index_cache/html/unnamed-chunk-16_20fedaa4129096dd9b515c1a1bc80fa3'}\n\n```{.r .cell-code}\n# number of bootstrap iterations\nn_bs <- 10000\n\nbs_se <- numeric(n_bs)  # a container\nfor (i in 1:n_bs){\n  # resample 367 observations from each condition\n  bs_data <- robbins2_main %>%\n    group_by(amplify) %>%\n    sample_n(size = 367, replace = TRUE)\n  # run planned analysis\n  bs_fit <- t.test(cong_overall ~ amplify, data = bs_data)\n  # grab se\n  bs_se[i] <- bs_fit$stderr\n}\n\n# compute 2.5th percentile of power to obtain conservative estimate\npwr <- 1 - pnorm(1.64 - 0.35/bs_se)\nquantile(pwr, probs = 0.025)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     2.5% \n0.8206062 \n```\n:::\n:::\n\n\nUsing the analytical approximation, we got 0.815 as a conservative estimate of power. The bootstrap gave us 0.820 as a conservative estimate. The actual power in the full study turned out to be about 0.843. (Remember, all of these power calculations are *power to detected an effect of 0.35 points on the seven-point scale.*)\n\n## The paper\n\nI have an [early draft of a paper](https://github.com/carlislerainey/power-rules/blob/main/power-rules.pdf) on these (and other) ideas. Please test them out in your own work and let me know if you have questions, comments, and suggestions. I'm interested in making the paper as clear and useful as I can.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}