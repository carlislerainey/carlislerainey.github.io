{
  "hash": "7abdbaf65f9b9428132c0ef41987f384",
  "result": {
    "markdown": "---\ntitle: \"Power, Part II: What Do Confidence Intervals from High-Powered Studies Look Like?\"\nsubtitle: \"Confidence Intervals Should Rarely Nestle Against Zero\"\nauthor: \"Carlisle Rainey\"\ndate: \"2023-05-25\"\ncategories: [statistical power, hypothesis tests, power analysis, methodology, confidence intervals, computing, R]\nreference-location: margin\ncode-fold: true\ntoc: false\ntwitter-card:\n  image: \"twitter-card.png\"\ndescription: \"Under-powered studies tend to produce confidence intervals that are nestled right up against zero. Well-powered studies tend to produce confidence intervals that fall further away. A literature that produces confidence intervals that consistently nestle right up against zero is likely a collection of under-powered studies.\"\ndraft: false\nformat: \n  html: \n    include-in-header: \n      - \"../../pop-up-footer.html\"\n---\n\n\n## Background\n\nIn this post, I address confidence intervals that are nestled right up against zero.^[This is the second post in a series. In my [previous post](blog/2023-05-22-power-1-for-you-not-reviewer-2), I mentioned two new papers that have me thinking about power: [Arel-Bundock *et al.*'s \"Quantitative Political Science Research Is Greatly Underpowered\"](https://doi.org/10.31219/osf.io/7vy2f) and [Kane's \"More Than Meets the ITT: A Guide for Investigating Null Results\"](https://doi.org/10.33774/apsa-2023-h4p0q-v2). Go check out that post and those papers if you haven't.] These intervals indicate that an estimate is \"barely\" significant. I want to be clear: \"barely significant\" is still significant, so you should still reject the null hypothesis.[^1]\n\n[^1]: I'm focusing on confidence intervals here because inference from confidence intervals is a bit more intuitive (see [Rainey 2014](http://www.carlislerainey.com/papers/nme.pdf) and [Rainey 2015](http://www.carlislerainey.com/papers/meaningful.pdf). In the cases I discuss, whether one checks whether the *p*-value is less than 0.05 or checks that confidence interval contains zero are equivalent.\n\nBut I want to address a *feeling* that can come along with a confidence interval nestled right up against zero. A feeling of victory. It seems like a perfectly designed study. You rejected the null and collected just enough data to do it.\n\nBut instead, it should feel like a near-miss. Like an accident narrowly avoided. A confidence interval nestled right up against zero indicates that one of two things has happened: either you were (1) unlucky or (2) under-powered.\n\nBecause \"unlucky\" is always a possibility, we can't learn much from a *particular* confidence interval, but we can learn a lot from a *literature*. A literature with well-powered studies produces confidence intervals that often fall far from zero. A well-powered literature does not produce confidence intervals that consistently nestle up against zero. Under-powered studies, though, *do* tend to produce confidence intervals that nestle right up against zero.\n\n## A Simulation\n\nI'm going to explore the behavior of confidence intervals with a little simulation. In this simulation, I'm going to assert a standard error rather than create the standard error endogenously through sample size, etc. I use a true effect size of 0.5 and standard errors of 0.5, 0.3, 0.2, and 0.15 to create studies with 25%, 50%, 80%, and 95% power, respectively.[^2] I think of 80% as \"minimally-powered\" and 95% as \"well-powered.\"\n\n[^2]: I'm ignoring how to choose the true effect, estimate the standard error, and compute power. For now, I'm placing all this behind the curtain. See DaniÃ«l Lakens' book \\[*Improving Your Statistical Inferences*\\] for discussion (h/t [Bermond Scoggins](https://bersco.github.io)).\n\nI'm using a one-sided test (hypothesizing a positive effect), so I'll use 90% confidence intervals with arms that are 1.64 standard errors wide. Let's simulate some estimates from each of our four studies and compute their confidence intervals. I simulate 5,000 confidence intervals to explore below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load packages\nlibrary(tidyverse)\n\n# create a parameter for the true effect\ntrue_effect <- 0.5 # just assumed by me\n\n# create a data frame of standard errors (with approximate power)\nse_df <- tribble(\n  ~se,    ~pwr,\n  0.5,    \"about 25% power\",\n  0.3,    \"about 50% power\",\n  0.2,    \"about 80% power\",\n  0.15,   \"about 95% power\"\n)\n\n# create function to simulate estimates for each standard error\nsimulate_estimates <- function(se, pwr) {\n  tibble(\n    est = rnorm(n_cis, mean = true_effect, sd = se),\n    se = se,\n    pwr = pwr\n  )\n}\n\n# simulate the estimates, compute the confidence intervals, and wrangle\nn_cis <- 5000  # the number of cis to create\nci_df <- se_df %>% \n  # simulate estimates\n  pmap_dfr(simulate_estimates) %>%\n  # compute confidence intervals\n  mutate(lwr = est - 1.64*se, \n         upr = est + 1.64*se) %>%\n  # summarize the location of the confidence interval\n  mutate(result = case_when(lwr < 0 ~ \"Not significant\",\n                            lwr < se ~ \"Nestled against zero\",\n                            lwr >= se~ \"Not nestled against zero\"))\n```\n:::\n\n\nNow let's quickly confirm my power calculations by computing the proportion of confidence intervals to the right of zero. These are about right. In a later post, I'll describe how I think about computing these quantities.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# confirm power calculations\nci_df %>%\n  group_by(se, pwr) %>%\n  summarize(sim_pwr = 1 - mean(result == \"Not significant\"),\n            sim_pwr = scales::percent(sim_pwr, accuracy = 1)) %>%\n    select(SE = se, \n         Power = pwr,\n         `Percent Significant` = sim_pwr) %>%\n  kableExtra::kable(format = \"markdown\")\n```\n\n::: {.cell-output-display}\n|   SE|Power           |Percent Significant |\n|----:|:---------------|:-------------------|\n| 0.15|about 95% power |96%                 |\n| 0.20|about 80% power |80%                 |\n| 0.30|about 50% power |52%                 |\n| 0.50|about 25% power |26%                 |\n:::\n:::\n\n\n## What Do Confidence Intervals from Well-Powered Studies Look Like?\n\nNow let's see what these confidence intervals look like. 5,000 is too many to plot, so I sample 25. But I applied the statistical significance filter first. This mimics the publication process and makes the plots a little easier to compare. My argument doesn't depend on this filter, though.\n\nI plotted these 100 intervals below^[4 studies x 25 simulated intervals per study = 100 intervals.]\n\nThere are three important vertical lines in these plots.\n\n1.  The **solid** line indicates zero. All confidence intervals are above zero because I applied the significance filter.\n2.  The **dotted** line indicates one standard error above zero. This varies across panels because the standard error varies across panels.\n3.  The **dashed** line indicates the true effect of 0.5. Because I applied the significance filter, the lower-powered studies are consistently over-estimating the true effect.\n\nThe intervals are green when the lower bound of the 90% confidence interval falls within one standard error of zero---that's my definition of \"nestled up against zero.\" The intervals are orange when the lower bound falls further than one standard error above zero.\n\nNotice how low-powered studies tend to nestle their confidence intervals right up against zero. Almost all of the confidence intervals from the study with 25% power are nestled right up against zero. Very few of the confidence intervals from the study with 95% power are nestled up against zero.\n\nAgain, you should apply this standard to a *literature*. You should not apply this standard to a particular study because even well-powered studies sometimes produce confidence intervals that nestle up against zero. But when you start to see confidence intervals consistently falling close to zero, you should start to assume that the literature uses under-powered studies and that the estimates in that literature are inflated due to Type M errors [(Gelman and Stern 2014)](https://journals.sagepub.com/doi/10.1177/1745691614551642).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngg_df <- ci_df %>%\n  filter(lwr > 0) %>% # apply significance filter \n  # sample 25 intervals (from those that are significant)\n  group_by(se, pwr) %>%\n  sample_n(25) %>%\n  # create id (ordered by estimate value)\n  group_by(se, pwr) %>%\n  arrange(est) %>%\n  mutate(ci_id = 1:n())\n  \nggplot(gg_df, aes(x = est, xmin = lwr, xmax = upr, y = ci_id,\n                    color = result)) + \n  facet_wrap(vars(pwr), ncol = 1, scales = \"free_x\") + \n  geom_vline(data = se_df, aes(xintercept = se), linetype = \"dotted\") +\n  geom_vline(xintercept = 0) + \n  geom_vline(xintercept = true_effect, linetype = \"dashed\") + \n  geom_errorbarh(height = 0) + \n  geom_point() + \n  scale_color_brewer(type = \"qual\", palette = 2) + \n  theme_bw() + \n  labs(x = \"Estimate and 90% CI\",\n       y = NULL,\n       color = \"Result\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=576}\n:::\n:::\n\n\n## Showing This Another Way: Density of the Lower Bounds\n\nWe can also plot the density of the lower bounds of these 5,000 intervals. This approach shows the \"nestling\" most clearly. The plots below show that the lower bounds of confidence intervals tend to nestle close to zero when the power is low, and lie further from zero when the power is high.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngg_df <- ci_df %>%\n  filter(lwr > 0) # apply significance filter \nggplot(gg_df, aes(x = lwr)) + \n  facet_wrap(vars(pwr), scales = \"free_x\") + \n  geom_density(fill = \"grey50\") + \n  geom_vline(data = se_df, aes(xintercept = se), linetype = \"dotted\") + \n  theme_bw() + \n  labs(x = \"Location of Lower Bound of 90% CI\",\n       y = \"Density\",\n       color = \"Power\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n## Showing This Another Way: Frequency of Nestling\n\nLastly, I compute the percent of confidence intervals that are nestled right up against zero. For a well-powered study with 95% power, only about 1 in 5 confidence intervals nestle up against zero. For a poorly-powered study with 25% power, about 4 in 5 of confidence intervals nestle up against zero (among those that are above zero). The table below shows the remaining frequencies.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nci_df %>%\n  group_by(se, pwr, result) %>%\n  summarize(frac = n()/n_cis, .groups = \"drop\") %>%\n  pivot_wider(names_from = result, values_from = frac) %>%\n  mutate(`Nestled, given significant` = `Nestled against zero`/(1 - `Not significant`),\n         `Not nestled, given significant` = `Not nestled against zero`/(1 - `Not significant`)) %>%\n  select(SE = se, \n         Power = pwr,\n         `Not significant`,\n         `Nestled against zero`,\n         `Not nestled against zero`,\n         `Nestled, given significant`,\n         `Not nestled, given significant`) %>%\n  mutate(across(`Not significant`:`Not nestled, given significant`, ~ scales::percent(., accuracy = 1))) %>%\n  kableExtra::kable()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:right;\"> SE </th>\n   <th style=\"text-align:left;\"> Power </th>\n   <th style=\"text-align:left;\"> Not significant </th>\n   <th style=\"text-align:left;\"> Nestled against zero </th>\n   <th style=\"text-align:left;\"> Not nestled against zero </th>\n   <th style=\"text-align:left;\"> Nestled, given significant </th>\n   <th style=\"text-align:left;\"> Not nestled, given significant </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:right;\"> 0.15 </td>\n   <td style=\"text-align:left;\"> about 95% power </td>\n   <td style=\"text-align:left;\"> 4% </td>\n   <td style=\"text-align:left;\"> 21% </td>\n   <td style=\"text-align:left;\"> 75% </td>\n   <td style=\"text-align:left;\"> 22% </td>\n   <td style=\"text-align:left;\"> 78% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 0.20 </td>\n   <td style=\"text-align:left;\"> about 80% power </td>\n   <td style=\"text-align:left;\"> 20% </td>\n   <td style=\"text-align:left;\"> 35% </td>\n   <td style=\"text-align:left;\"> 45% </td>\n   <td style=\"text-align:left;\"> 44% </td>\n   <td style=\"text-align:left;\"> 56% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 0.30 </td>\n   <td style=\"text-align:left;\"> about 50% power </td>\n   <td style=\"text-align:left;\"> 48% </td>\n   <td style=\"text-align:left;\"> 35% </td>\n   <td style=\"text-align:left;\"> 17% </td>\n   <td style=\"text-align:left;\"> 67% </td>\n   <td style=\"text-align:left;\"> 33% </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:right;\"> 0.50 </td>\n   <td style=\"text-align:left;\"> about 25% power </td>\n   <td style=\"text-align:left;\"> 74% </td>\n   <td style=\"text-align:left;\"> 21% </td>\n   <td style=\"text-align:left;\"> 5% </td>\n   <td style=\"text-align:left;\"> 82% </td>\n   <td style=\"text-align:left;\"> 18% </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n## Summary\n\nIn this post, I address confidence intervals that are nestled right up against zero. These intervals can suggest a perfectly powered study---not too much, not too little. But instead, a confidence interval nestled right up against zero indicates that one of two things has happened: either you were (1) unlucky or (2) under-powered.\n\nBecause \"unlucky\" is always a possibility, we can't learn much from a *particular* confidence interval, but we can learn a lot from a *literature*. A literature with well-powered studies produces confidence intervals that often fall far from zero. A well-powered literature does not produce confidence intervals that consistently nestle up against zero. Under-powered studies, though, *do* tend to produce confidence intervals that nestle right up against zero.\n\n::: callout\n## Key Takeaway\nUnder-powered studies tend to produce confidence intervals that are nestled right up against zero. Well-powered studies tend to produce confidence intervals that fall further away. A literature that produces confidence intervals that consistently nestle right up against zero is likely a collection of under-powered studies.\n:::\n\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}