[
  {
    "objectID": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html",
    "href": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html",
    "title": "Power, Part I: Power Is for You, Not for Reviewer Two",
    "section": "",
    "text": "There’s been some really good work lately on statistical power. I’ll point you to two really great papers.\n\nArel-Bundock, Vincent, Ryan C. Briggs, Hristos Doucouliagos, Marco Mendoza Aviña, and T.D. Stanley. 2022. “Quantitative Political Science Research Is Greatly Underpowered.” OSF Preprints. July 5. doi: 10.31219/osf.io/7vy2f.\nKane, John V. 2023. “More Than Meets the ITT: A Guide for Investigating Null Results .” APSA Preprints. doi: 10.33774/apsa-2023-h4p0q-v2.\n\nI’ve been long interested in statistical power (see Rainey 20141 and Rainey 20152), and these new papers have me thinking even more about the importance of power.1 Rainey, Carlisle. 2014. “Arguing for a Negligible Effect.” American Journal of Political Science 58(4): 1083-1091.2 McCaskey, Kelly and Carlisle Rainey. 2015. “Substantive Importance and the Veil of Statistical Significance.” Statistics, Politics, and Policy 6(1-2): 77-96.\nIn this post, I argue that statistical power isn’t something ancillary. Power is primary. I also argue that power isn’t something you–the researcher–build to satisfy an especially cranky Reviewer 2, it’s something you do for yourself, to make sure that your study succeeds."
  },
  {
    "objectID": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#background",
    "href": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#background",
    "title": "Power, Part I: Power Is for You, Not for Reviewer Two",
    "section": "",
    "text": "There’s been some really good work lately on statistical power. I’ll point you to two really great papers.\n\nArel-Bundock, Vincent, Ryan C. Briggs, Hristos Doucouliagos, Marco Mendoza Aviña, and T.D. Stanley. 2022. “Quantitative Political Science Research Is Greatly Underpowered.” OSF Preprints. July 5. doi: 10.31219/osf.io/7vy2f.\nKane, John V. 2023. “More Than Meets the ITT: A Guide for Investigating Null Results .” APSA Preprints. doi: 10.33774/apsa-2023-h4p0q-v2.\n\nI’ve been long interested in statistical power (see Rainey 20141 and Rainey 20152), and these new papers have me thinking even more about the importance of power.1 Rainey, Carlisle. 2014. “Arguing for a Negligible Effect.” American Journal of Political Science 58(4): 1083-1091.2 McCaskey, Kelly and Carlisle Rainey. 2015. “Substantive Importance and the Veil of Statistical Significance.” Statistics, Politics, and Policy 6(1-2): 77-96.\nIn this post, I argue that statistical power isn’t something ancillary. Power is primary. I also argue that power isn’t something you–the researcher–build to satisfy an especially cranky Reviewer 2, it’s something you do for yourself, to make sure that your study succeeds."
  },
  {
    "objectID": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#the-hypothesis-testing-framework",
    "href": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#the-hypothesis-testing-framework",
    "title": "Power, Part I: Power Is for You, Not for Reviewer Two",
    "section": "The Hypothesis Testing Framework",
    "text": "The Hypothesis Testing Framework\nIn the hypothesis testing framework, you consider two hypotheses: the null hypothesis and the alternative hypothesis.\nThe hypothesis test is all about arguing against the null hypothesis \\(H_0\\) (leaving the alternative \\(H_A\\) as the only remaining possibility). You will (try to) show that your data would be “unusual” if the null hypothesis were correct.33 When hypothesizing about the average treatment effect (ATE), this can take a variety of forms. The form doesn’t really matter.\nIf the data would NOT be unusual under the null hypothesis, then you do not reject the null hypothesis."
  },
  {
    "objectID": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#intepreting-a-failure-to-reject",
    "href": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#intepreting-a-failure-to-reject",
    "title": "Power, Part I: Power Is for You, Not for Reviewer Two",
    "section": "Intepreting a Failure to Reject",
    "text": "Intepreting a Failure to Reject\nA failure to reject means that the data “would not be unusual under the null hypothesis.” This does not imply that you should conclude the data are only consistent with the null. Indeed, there is a sharp asymmetry in hypothesis testing. I describe this in my 2014 AJPS:\n\nPolitical scientists commonly interpret a lack of statistical significance (i.e., a failure to reject the null) as evidence for a negligible effect (Gill 1999), but this approach acts as a broken compass… If the sample size is too small, the researcher often concludes that the effect is negligible even though the data are also consistent with large, meaningful effects. This occurs because the small sample leads to a large confidence interval, which is likely to contain both “no effect” and large effects.\n\nGill (1999)4 describes this more forcefully:4 Gill, Jeff. 1999. “The Insignificance of Null Hypothesis Significance Testing.” Political Research Quarterly 52(3): 647-674.\n\nWe teach graduate students to be very careful when describing the occurrence of not rejecting the null hypothesis. This is because failing to reject the null hypothesis does not rule out an infinite number of other competing research hypotheses. Null hypothesis significance testing is asymmetric: if the test statistic is sufficiently atypical given the null hypothesis then the null hypothesis is rejected, but if the test statistic is insufficiently atypical given the null hypothesis then the null hypothesis is not accepted. This is a double standard: H1 is held innocent until proven guilty and Ho is held guilty until proven innocent (Rozeboom 1960)…\n\n\nThere are two problems that develop as a result of asymmetry. The first is a misinterpretation of the asymmetry to assert that finding a non-statistically significant difference or effect is evidence that it is equal to zero or nearly zero. Regarding the impact of this acceptance error Schmidt (1996: 126) asserts that this: “belief held by many researchers is the most devastating of all to the research enterprise.” This acceptance of the null hypothesis is damaging because it inhibits the exploration of competing research hypotheses. The second problem pertains to the correct interpretation of failing to reject the null hypotheses. Failing to reject the null hypothesis essentially provides almost no information about the state of the world. It simply means that given the evidence at hand one cannot make an assertion about some relationship: all you can conclude is that you can’t conclude that the null was false (Cohen 1962).\n\nThere are many incorrect, but somewhat innocent interpretations of p-values. Interpreting a lack of statistical significance as evidence for the null is incorrect and wildly misleading in many cases.\n\n\n\n\n\n\nImportant Point\n\n\n\nA non-statistically significant difference is not evidence that an effect is equal to zero or nearly zero. Interpreting a non-statistically significant effect otherwise is “devastating.”"
  },
  {
    "objectID": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#the-implication-of-a-non-conclusion",
    "href": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#the-implication-of-a-non-conclusion",
    "title": "Power, Part I: Power Is for You, Not for Reviewer Two",
    "section": "The Implication of a Non-Conclusion",
    "text": "The Implication of a Non-Conclusion\nIf you cannot draw a conclusion then, what exactly has happened? Obtaining \\(p &gt; 0.05\\) will not be an “error” because you won’t make a strong claim that the research hypothesis is wrong. Instead, you will simply admit that you failed to uncover evidence against the null. Failing to uncover evidence isn’t an error.\nIndeed, Jones and Tukey (2000)5 write:5 Jones, Lyle V., and John W. Tukey. 2000. “A Sensible Formulation of the Significance Test.” Psychological Methods 5(4): 411-414.\n\nA conclusion is in error only when it is “a reversal,” when it asserts one direction while the (unknown) truth is the other direction. Asserting that the direction is not yet established may constitute a wasted opportunity, but it is not an error.\n\n\n\n\n\n\n\nImportant Point\n\n\n\nFailing to uncover evidence isn’t an “error,” it is a “wasted effort.”\n\n\nThis is worth emphasizing in a different way. Tests are not magical tools that tell you which hypothesis is correct. Instead, tests summarize the evidence against the null. There are two critical pieces to “evidence against the null”: (1) the amount of evidence and (2) whether the evidence is against the null. If you buy your own argument that the null is false (surely you do!), then (2) is taken care of. Only the amount of evidence remains, and you–the researcher–choose the amount of evidence to supply."
  },
  {
    "objectID": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#the-implication-for-power-calculations",
    "href": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#the-implication-for-power-calculations",
    "title": "Power, Part I: Power Is for You, Not for Reviewer Two",
    "section": "The Implication for Power Calculations",
    "text": "The Implication for Power Calculations\nThis perspective helps motivate power calculations. By their design, tests control the error rate in certain situations (when then null is correct). You do not need to worry about Type I errors. First, the test controls the error rate under the null. Second, you are pretty sure the null is wrong (see your theory section).\n\n\n\n\n\n\nImportant Point\n\n\n\nThe hypothesis test takes care of the the Type I error rate. If you choose a properly-sized test, you don’t need to worry about those errors any more.\n\n\nIf you aren’t worried about Type I errors, what are you worried about? They only thing left to worry about is wasting your time and money. Statistical power is the chance not of wasting your time and money.\nPower isn’t a secondary quantity that you compute for thoroughness or in anticipation of a comment from Reviewer 2. Power is something that you build for yourself.\nStatisticians talk a lot about Type I errors because that’s their contribution. It’s your job to bring the power.\nAnd importantly, power is under your control. Kane provides a rich summary of ways to increase the power of your experiment. At a minimum, you have brute force control through sample size.\nPower isn’t an ancillary concern, it’s the entire game from the very beginning of the planning stage. It should be at the forefront of the researchers mind from the very beginning. You should want the power as high as possible.66 I hear that 80% is the standard, but I’m pretty uncomfortable spending dozens of hours and thousands of dollars running for a 1 in 5 chance of wasting my time. I want that chance as close to zero as I can get it. I want power close to 100%. 99% power and 80% power might both seem “high” or “acceptable,” but these are not the same. 80% power means 1 in 5 studies fail. 99% power means that 1 in 100 studies fail.\nYou have to supply a test overwhelming evidence to consistently reject the null. Careful power calculations help you make sure you succeed in this war against the null.\nPower isn’t about Type S and M errors (Gelman and Carlin 2014)7. Power is about you protecting yourself from a failed study. And that seems like a protection worth pursuing carefully.87 Gelman, Andrew, and John Carlin. “Beyond Power Calculations: Assessing Type S (Sign) and Type M (Magnitude) Errors.” Perspectives on Psychological Science 9(6): 641-651.8 Of course it’s also about Type S and M errors, but those are discipline-level concerns. I’m talking about your incentives as a researcher."
  },
  {
    "objectID": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#summary",
    "href": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#summary",
    "title": "Power, Part I: Power Is for You, Not for Reviewer Two",
    "section": "Summary",
    "text": "Summary\nHere are the takeaways:\n\nStatistical power is the chance of using your time and money productively (i.e., not wasting it).\nStatistical power is under your control (see Kane).\nYour power might be (much) lower than you think–you should check (see Arel-Bundock et al.).\nPower should be a primary concern throughout the design. The researcher should care deeply about power, perhaps more than anything else.\n\n\n\n\n\n\n\nImportant Point\n\n\n\nThe hypothesis test is no oracle. It will not consistently reject the null (even when the null is wrong) unless you supply overwhelming evidence. In experimental design, that’s not a task, that’s the task."
  },
  {
    "objectID": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html",
    "href": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html",
    "title": "Power, Part II: What Do Confidence Intervals from Well-Powered Studies Look Like?",
    "section": "",
    "text": "In this post, I address confidence intervals that are nestled right up against zero.1 These intervals indicate that an estimate is “barely” significant. I want to be clear: “barely significant” is still significant, so you should still reject the null hypothesis.21 This is the second post in a series. In my previous post, I mentioned two new papers that have me thinking about power: Arel-Bundock et al.’s “Quantitative Political Science Research Is Greatly Underpowered” and Kane’s “More Than Meets the ITT: A Guide for Investigating Null Results”. Go check out that post and those papers if you haven’t.2 I’m focusing on confidence intervals here because inference from confidence intervals is a bit more intuitive (see Rainey 2014 and Rainey 2015. In the cases I discuss, whether one checks whether the p-value is less than 0.05 or checks that confidence interval contains zero are equivalent.\nBut I want to address a feeling that can come along with a confidence interval nestled right up against zero. A feeling of victory. It seems like a perfectly designed study. You rejected the null and collected just enough data to do it.\nBut instead, it should feel like a near-miss. Like an accident narrowly avoided. A confidence interval nestled right up against zero indicates that one of two things has happened: either you were (1) unlucky or (2) under-powered.\nBecause “unlucky” is always a possibility, we can’t learn much from a particular confidence interval, but we can learn a lot from a literature. A literature with well-powered studies produces confidence intervals that often fall far from zero. A well-powered literature does not produce confidence intervals that consistently nestle up against zero. Under-powered studies, though, do tend to produce confidence intervals that nestle right up against zero."
  },
  {
    "objectID": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#background",
    "href": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#background",
    "title": "Power, Part II: What Do Confidence Intervals from Well-Powered Studies Look Like?",
    "section": "",
    "text": "In this post, I address confidence intervals that are nestled right up against zero.1 These intervals indicate that an estimate is “barely” significant. I want to be clear: “barely significant” is still significant, so you should still reject the null hypothesis.21 This is the second post in a series. In my previous post, I mentioned two new papers that have me thinking about power: Arel-Bundock et al.’s “Quantitative Political Science Research Is Greatly Underpowered” and Kane’s “More Than Meets the ITT: A Guide for Investigating Null Results”. Go check out that post and those papers if you haven’t.2 I’m focusing on confidence intervals here because inference from confidence intervals is a bit more intuitive (see Rainey 2014 and Rainey 2015. In the cases I discuss, whether one checks whether the p-value is less than 0.05 or checks that confidence interval contains zero are equivalent.\nBut I want to address a feeling that can come along with a confidence interval nestled right up against zero. A feeling of victory. It seems like a perfectly designed study. You rejected the null and collected just enough data to do it.\nBut instead, it should feel like a near-miss. Like an accident narrowly avoided. A confidence interval nestled right up against zero indicates that one of two things has happened: either you were (1) unlucky or (2) under-powered.\nBecause “unlucky” is always a possibility, we can’t learn much from a particular confidence interval, but we can learn a lot from a literature. A literature with well-powered studies produces confidence intervals that often fall far from zero. A well-powered literature does not produce confidence intervals that consistently nestle up against zero. Under-powered studies, though, do tend to produce confidence intervals that nestle right up against zero."
  },
  {
    "objectID": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#a-simulation",
    "href": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#a-simulation",
    "title": "Power, Part II: What Do Confidence Intervals from Well-Powered Studies Look Like?",
    "section": "A Simulation",
    "text": "A Simulation\nI’m going to explore the behavior of confidence intervals with a little simulation. In this simulation, I’m going to assert a standard error rather than create the standard error endogenously through sample size, etc. I use a true effect size of 0.5 and standard errors of 0.5, 0.3, 0.2, and 0.15 to create studies with 25%, 50%, 80%, and 95% power, respectively.3 I think of 80% as “minimally-powered” and 95% as “well-powered.”3 I’m ignoring how to choose the true effect, estimate the standard error, and compute power. For now, I’m placing all this behind the curtain. See Daniël Lakens’ book [Improving Your Statistical Inferences] for discussion (h/t Bermond Scoggins).\nI’m using a one-sided test (hypothesizing a positive effect), so I’ll use 90% confidence intervals with arms that are 1.64 standard errors wide. Let’s simulate some estimates from each of our four studies and compute their confidence intervals. I simulate 5,000 confidence intervals to explore below.\n\n\nCode\n# load packages\nlibrary(tidyverse)\n\n# create a parameter for the true effect\ntrue_effect &lt;- 0.5 # just assumed by me\n\n# create a data frame of standard errors (with approximate power)\nse_df &lt;- tribble(\n  ~se,    ~pwr,\n  0.5,    \"about 25% power\",\n  0.3,    \"about 50% power\",\n  0.2,    \"about 80% power\",\n  0.15,   \"about 95% power\"\n)\n\n# create function to simulate estimates for each standard error\nsimulate_estimates &lt;- function(se, pwr) {\n  tibble(\n    est = rnorm(n_cis, mean = true_effect, sd = se),\n    se = se,\n    pwr = pwr\n  )\n}\n\n# simulate the estimates, compute the confidence intervals, and wrangle\nn_cis &lt;- 5000  # the number of cis to create\nci_df &lt;- se_df %&gt;% \n  # simulate estimates\n  pmap_dfr(simulate_estimates) %&gt;%\n  # compute confidence intervals\n  mutate(lwr = est - 1.64*se, \n         upr = est + 1.64*se) %&gt;%\n  # summarize the location of the confidence interval\n  mutate(result = case_when(lwr &lt; 0 ~ \"Not significant\",\n                            lwr &lt; se ~ \"Nestled against zero\",\n                            lwr &gt;= se~ \"Not nestled against zero\"))\n\n\nNow let’s quickly confirm my power calculations by computing the proportion of confidence intervals to the right of zero. These are about right. In a later post, I’ll describe how I think about computing these quantities.\n\n\nCode\n# confirm power calculations\nci_df %&gt;%\n  group_by(se, pwr) %&gt;%\n  summarize(sim_pwr = 1 - mean(result == \"Not significant\"),\n            sim_pwr = scales::percent(sim_pwr, accuracy = 1)) %&gt;%\n    select(SE = se, \n         Power = pwr,\n         `Percent Significant` = sim_pwr) %&gt;%\n  kableExtra::kable(format = \"markdown\")\n\n\n\n\n\nSE\nPower\nPercent Significant\n\n\n\n\n0.15\nabout 95% power\n95%\n\n\n0.20\nabout 80% power\n80%\n\n\n0.30\nabout 50% power\n51%\n\n\n0.50\nabout 25% power\n25%"
  },
  {
    "objectID": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#what-do-confidence-intervals-from-well-powered-studies-look-like",
    "href": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#what-do-confidence-intervals-from-well-powered-studies-look-like",
    "title": "Power, Part II: What Do Confidence Intervals from Well-Powered Studies Look Like?",
    "section": "What Do Confidence Intervals from Well-Powered Studies Look Like?",
    "text": "What Do Confidence Intervals from Well-Powered Studies Look Like?\nNow let’s see what these confidence intervals look like. 5,000 is too many to plot, so I sample 25. But I applied the statistical significance filter first. This mimics the publication process and makes the plots a little easier to compare. My argument doesn’t depend on this filter, though.\nI plotted these 100 intervals below44 4 studies x 25 simulated intervals per study = 100 intervals.\nThere are three important vertical lines in these plots.\n\nThe solid line indicates zero. All confidence intervals are above zero because I applied the significance filter.\nThe dotted line indicates one standard error above zero. This varies across panels because the standard error varies across panels.\nThe dashed line indicates the true effect of 0.5. Because I applied the significance filter, the lower-powered studies are consistently over-estimating the true effect.\n\nThe intervals are green when the lower bound of the 90% confidence interval falls within one standard error of zero—that’s my definition of “nestled up against zero.” The intervals are orange when the lower bound falls further than one standard error above zero.\nNotice how low-powered studies tend to nestle their confidence intervals right up against zero. Almost all of the confidence intervals from the study with 25% power are nestled right up against zero. Very few of the confidence intervals from the study with 95% power are nestled up against zero.\nAgain, you should apply this standard to a literature. You should not apply this standard to a particular study because even well-powered studies sometimes produce confidence intervals that nestle up against zero. But when you start to see confidence intervals consistently falling close to zero, you should start to assume that the literature uses under-powered studies and that the estimates in that literature are inflated due to Type M errors (Gelman and Stern 2014).\n\n\nCode\ngg_df &lt;- ci_df %&gt;%\n  filter(lwr &gt; 0) %&gt;% # apply significance filter \n  # sample 25 intervals (from those that are significant)\n  group_by(se, pwr) %&gt;%\n  sample_n(25) %&gt;%\n  # create id (ordered by estimate value)\n  group_by(se, pwr) %&gt;%\n  arrange(est) %&gt;%\n  mutate(ci_id = 1:n())\n  \nggplot(gg_df, aes(x = est, xmin = lwr, xmax = upr, y = ci_id,\n                    color = result)) + \n  facet_wrap(vars(pwr), ncol = 1, scales = \"free_x\") + \n  geom_vline(data = se_df, aes(xintercept = se), linetype = \"dotted\") +\n  geom_vline(xintercept = 0) + \n  geom_vline(xintercept = true_effect, linetype = \"dashed\") + \n  geom_errorbarh(height = 0) + \n  geom_point() + \n  scale_color_brewer(type = \"qual\", palette = 2) + \n  theme_bw() + \n  labs(x = \"Estimate and 90% CI\",\n       y = NULL,\n       color = \"Result\")"
  },
  {
    "objectID": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#showing-this-another-way-density-of-the-lower-bounds",
    "href": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#showing-this-another-way-density-of-the-lower-bounds",
    "title": "Power, Part II: What Do Confidence Intervals from Well-Powered Studies Look Like?",
    "section": "Showing This Another Way: Density of the Lower Bounds",
    "text": "Showing This Another Way: Density of the Lower Bounds\nWe can also plot the density of the lower bounds of these 5,000 intervals. This approach shows the “nestling” most clearly. The plots below show that the lower bounds of confidence intervals tend to nestle close to zero when the power is low, and lie further from zero when the power is high.\n\n\nCode\ngg_df &lt;- ci_df %&gt;%\n  filter(lwr &gt; 0) # apply significance filter \nggplot(gg_df, aes(x = lwr)) + \n  facet_wrap(vars(pwr), scales = \"free_x\") + \n  geom_density(fill = \"grey50\") + \n  geom_vline(data = se_df, aes(xintercept = se), linetype = \"dotted\") + \n  theme_bw() + \n  labs(x = \"Location of Lower Bound of 90% CI\",\n       y = \"Density\",\n       color = \"Power\")"
  },
  {
    "objectID": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#showing-this-another-way-frequency-of-nestling",
    "href": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#showing-this-another-way-frequency-of-nestling",
    "title": "Power, Part II: What Do Confidence Intervals from Well-Powered Studies Look Like?",
    "section": "Showing This Another Way: Frequency of Nestling",
    "text": "Showing This Another Way: Frequency of Nestling\nLastly, I compute the percent of confidence intervals that are nestled right up against zero. For a well-powered study with 95% power, only about 1 in 5 confidence intervals nestle up against zero. For a poorly-powered study with 25% power, about 4 in 5 of confidence intervals nestle up against zero (among those that are above zero). The table below shows the remaining frequencies.\n\n\nCode\nci_df %&gt;%\n  group_by(se, pwr, result) %&gt;%\n  summarize(frac = n()/n_cis, .groups = \"drop\") %&gt;%\n  pivot_wider(names_from = result, values_from = frac) %&gt;%\n  mutate(`Nestled, given significant` = `Nestled against zero`/(1 - `Not significant`),\n         `Not nestled, given significant` = `Not nestled against zero`/(1 - `Not significant`)) %&gt;%\n  select(SE = se, \n         Power = pwr,\n         `Not significant`,\n         `Nestled against zero`,\n         `Not nestled against zero`,\n         `Nestled, given significant`,\n         `Not nestled, given significant`) %&gt;%\n  mutate(across(`Not significant`:`Not nestled, given significant`, ~ scales::percent(., accuracy = 1))) %&gt;%\n  kableExtra::kable()\n\n\n\n\n\nSE\nPower\nNot significant\nNestled against zero\nNot nestled against zero\nNestled, given significant\nNot nestled, given significant\n\n\n\n\n0.15\nabout 95% power\n5%\n20%\n75%\n21%\n79%\n\n\n0.20\nabout 80% power\n20%\n37%\n44%\n46%\n54%\n\n\n0.30\nabout 50% power\n49%\n34%\n17%\n67%\n33%\n\n\n0.50\nabout 25% power\n75%\n21%\n5%\n81%\n19%"
  },
  {
    "objectID": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#summary",
    "href": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#summary",
    "title": "Power, Part II: What Do Confidence Intervals from Well-Powered Studies Look Like?",
    "section": "Summary",
    "text": "Summary\nIn this post, I address confidence intervals that are nestled right up against zero. These intervals can suggest a perfectly powered study—not too much, not too little. But instead, a confidence interval nestled right up against zero indicates that one of two things has happened: either you were (1) unlucky or (2) under-powered.\nBecause “unlucky” is always a possibility, we can’t learn much from a particular confidence interval, but we can learn a lot from a literature. A literature with well-powered studies produces confidence intervals that often fall far from zero. A well-powered literature does not produce confidence intervals that consistently nestle up against zero. Under-powered studies, though, do tend to produce confidence intervals that nestle right up against zero.\n\n\n\n\n\n\nKey Takeaway\n\n\n\nUnder-powered studies tend to produce confidence intervals that are nestled right up against zero. Well-powered studies tend to produce confidence intervals that fall further away. A literature that produces confidence intervals that consistently nestle right up against zero is likely a collection of under-powered studies."
  },
  {
    "objectID": "blog/2023-06-01-teaching-confidence-intervals-with-gganimate/index.html",
    "href": "blog/2023-06-01-teaching-confidence-intervals-with-gganimate/index.html",
    "title": "Teaching Confidence Intervals and Hypothesis Testing with gganimate",
    "section": "",
    "text": "When I give students formula for confidence intervals, I find that students don’t have a sharp concept of how those confidence intervals work—even if I explain the components of the formula well.\nEven though they understand—seemingly very well—that the point estimate is noisy, they struggle to conceptualize that a confidence interval can often include values on the incorrect side of zero. Stated differently, they have a hard time understanding how a hypothesis test can fail to reject the null (when the null is incorrect). Their intuition suggests that a “test” should give you the correct answer.\nBecause their instincts are wrong, I want to undermine their trust in hypothesis tests. I want them to feel the riskiness of poorly-powered experiments that consistently nestle confidence intervals right up against zero. I want them ready and eager to work hard to avoid that risk—to make sure they have adequate statistical power.\nTo help undermine their confidence in confidence intervals, I like three exercises that mimic a test with 80% power. In each case, we are assuming that we have formulated a correct hypothesis and designed an excellent experiment with 80% power.\n\nFirst, I have students roll a six-sided die. If the die produces a , then their study fails—they wasted their opportunity. See this post for details on this perspective. This simulates the riskiness of an experiment with 80% power quite well. They get lots of failed experiments in a short period of time. They become well-aware of the possibility of a failed study and grow increasingly interested in reducing this risk.\nSecond, I use a computer to produce a plot of many (about 50 seems right) confidence intervals from the same repeated study. I explain that the interval we will get in the study we actually conduct is like a random draw from this collection. (See below for an example of this figure.)\nThird, I make the plot dynamic. I find that dynamics make the plot more memorable and convey the (appropriate) idea that hypothesis tests and confidence intervals are chaotic, noisy quantities.\n\nThese exercises make it clear that failed studies are real possibilities. Hopefully they clearly see and “feel”:\n\nThe hypothesis test is no oracle. It will not consistently reject the null (even when the null is wrong) unless you supply overwhelming evidence. In experimental design, that’s not a task, that’s the task.\n\nBelow, I walk through the plots I use in parts 2 and 3."
  },
  {
    "objectID": "blog/2023-06-01-teaching-confidence-intervals-with-gganimate/index.html#background",
    "href": "blog/2023-06-01-teaching-confidence-intervals-with-gganimate/index.html#background",
    "title": "Teaching Confidence Intervals and Hypothesis Testing with gganimate",
    "section": "",
    "text": "When I give students formula for confidence intervals, I find that students don’t have a sharp concept of how those confidence intervals work—even if I explain the components of the formula well.\nEven though they understand—seemingly very well—that the point estimate is noisy, they struggle to conceptualize that a confidence interval can often include values on the incorrect side of zero. Stated differently, they have a hard time understanding how a hypothesis test can fail to reject the null (when the null is incorrect). Their intuition suggests that a “test” should give you the correct answer.\nBecause their instincts are wrong, I want to undermine their trust in hypothesis tests. I want them to feel the riskiness of poorly-powered experiments that consistently nestle confidence intervals right up against zero. I want them ready and eager to work hard to avoid that risk—to make sure they have adequate statistical power.\nTo help undermine their confidence in confidence intervals, I like three exercises that mimic a test with 80% power. In each case, we are assuming that we have formulated a correct hypothesis and designed an excellent experiment with 80% power.\n\nFirst, I have students roll a six-sided die. If the die produces a , then their study fails—they wasted their opportunity. See this post for details on this perspective. This simulates the riskiness of an experiment with 80% power quite well. They get lots of failed experiments in a short period of time. They become well-aware of the possibility of a failed study and grow increasingly interested in reducing this risk.\nSecond, I use a computer to produce a plot of many (about 50 seems right) confidence intervals from the same repeated study. I explain that the interval we will get in the study we actually conduct is like a random draw from this collection. (See below for an example of this figure.)\nThird, I make the plot dynamic. I find that dynamics make the plot more memorable and convey the (appropriate) idea that hypothesis tests and confidence intervals are chaotic, noisy quantities.\n\nThese exercises make it clear that failed studies are real possibilities. Hopefully they clearly see and “feel”:\n\nThe hypothesis test is no oracle. It will not consistently reject the null (even when the null is wrong) unless you supply overwhelming evidence. In experimental design, that’s not a task, that’s the task.\n\nBelow, I walk through the plots I use in parts 2 and 3."
  },
  {
    "objectID": "blog/2023-06-01-teaching-confidence-intervals-with-gganimate/index.html#an-experiment-that-we-can-repeat",
    "href": "blog/2023-06-01-teaching-confidence-intervals-with-gganimate/index.html#an-experiment-that-we-can-repeat",
    "title": "Teaching Confidence Intervals and Hypothesis Testing with gganimate",
    "section": "An Experiment that We Can Repeat",
    "text": "An Experiment that We Can Repeat\nFirst, let’s choose to conduct an experiment with 80% power. To get this, we’ll suppose that the true effect is 1 and the standard error is 0.4. To obtain 80% power, you can use the guideline that the standard error should be about 40% of the true effect (or the true effect divided by 2.48). I’ll show where these guidelines come from in a future post. With the true effect and standard error in hand, we can compute the long-run properties of the experiment.\n\n\nCode\nlibrary(tidyverse)\n\n# study parameters\ntrue_effect &lt;- 1\nse &lt;- 0.40  # 1/2.48\n\n# identify effects of interest\neoi &lt;- tribble(\n  ~Effect, ~Description,\n  true_effect, \"True Effect (known in this exercise)\"\n) \n\n# compute quantities of interest regarding power\neoi %&gt;%\n  mutate(Power = 1 - pnorm(1.64*se, Effect, se),\n         Power = scales::percent(Power, accuracy = 1),\n         `Type S` = retrodesign::type_s(Effect, se)$type_s,\n         `Type S` = scales::number(`Type S`, accuracy = 0.01),\n         `Type M` = retrodesign::type_m(Effect, se)$type_m,\n         `Type M` = scales::number(`Type M`, accuracy = 0.01),\n         Effect = scales::number(Effect, accuracy = 0.01)) %&gt;% \n  pivot_longer(cols = Effect:`Type M`) %&gt;%\n  kableExtra::kable(format = \"markdown\", col.names = NULL)\n\n\n\n\n\nEffect\n1.00\n\n\nDescription\nTrue Effect (known in this exercise)\n\n\nPower\n81%\n\n\nType S\n0.00\n\n\nType M\n1.20"
  },
  {
    "objectID": "blog/2023-06-01-teaching-confidence-intervals-with-gganimate/index.html#a-static-plot",
    "href": "blog/2023-06-01-teaching-confidence-intervals-with-gganimate/index.html#a-static-plot",
    "title": "Teaching Confidence Intervals and Hypothesis Testing with gganimate",
    "section": "A Static Plot",
    "text": "A Static Plot\nBut these long-run properties remain a bit abstract and seem “distant” from the practical implications of our particular experiment. This is where the three exercises above come in handy.\nThe second exercise is a static plot. The plot shows 50 intervals; we can see that several include zero. These are wasted opporunities. We set out to reject a null hypothesis that the effect was less than or equal to zero, and we failed to do that.\n\n\nCode\n# number of studies to simulate\nn_studies &lt;- 50\n\n# a data frame of studies \nset.seed(123)\nests &lt;- tibble(study_id = 1:n_studies,\n               est = c(rnorm(n_studies, true_effect, se))) %&gt;%\n  mutate(reject_null = ifelse(est - 1.64*se &gt; 0, \"Yes\", \"No\")) \n\n# plot the confidence intervals for each study\ngg &lt;- ggplot(ests, aes(x = est, \n                         y = study_id, \n                         xmin = est - 1.64*se, \n                         xmax = est + 1.64*se, \n                         color = reject_null)) + \n  geom_vline(xintercept = true_effect, \n             linetype = \"dotted\") + \n  geom_vline(xintercept = 0) + \n  geom_point() + \n  geom_errorbarh(height = 0) + \n  geom_rug(sides = \"b\", \n           aes(x = est - 1.64*se, color = NULL), \n           alpha = 0.5, \n           length = unit(0.025, \"npc\")) + \n  scale_color_manual(values = c(\"No\" = \"#d95f02\", \"Yes\" = \"#1b9e77\")) +  # from https://colorbrewer2.org/#type=qualitative&scheme=Dark2&n=3\n  theme_bw() +\n    theme(panel.grid = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.title.y = element_blank()) + \n  labs(x = \"Estimate and 90% Confidence Interval\",\n       y = \"Study ID\", \n       color = \"Reject Null?\", \n       caption = \"Rug shows distribution of lower bound of confidence interval.\") \n\n# print plot\nprint(gg)"
  },
  {
    "objectID": "blog/2023-06-01-teaching-confidence-intervals-with-gganimate/index.html#dynamic-plot",
    "href": "blog/2023-06-01-teaching-confidence-intervals-with-gganimate/index.html#dynamic-plot",
    "title": "Teaching Confidence Intervals and Hypothesis Testing with gganimate",
    "section": "Dynamic Plot",
    "text": "Dynamic Plot\nThe static plot above is nice, but doesn’t convey an appropriate sense of randomness or “you don’t know what you’ll get this time.” To convey this feeling, I like to add dynamics. In particular, I like a confidence interval that’s moving and you’re not sure if it will cover zero or not. This conveys the sense that “something bad might happen” with each draw. Even though only about 10 of the 50 confidence intervals will cross zero, you feel the danger on all 50 simulations.\nIn designing this plot, two features are in tension:\n\nThe plot conveys the ideas.\nThe code is easy to update and understand.\n\nIn the past, I’ve prioritized making the plot look exactly like I want. But there are concrete downsides to using hacky solutions—using functions in ways not intended. The code is brittle and difficult. For examples that clearly convey the ideas, see Presidential Plinko and this “raindrop” plot. These are great ways to convey the randomness, but producing these plots requires some “tedious” coding.\nWith this code, I tried to illustrate the concept well while avoid hacky solutions to minor problems. This makes the code easier to understand, change, and update.\nFirst, let’s start by creating the data frame to plot. We need to make two small changes to the data frame above. These are both hacks, but worth it.\n\nAdd a dummy row to the data frame to trick gganimate into starting with an empty plot.\nAdd a grouping variable to indicate the states for the transitions. This is simply a row ID variable.\n\n\n\nCode\n# load packages\nlibrary(gganimate)\n\n# add two things to the data frame of confidence intervals\n# 1. an initial row with study_id = 1 and est = NA so that \n#    the plot starts empty (gganimate would start with the \n#    first observation in place otherwise).\n# 2. a group variable that defines the row. This is the same\n#    as the study_id, except the dummy row from (1) and the \n#    actual first row have different groups.\nanimate_data &lt;- bind_rows(\n  tibble(study_id = 1, est = NA),  # study_id = 1, est = NA\n  ests                             # combine dummy row with ests data frame from above\n  ) %&gt;%\n  mutate(group = 1:n())            # group (row index)\n\n\nNow let’s plot the confidence intervals much like above, except with expanded scales to give some more room for movement.\n\n\nCode\n# same ggplot, except three annotated changes\n1gg_exp &lt;- ggplot(animate_data, aes(x = est,\n                         y = study_id, \n                         xmin = est - 1.64*se, \n                         xmax = est + 1.64*se, \n                         color = reject_null, \n2                         group = group)) +\n  geom_vline(xintercept = true_effect, \n             linetype = \"dotted\") + \n  geom_vline(xintercept = 0) + \n  geom_point() + \n  geom_errorbarh(height = 0) + \n  geom_rug(sides = \"b\", \n           aes(x = est - 1.64*se, color = NULL), \n           alpha = 0.5, \n           length = unit(0.025, \"npc\")) + \n  scale_color_manual(values = c(\"No\" = \"#d95f02\", \"Yes\" = \"#1b9e77\")) +  \n  theme_bw() +\n    theme(panel.grid = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.title.y = element_blank()) + \n  labs(x = \"Estimate and 90% Confidence Interval\",\n       y = \"Study ID\", \n       color = \"Reject Null?\", \n       caption = \"Rug shows distribution of lower bound of confidence interval.\n                  Solid lines shows zero.\n                  Dotted line shows the true effect.\") + \n  # new scales here\n3  scale_x_continuous(expand = expansion(add = c(0.6, 1))) +\n  scale_y_continuous(expand = expansion(add = 2))\n  \n\n# print plot\nprint(gg_exp)\n\n\n\n1\n\nUse dataset with group variable.\n\n2\n\nSet group explicitly.\n\n3\n\nExpand x- and y-axis.\n\n\n\n\n\n\n\nNow let’s add the animation. I like the confidence intervals to be shooting toward zero. This gives the feeling that “it might cross!” and makes the fear of a failed study real for each simulation.\n\n\nCode\n# add dyamics to the plot\n1anim &lt;- gg_exp +\n2  transition_states(states = group) +\n  # how points enter\n3  enter_drift(x_mod = 2) +\n4  enter_grow() +\n5  enter_recolor(color = \"black\") +\n6  ease_aes(color = \"exponential-in\") +\n  # how points exit/remain\n7  exit_fade(alpha = 0.3) +\n8  shadow_mark(alpha = 0.3)\n\n# make magic happen!\nanimate(anim, duration = n_studies, fps = 10, \n        height = 6, width = 8, units = \"in\", res = 150)\n\n\n\n1\n\n‘gg’ is created earlier. It’s the plot we want to make dynamic.\n\n2\n\nThe transition_states() function from gganimate creates an animation transitioning between different states of the data. In this case, the argument states is set to group. This makes each group (each row in the data frame animate_data) appear in the plot, one at a time.\n\n3\n\nThe enter_drift() function describes how new data points enter the frame. x_mod = 2 makes new data enter by drifting along the x-axis 2 points from the right of their ending position.\n\n4\n\nThe enter_grow() function describes how new data points enter the frame. In this case, it makes them will grow from a size of 0 to their ending size.\n\n5\n\nThe enter_recolor() function again describes how new data points should enter the frame. Here, it makes the points and CIs change the color from black to their ending color as they appear. I want the final color to be a bit of a surprise, so I start them as black.\n\n6\n\nThe ease_aes() function determines how the aesthetics of the points change over the transitions. color = \"exponential-in\" means the color change will be at an exponential rate at the start of the transition. This makes the color change really fast at the end of the transition to maintain the surprise of the result.\n\n7\n\nThe exit_fade() function describes how data points leave the frame. alpha = 0.3 specifies that points will fade out to 30% transparency when they exit (when the next data point reaches its position).\n\n8\n\nshadow_mark() function keeps past data points in the frame. alpha = 0.3 sets the transparency of the shadow marks to 30% to match the exit_fade() transparency."
  },
  {
    "objectID": "blog/2023-06-01-teaching-confidence-intervals-with-gganimate/index.html#summary",
    "href": "blog/2023-06-01-teaching-confidence-intervals-with-gganimate/index.html#summary",
    "title": "Teaching Confidence Intervals and Hypothesis Testing with gganimate",
    "section": "Summary",
    "text": "Summary\nThe dynamic plot above is a good tool to help students understand that confidence intervals can quite easily include values on the wrong side of zero. Stated differently, it’s easy for a hypothesis test to fail to reject the null (when the null is wrong). Their intuition suggests that the test should tell you the correct answer.\nThe exercises above (appropriately) undermine their trust in hypothesis tests. I want them to feel the riskiness of poorly-powered experiments that consistently nestle confidence intervals right up against zero. I want them ready to work hard to avoid that risk—to make sure they have statistical power. Hopefully, they see that carefully building power into your experiment isn’t a task, it’s the task of experimental design.\nAs a concluding example, here’s the same dynamic plot for a study with about 98% power. Notice how “safe” this study feels compared to the one above with 80% power. I think this plot does a good job a translating probabilities into an appropriate sense of “danger.”\n\n\nCode\n# study parameters\nse &lt;- 0.27  # 1/3.64\n\n# identify effects of interest\neoi &lt;- tribble(\n  ~Effect, ~Description,\n  true_effect, \"True Effect (known in this exercise)\"\n) \n\n# compute quantities of interest regarding power\neoi %&gt;%\n  mutate(Power = 1 - pnorm(1.64*se, Effect, se),\n         Power = scales::percent(Power, accuracy = 1),\n         `Type S` = retrodesign::type_s(Effect, se)$type_s,\n         `Type S` = scales::number(`Type S`, accuracy = 0.01),\n         `Type M` = retrodesign::type_m(Effect, se)$type_m,\n         `Type M` = scales::number(`Type M`, accuracy = 0.01),\n         Effect = scales::number(Effect, accuracy = 0.01)) %&gt;% \n  pivot_longer(cols = Effect:`Type M`) %&gt;%\n  kableExtra::kable(format = \"markdown\", col.names = NULL)\n\n# a data frame of studies \nests &lt;- tibble(study_id = 1:n_studies,\n               est = c(rnorm(n_studies, true_effect, se))) %&gt;%\n  mutate(reject_null = ifelse(est - 1.64*se &gt; 0, \"Yes\", \"No\")) \n\nanimate_data &lt;- bind_rows(\n  tibble(study_id = 1, est = NA),  # study_id = 1, est = NA\n  ests                             # combine dummy row with ests data frame from above\n  ) %&gt;%\n  mutate(group = 1:n())            # group (row index)\n\n# same ggplot, except three annotated changes\ngg_exp &lt;- ggplot(animate_data, aes(x = est,\n                         y = study_id, \n                         xmin = est - 1.64*se, \n                         xmax = est + 1.64*se, \n                         color = reject_null, \n                         group = group)) +\n  geom_vline(xintercept = true_effect, \n             linetype = \"dotted\") + \n  geom_vline(xintercept = 0) + \n  geom_point() + \n  geom_errorbarh(height = 0) + \n  geom_rug(sides = \"b\", \n           aes(x = est - 1.64*se, color = NULL), \n           alpha = 0.5, \n           length = unit(0.025, \"npc\")) + \n  scale_color_manual(values = c(\"No\" = \"#d95f02\", \"Yes\" = \"#1b9e77\")) +  \n  theme_bw() +\n    theme(panel.grid = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.title.y = element_blank()) + \n  labs(x = \"Estimate and 90% Confidence Interval\",\n       y = \"Study ID\", \n       color = \"Reject Null?\", \n       caption = \"Rug shows distribution of lower bound of confidence interval.\n                  Solid line shows zero.\n                  Dotted line shows the true effect.\") + \n  # new scales here\n  scale_x_continuous(expand = expansion(add = c(0.6, 1))) +\n  scale_y_continuous(expand = expansion(add = 2))\n\n# add dyamics to the plot\nanim &lt;- gg_exp +\n  transition_states(states = group) +\n  # how points enter\n  enter_drift(x_mod = 2) +\n  enter_grow() +\n  enter_recolor(color = \"black\") +\n  ease_aes(color = \"exponential-in\") +\n  # how points exit/remain\n  exit_fade(alpha = 0.3) +\n  shadow_mark(alpha = 0.3)\n\n# make magic happen!\nanimate(anim, duration = n_studies, fps = 10, \n        height = 6, width = 8, units = \"in\", res = 150)\n\n\n\n\n\nEffect\n1.00\n\n\nDescription\nTrue Effect (known in this exercise)\n\n\nPower\n98%\n\n\nType S\n0.00\n\n\nType M\n1.02\n\n\n\n\n\n\n\n\n\n Email List\n\n\nIf you like this content, you should subscribe to my email list. Each semester, I send around a dense digest of things I’m thinking about, finding useful, and working on. You can also subscribe to blog posts and working papers, if you want more frequent updates. You can also subscribe to blog posts via   RSS if you prefer that format.\n\n\n   Join the Email List! \n\n And if you found this fun or useful, don’t forget to like and retweet the tweet below! 🚀\n\n\nI use 🎲 and gganimate to help students \"feel\" the riskiness of under-powered tests.https://t.co/Qvredn4tPr\n\n— Carly 📈👨‍🏫📊 (@carlislerainey) June 5, 2023"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\nJun 1, 2023\n\n\nTeaching Confidence Intervals and Hypothesis Testing with gganimate\n\n\n20 min\n\n\n\n\nMay 25, 2023\n\n\nPower, Part II: What Do Confidence Intervals from Well-Powered Studies Look Like?\n\n\n9 min\n\n\n\n\nMay 21, 2023\n\n\nPower, Part I: Power Is for You, Not for Reviewer Two\n\n\n8 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Carlisle Rainey",
    "section": "",
    "text": "email\n  \n  \n    \n     CV\n  \n\n  \n  \nI’m an Associate Professor in the Department of Political Science at Florida State University. My research has appeared in the American Political Science Review, the American Journal of Political Science, Political Analysis, and other peer-reviewed journals. I teach courses in American politics, comparative politics, and political methodology.\n\nConnect\nIf want to stay up-to-date on my work, please join my email list. Each semester, I send around a brief summary of things I’m working on, finding useful, and thinking about. You can also subscribe to blog posts and working papers, if you want.\n\n  \n      Join My Email List!\n  \n\n\nYou can also subscribe to blog posts via RSS. I’m on Twitter and Mastodon. I publicly version-control many of research projects and courses on GitHub. I upload my talks to Speaker Deck. I’m on Google Scholar, ORCID and OSF."
  },
  {
    "objectID": "papers2/index.html",
    "href": "papers2/index.html",
    "title": "Papers",
    "section": "",
    "text": "Pandemic Pass? Treaty Derogations and Human Rights Practices During COVID-19 \n    \n    Suparna Chaudhry, Audrey L. Comstock, Andrew Heiss. \n    . \n    .\n    \n     PDF \n     Details \n  \n  \n       The Marginal Effects Zoo: A Guide to Interpretation Using `marginaleffects` for R \n    \n    Vincent Arel-Bundock, Noah Greifer, Andrew Heiss. \n    . \n    .\n    \n     PDF \n     Details \n  \n\n\nNo matching items"
  },
  {
    "objectID": "papers2/index.html#unpublished-papers",
    "href": "papers2/index.html#unpublished-papers",
    "title": "Papers",
    "section": "",
    "text": "Pandemic Pass? Treaty Derogations and Human Rights Practices During COVID-19 \n    \n    Suparna Chaudhry, Audrey L. Comstock, Andrew Heiss. \n    . \n    .\n    \n     PDF \n     Details \n  \n  \n       The Marginal Effects Zoo: A Guide to Interpretation Using `marginaleffects` for R \n    \n    Vincent Arel-Bundock, Noah Greifer, Andrew Heiss. \n    . \n    .\n    \n     PDF \n     Details \n  \n\n\nNo matching items"
  },
  {
    "objectID": "papers2/index.html#published-papers",
    "href": "papers2/index.html#published-papers",
    "title": "Papers",
    "section": "Published Papers",
    "text": "Published Papers\n\n\n    \n      \n      \n    \n\n\n  \n       Compression and Conditional Effects: A Product Term Is Essential When Using Logistic Regression to Test for Interaction  \n    \n    Carlisle Rainey. \n    2016. \n    Political Science Research and Methods.\n    \n     PDF \n     Details \n  \n  \n       The Question(s) of Political Knowledge \n    \n    Jason Barabas, Jennifer Jerit, William Pollock, Carlisle Rainey. \n    2014. \n    American Political Science Review.\n    \n     PDF \n     Details \n  \n  \n       Arguing for a Negligible Effect \n    \n    Carlisle Rainey. \n    2014. \n    American Journal of Political Science.\n    \n     PDF \n     Details \n  \n\n\nNo matching items"
  },
  {
    "objectID": "papers2/published-papers/2014-barabas-questions/index.html",
    "href": "papers2/published-papers/2014-barabas-questions/index.html",
    "title": "The Question(s) of Political Knowledge",
    "section": "",
    "text": "Paper\nAppendix (preprint)\nDataverse\nGitHub"
  },
  {
    "objectID": "papers2/published-papers/2014-barabas-questions/index.html#important-links",
    "href": "papers2/published-papers/2014-barabas-questions/index.html#important-links",
    "title": "The Question(s) of Political Knowledge",
    "section": "",
    "text": "Paper\nAppendix (preprint)\nDataverse\nGitHub"
  },
  {
    "objectID": "papers2/published-papers/2014-barabas-questions/index.html#data-and-code",
    "href": "papers2/published-papers/2014-barabas-questions/index.html#data-and-code",
    "title": "The Question(s) of Political Knowledge",
    "section": "Data and code",
    "text": "Data and code\nThis projects code and data are availabe through Dataverse and GitHub."
  },
  {
    "objectID": "papers2/published-papers/2014-barabas-questions/index.html#citation",
    "href": "papers2/published-papers/2014-barabas-questions/index.html#citation",
    "title": "The Question(s) of Political Knowledge",
    "section": "Citation",
    "text": "Citation\n\nBarabas, Jason, Jennifer Jerit, William Pollock, and Carlisle Rainey. 2014. “The Question(s) of Political Knowledge.” American Political Science Review 108(4): 840-855.\n Add to Zotero \n\n@article{rainey-2014-arguing,\n    author = {Carlisle Rainey},\n    year = {2014}\n    journal = {American Journal of Political Science},\n    month = {10},\n    number = {S5},\n    pages = {45--58},\n    title = {Arguing for a Negligible Effect},\n    volume = {58},\n    issue = {4}\n    doi = {10.1111/1758-5899.12984},\n}"
  },
  {
    "objectID": "papers2/published-papers/2014-rainey-arguing/index.html",
    "href": "papers2/published-papers/2014-rainey-arguing/index.html",
    "title": "Arguing for a Negligible Effect",
    "section": "",
    "text": "Paper\nAppendix (preprint)\nDataverse\nGitHub"
  },
  {
    "objectID": "papers2/published-papers/2014-rainey-arguing/index.html#important-links",
    "href": "papers2/published-papers/2014-rainey-arguing/index.html#important-links",
    "title": "Arguing for a Negligible Effect",
    "section": "",
    "text": "Paper\nAppendix (preprint)\nDataverse\nGitHub"
  },
  {
    "objectID": "papers2/published-papers/2014-rainey-arguing/index.html#data-and-code",
    "href": "papers2/published-papers/2014-rainey-arguing/index.html#data-and-code",
    "title": "Arguing for a Negligible Effect",
    "section": "Data and code",
    "text": "Data and code\nThis projects code and data are availabe through Dataverse and GitHub."
  },
  {
    "objectID": "papers2/published-papers/2014-rainey-arguing/index.html#citation",
    "href": "papers2/published-papers/2014-rainey-arguing/index.html#citation",
    "title": "Arguing for a Negligible Effect",
    "section": "Citation",
    "text": "Citation\n\nRainey, Carlisle. 2014. “Arguing for a Negligible Effect.” American Journal of Political Science 58(4): 1083-1091.\n Add to Zotero \n\n@article{rainey-2014-arguing,\n    author = {Carlisle Rainey},\n    year = {2014}\n    journal = {American Journal of Political Science},\n    month = {10},\n    number = {S5},\n    pages = {45--58},\n    title = {Arguing for a Negligible Effect},\n    volume = {58},\n    issue = {4}\n    doi = {10.1111/1758-5899.12984},\n}"
  },
  {
    "objectID": "papers2/published-papers/2016-rainey-compression/index.html",
    "href": "papers2/published-papers/2016-rainey-compression/index.html",
    "title": "Compression and Conditional Effects",
    "section": "",
    "text": "PDF\nAppendix (preprint)\nDataverse\nGitHub"
  },
  {
    "objectID": "papers2/published-papers/2016-rainey-compression/index.html#important-links",
    "href": "papers2/published-papers/2016-rainey-compression/index.html#important-links",
    "title": "Compression and Conditional Effects",
    "section": "",
    "text": "PDF\nAppendix (preprint)\nDataverse\nGitHub"
  },
  {
    "objectID": "papers2/published-papers/2016-rainey-compression/index.html#data-and-code",
    "href": "papers2/published-papers/2016-rainey-compression/index.html#data-and-code",
    "title": "Compression and Conditional Effects",
    "section": "Data and code",
    "text": "Data and code\nThis projects code and data are availabe through Dataverse and GitHub."
  },
  {
    "objectID": "papers2/published-papers/2016-rainey-compression/index.html#citation",
    "href": "papers2/published-papers/2016-rainey-compression/index.html#citation",
    "title": "Compression and Conditional Effects",
    "section": "Citation",
    "text": "Citation\nRainey, Carlisle. 2016. “Compression and Conditional Effects: A Product Term Is Essential When Using Logistic Regression to Test for Interaction.” Political Science Research and Methods.4(3): 621-639.\n\n  Add to Zotero \n\n@article{rainey-2014-arguing,\n    author = {Carlisle Rainey},\n    year = {2014}\n    journal = {American Journal of Political Science},\n    pages = {45--58},\n    title = {Arguing for a Negligible Effect},\n    volume = {58},\n    issue = {4}\n    doi = {10.1111/1758-5899.12984},\n    month = {10}\n}"
  },
  {
    "objectID": "papers2/published-papers/2016-rainey-compression/index.html#related-papers",
    "href": "papers2/published-papers/2016-rainey-compression/index.html#related-papers",
    "title": "Compression and Conditional Effects",
    "section": "Related Papers",
    "text": "Related Papers\n\nBerry, William D., Jaqueline H. R. DeMeritt, and Justin Esarey. 2010. “Testing for Interaction in Binary Logit and Probit Models: Is a Product Term Essential?.” American Journal of Political Science 54: 248-266. DOI\nBerry, William D., Jaqueline H. R. DeMeritt, and Justin Esarey. 2016. “Bias and Overconfidence in Parametric Models of Interactive Processes.” American Journal of Political Science 60(2): 521–539. DOI"
  },
  {
    "objectID": "papers2/unpublished-papers/arel-bundock-greifer-heiss-mfxplainer/index.html",
    "href": "papers2/unpublished-papers/arel-bundock-greifer-heiss-mfxplainer/index.html",
    "title": "The Marginal Effects Zoo: A Guide to Interpretation Using marginaleffects for R",
    "section": "",
    "text": "GitHub repository"
  },
  {
    "objectID": "papers2/unpublished-papers/arel-bundock-greifer-heiss-mfxplainer/index.html#important-links",
    "href": "papers2/unpublished-papers/arel-bundock-greifer-heiss-mfxplainer/index.html#important-links",
    "title": "The Marginal Effects Zoo: A Guide to Interpretation Using marginaleffects for R",
    "section": "",
    "text": "GitHub repository"
  },
  {
    "objectID": "papers2/unpublished-papers/arel-bundock-greifer-heiss-mfxplainer/index.html#abstract",
    "href": "papers2/unpublished-papers/arel-bundock-greifer-heiss-mfxplainer/index.html#abstract",
    "title": "The Marginal Effects Zoo: A Guide to Interpretation Using marginaleffects for R",
    "section": "Abstract",
    "text": "Abstract\nAnalysts often transform their models’ parameter estimates to report more meaningful and interpretable quantities of interest. This article presents a simple conceptual framework to describe a vast array of estimands which are reported under imprecise and inconsistent terminology across disciplines: predictions, marginal predictions, marginal means, marginal effects, conditional effects, slopes, contrasts, risk ratios, etc. We introduce marginaleffects, an R package which offers a simple and powerful interface to compute all of those quantites, and to conduct hypothesis tests on them. marginaleffects is lightweight; extensible; it works well in combination with other R packages; and it supports over 70 classes of models, including Generalized Linear, Generalized Additive, Mixed Effects, and Bayesian models."
  },
  {
    "objectID": "papers2/unpublished-papers/chaudhry-comstock-heiss-pandemic-pass/index.html",
    "href": "papers2/unpublished-papers/chaudhry-comstock-heiss-pandemic-pass/index.html",
    "title": "Pandemic Pass? Treaty Derogations and Human Rights Practices During COVID-19",
    "section": "",
    "text": "Statistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "papers2/unpublished-papers/chaudhry-comstock-heiss-pandemic-pass/index.html#important-links",
    "href": "papers2/unpublished-papers/chaudhry-comstock-heiss-pandemic-pass/index.html#important-links",
    "title": "Pandemic Pass? Treaty Derogations and Human Rights Practices During COVID-19",
    "section": "",
    "text": "Statistical analysis notebook\nGitHub repository"
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Research",
    "section": "",
    "text": "Rainey, Carlisle. “Hypothesis Tests Under Separation.” Conditionally accepted at Political Analysis. [Paper][GitHub] [OSF] [Dataverse]\nClifford, Scott, Thomas Leeper, and Carlisle Rainey. “Generalizing Survey Experiments Using Topic Sampling: Theory, Evaluations, and Extensions.” [Draft]\nRainey, Carlisle, Harley Roe, and Qing Wang. “A Latent Measure of Dissent.” [GitHub]"
  },
  {
    "objectID": "research/index.html#unpublished-papers",
    "href": "research/index.html#unpublished-papers",
    "title": "Research",
    "section": "",
    "text": "Rainey, Carlisle. “Hypothesis Tests Under Separation.” Conditionally accepted at Political Analysis. [Paper][GitHub] [OSF] [Dataverse]\nClifford, Scott, Thomas Leeper, and Carlisle Rainey. “Generalizing Survey Experiments Using Topic Sampling: Theory, Evaluations, and Extensions.” [Draft]\nRainey, Carlisle, Harley Roe, and Qing Wang. “A Latent Measure of Dissent.” [GitHub]"
  },
  {
    "objectID": "research/index.html#published-papers",
    "href": "research/index.html#published-papers",
    "title": "Research",
    "section": "Published Papers",
    "text": "Published Papers\n\nForthcoming\n\nClifford, Scott, Thomas Leeper, and Carlisle Rainey. “Generalizing Survey Experiments Using Topic Sampling: An Application to Party Cues.” Forthcoming in Political Behavior. [Paper]\nRainey, Carlisle. “A Careful Consideration of CLARIFY: Simulation-Induced Bias in Point Estimates of Quantities of Interest.” Forthcoming in Political Science Research and Methods. [GitHub]\n\n\n\n2021\n\nMcCaskey, Kelly and Carlisle Rainey. 2021 “Estimating Logit Models with Small Samples.” Political Science Research and Methods 9(3) 549-564. [Paper] [GitHub]\n\n\n\n2020\n\nBaissa, Daniel K. and Carlisle Rainey. 2020. “When BLUE Is Not Best: Non-Normal Errors and the Linear Model.” Political Science Research and Methods 8(1) 136-148. [Paper] [GitHub]\n\n\n\n2018\n\nRainey, Carlisle and Robert Jackson. “Unreliable Inferences about Unobservable Processes: A Critique of Partial Observability Models.” Political Science Research and Methods 6(2): 381-391. [Paper] [Journal [GitHub]\n\n\n\n2017\n\nRainey, Carlisle. 2017. “Transformation-Induced Bias: Unbiased Coefficients Do Not Imply Unbiased Quantities of Interest.” Political Analysis 25(3): 402-409. [Paper] [Journal] [Dataverse] [GitHub]\n\n\n\n2016\n\nRainey, Carlisle. 2016. “Dealing with Separation in Logistic Regression Models.” Political Analysis. 24(3): 339-355. [Paper] [Journal] [Slides] [Appendix] [GitHub] [Software]\nRainey, Carlisle. 2016. “Compression and Conditional Effects: A Product Term Is Essential When Using Logistic Regression to Test for Interaction.” Political Science Research and Methods. 4(3): 621-639. [Paper] [Appendix] [Journal] [GitHub]\nRainey, Carlisle. 2016. “Does District Magnitude Matter: The Case of Taiwan.” Electoral Studies. 41: 202-212. [Paper] [Journal] [Dataverse] [GitHub]\n\n\n\n2015\n\nMcCaskey, Kelly and Carlisle Rainey. 2015. “Substantive Importance and the Veil of Statistical Significance.” Statistics, Politics, and Policy 6(1-2): 77-96. [Paper] [Journal] [GitHub]\nClifford, Scott, Jennifer Jerit, Matt Motyl, and Carlisle Rainey. 2015. “Moral Concerns and Culture War Attitudes: Investigating the Influence of Elite Rhetoric.” Political Communication. 32(2): 229-248. [Paper] [Journal]\nRainey, Carlisle. 2015. “Strategic Mobilization: Why Proportional Representation Decreases Voter Mobilization.” Electoral Studies. 37(1): 86-98. [Paper] [Appendix] [Journal] [Dataverse] [GitHub]\n\n\n\n2014\n\nRainey, Carlisle. 2014. “Arguing for a Negligible Effect.” American Journal of Political Science 58(4): 1083-1091. [Paper] [Appendix] [Journal] [Dataverse] [GitHub]\nBarabas, Jason, Jennifer Jerit, William Pollock, and Carlisle Rainey. 2014. “The Question(s) of Political Knowledge.” American Political Science Review 108(4): 840-855. [Paper] [Journal] [Dataverse] [GitHub]\nBarrilleaux, Charles and Carlisle Rainey. 2014. “The Politics of Need: Examining Governors’ Decisions to Oppose the ‘Obamacare’ Medicaid Expansion.” State Politics and Policy Quarterly. 14(4): 437-460. [Paper] [Appendix] [Journal] [Dataverse] [GitHub]"
  },
  {
    "objectID": "talks/2023-topic-sampling/index.html",
    "href": "talks/2023-topic-sampling/index.html",
    "title": "Topic Sampling @ EPOVB 2023",
    "section": "",
    "text": "The two papers are below.\n\nClifford, Scott, Thomas Leeper, and Carlisle Rainey. “Generalizing Survey Experiments Using Topic Sampling: An Application to Party Cues.” Forthcoming in Political Behavior. [Draft]\nClifford, Scott, Thomas Leeper, and Carlisle Rainey. “Generalizing Survey Experiments Using Topic Sampling: Theory, Evaluations, and Extensions.” [Draft]\n\nSlides, with transitions [Dropbox] and without [Dropbox]"
  },
  {
    "objectID": "talks/index.html",
    "href": "talks/index.html",
    "title": "Talks",
    "section": "",
    "text": "Topic Sampling @ EPOVB 2023\n\n\nA talk at the 2023 meeting of the Election, Public Opinion, and Voting Behavior section of APSA.\n\n\n\nCarlisle Rainey\n\n\nMar 4, 2023\n\n\n\n\n\n\n\n\nNo matching items\n\n\nFor older talks, see Speaker Deck."
  },
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Teaching",
    "section": "",
    "text": "Advanced Quantitative Methods"
  },
  {
    "objectID": "teaching/index.html#current-classes",
    "href": "teaching/index.html#current-classes",
    "title": "Teaching",
    "section": "",
    "text": "Advanced Quantitative Methods"
  },
  {
    "objectID": "teaching/index.html#archived-classes",
    "href": "teaching/index.html#archived-classes",
    "title": "Teaching",
    "section": "Archived Classes",
    "text": "Archived Classes\n\nPOS 3713: Introduction to Political Science Research Methods\nPOS 5737: Introduction to Data Analysis\nPOLS 209 at TAMU: Research Methods"
  },
  {
    "objectID": "teaching/pols-209/index.html",
    "href": "teaching/pols-209/index.html",
    "title": "POLS 209",
    "section": "",
    "text": "syllabus [pdf]\nassigned exercises from FPP [pdf]\ndata sets [zip]\n\nWriting Assignment 1 [Dropbox]\n\nRubric [Dropbox]\n\nWriting Assignment 2 [Dropbox] [checklist]\n\n\n\nAllen 2051, MW, 11am-12pm. Please reserve your slot here.\n\n\n\nTasks (bullets) below the date should be completed before the next class (unless a due-date is listed).\nAugust 30: Introduction\n. Read the syllabus carefully.\n. Install R and RStudio (complete by Sep 6).\n. Order textbook (have by Sep 11).\n. Obtain a pocket calculator (have by Sep 11).\nSep 1: Questions\n. Review Notes on Questions [pdf]. Complete exercises.\nSep 4: Models\n. Review Notes on Models [pdf] (and slides from lecture [pdf]). Complete exercises.\n. Install R and RStudio.\nSep 6: Model-Building Exercise\nSep 8: Computing in R\nSep 11: Computing in R, part 2\n. Read Notes on Computing in R [pdf]. Complete exercises.\nSep 13: Loading Data in R . Review your notes from my Lecture on Data Frames [pdf].\n. Review Notes on Loading Data in R [pdf]. Complete exercises.\n. Complete Computing Assignment 1 [pdf].\nSep 15: Causal Inference and Histograms\n. Have textbook and calculator.\n. Read chs. 1-2 of FPP. Complete assigned exercises (remember that assigned exercises are [at the top]).\n. Review your notes from my Lecture on Causal Inference [pdf]. Complete exercises at the end.\n. Read ch. 3 of FPP. Complete assigned exercises.\nSep 18: Histograms in R\n. Review Notes on Histograms in R [pdf]. Complete exercises.\n. Begin to work on Computing Assignment 2 [pdf] (due Sep 22).\nSep 20: Average and SD\n. Read ch. 4 of FPP. Complete assigned exercises.\n. Submit Computing Assignment 2 [pdf].\nSep 22: Catch-Up Day\n. Begin working on Writing Assignment 1 [Dropbox]. Come prepared with questions.\nSep 25: Average and SD in R\n. Review your notes from my Lecture on Average and SD in R [pdf].\n. Review Notes on Average and SD in R [pdf]. Complete exercises.\n. Begin working on Computing Assignment 3 [pdf].\nSep 27: Normal Approximation\n. Read ch. 5 of FPP. Complete assigned exercises.\n. Finish the leadership extremity exercise we began in class [pdf].\n. Submit Computing Assignment 3 [pdf].\nSep 29: Measurement\n. Read ch. 6 of FPP. Complete assigned exercises.\n. Prepare for Exam 1. Focus on the review exercises from notes, slides, and textbook.\nOct 2: Review for Exam 1\n. Study Guide [Google Doc]\nOct 4: Exam 1 (bring pencil, pocket calculator, and small green Scantron)\n. Read “Politics and the English Language” [pdf]. Expect a reading quiz.\nOct 6: Discussion of “Politics and the English Language”Scatterplots and Correlation, Part 1\n. Read chs. 7-8 of FPP. Complete assigned exercises.\n. See this sheet for p. 137, #9(a) [Google Sheet].\nOct 9: Measurement, Part 2\n. Review your notes from my Lecture on Measurement [pdf]. Complete exercises at the end.\nOct 11: Scatterplots and Correlation in R\n. Review Notes on Scatterplots and Correlation in R [pdf]. Complete exercises.\n. Read ch. 9 of FPP. Complete assigned exercises.\n. Play this game [web] and track your performance.\n. Don’t forget to submit Writing Assignment 1.\nOct 13: Regression, Part 1\n. Read ch. 10 of FPP. Complete assigned exercises.\nOct 16: Regression, Part 2\n. Read ch. 11 of FPP. Complete assigned exercises.\n. Note that I accidentally assigned ch. 11 on the 13th as well. I meant to assign ch. 10. Make sure you’ve finished both ch. 10 and 11.\n. Begin Computing Assignment 4 [pdf].\nOct 18: Regression, Part 3\n. Read ch. 12 of FPP. Complete assigned exercises.\n. Submit Computing Assignment 4 [pdf].\n. Submit peer review for Writing Assignment 1. Details on eCampus.\nOct 20: Regression in R\n. Review Notes on Regression in R [pdf]. Complete exercises.\nOct 23: Multiple Regression, Part 1\nOct 25: Multiple Regression, Part 2\n. Review your notes on my lecture on econometric notation. Make sure you can explain the similarities and differences between FPP’s simple notation and the more complicated econometric notation. What are the two advantages of econometric notation?\n. Review your notes on my lecture [pdf] on regression for prediction.\n. Read these notes [pdf] for more detail on prediction and BIC.\n. Complete Computing Assignment 5 [pdf].\n. Read “5 Steps toward Constructing a Better Sentence” [web].\n. Read “5 Steps toward Writing an Effective Paragraph” [web].\n. Use this example response memo [pdf] when writing your own response memo.\nOct 27: “Breakfast with Ben”\nOct 30: Exam 2 Review\n. Study Guide [Google Doc]\nNov 1: Exam 2 (bring pencil, pocket calculator, and small green Scantron)\n. Final submission of Writing Assignment 1.\nNov 3: Probability, Part 1\n. We’ll look at the Federalist papers [web] in class.\n. Read ch. 13 of FPP. Complete assigned exercises.\nNov 6: Probability, Part 2\n. Read ch. 14 of FPP. Complete assigned exercises.\n. Submit peer review by noon on July 26.\nNov 8: Law of Averages\n. Read ch. 16 of FPP. Complete assigned exercises.\nNov 10: Expected Value and Standard Error\n. Read ch. 17 of FPP. Complete assigned exercises.\n. Begin Writing Assignment 2 [Dropbox]\nNov 13: Normal Approximation for Probability Histograms\n. Read ch. 18 of FPP. Complete assigned exercises. . In class, fill in this table [Google Sheet].\n. In class, use these slides as needed [Google Slides].\nNov 15: Sample Surveys, Part 1\n. Read ch. 19 of FPP. Complete assigned exercises.\nNov 17: Sample Surveys, Part 2\n. Read ch. 20 of FPP. Complete assigned exercises.\nNov 20: FSAB Panel Day\n. Catch-up on any review exercises you haven’t done.\n. Writing Assignment 2 due (postponed to Tuesday, Nov. 28).\nNov 22: No Class (Reading Day)\nNov 24: No Class (Thanksgiving Holiday)\nNov 27: Catch-up Day\n. Make sure you’ve read through ch. 20 of FPP and completed assigned exercises.\nNov 29: The Accuracy of Percentages\n. Read ch. 21 of FPP. Complete assigned exercises.\nDec 1: The Accuracy of Averages\n. Read ch. 23 of FPP. Complete assigned exercises.\nDec 4: Hypothesis Tests . Read ch. 26 of FPP. Complete assigned exercises.\nDec 6: Final Exam Review\n. Example problem for hypothesis test and 95% CI for percent [Dropbox].\n. Final submission of Writing Assignment 2 due.\n. Study Guide [Google Doc]\nDec 8 or 11: Final Exam (bring pencil, pocket calculator, and small green Scantron) . For 901 (8:35-9:25am), 10am-12pm on Dec 8\n. For 902 (9:45-10:35am), 8-10am on Dec 11"
  },
  {
    "objectID": "teaching/pols-209/index.html#office-hours",
    "href": "teaching/pols-209/index.html#office-hours",
    "title": "POLS 209",
    "section": "",
    "text": "Allen 2051, MW, 11am-12pm. Please reserve your slot here."
  },
  {
    "objectID": "teaching/pols-209/index.html#schedule",
    "href": "teaching/pols-209/index.html#schedule",
    "title": "POLS 209",
    "section": "",
    "text": "Tasks (bullets) below the date should be completed before the next class (unless a due-date is listed).\nAugust 30: Introduction\n. Read the syllabus carefully.\n. Install R and RStudio (complete by Sep 6).\n. Order textbook (have by Sep 11).\n. Obtain a pocket calculator (have by Sep 11).\nSep 1: Questions\n. Review Notes on Questions [pdf]. Complete exercises.\nSep 4: Models\n. Review Notes on Models [pdf] (and slides from lecture [pdf]). Complete exercises.\n. Install R and RStudio.\nSep 6: Model-Building Exercise\nSep 8: Computing in R\nSep 11: Computing in R, part 2\n. Read Notes on Computing in R [pdf]. Complete exercises.\nSep 13: Loading Data in R . Review your notes from my Lecture on Data Frames [pdf].\n. Review Notes on Loading Data in R [pdf]. Complete exercises.\n. Complete Computing Assignment 1 [pdf].\nSep 15: Causal Inference and Histograms\n. Have textbook and calculator.\n. Read chs. 1-2 of FPP. Complete assigned exercises (remember that assigned exercises are [at the top]).\n. Review your notes from my Lecture on Causal Inference [pdf]. Complete exercises at the end.\n. Read ch. 3 of FPP. Complete assigned exercises.\nSep 18: Histograms in R\n. Review Notes on Histograms in R [pdf]. Complete exercises.\n. Begin to work on Computing Assignment 2 [pdf] (due Sep 22).\nSep 20: Average and SD\n. Read ch. 4 of FPP. Complete assigned exercises.\n. Submit Computing Assignment 2 [pdf].\nSep 22: Catch-Up Day\n. Begin working on Writing Assignment 1 [Dropbox]. Come prepared with questions.\nSep 25: Average and SD in R\n. Review your notes from my Lecture on Average and SD in R [pdf].\n. Review Notes on Average and SD in R [pdf]. Complete exercises.\n. Begin working on Computing Assignment 3 [pdf].\nSep 27: Normal Approximation\n. Read ch. 5 of FPP. Complete assigned exercises.\n. Finish the leadership extremity exercise we began in class [pdf].\n. Submit Computing Assignment 3 [pdf].\nSep 29: Measurement\n. Read ch. 6 of FPP. Complete assigned exercises.\n. Prepare for Exam 1. Focus on the review exercises from notes, slides, and textbook.\nOct 2: Review for Exam 1\n. Study Guide [Google Doc]\nOct 4: Exam 1 (bring pencil, pocket calculator, and small green Scantron)\n. Read “Politics and the English Language” [pdf]. Expect a reading quiz.\nOct 6: Discussion of “Politics and the English Language”Scatterplots and Correlation, Part 1\n. Read chs. 7-8 of FPP. Complete assigned exercises.\n. See this sheet for p. 137, #9(a) [Google Sheet].\nOct 9: Measurement, Part 2\n. Review your notes from my Lecture on Measurement [pdf]. Complete exercises at the end.\nOct 11: Scatterplots and Correlation in R\n. Review Notes on Scatterplots and Correlation in R [pdf]. Complete exercises.\n. Read ch. 9 of FPP. Complete assigned exercises.\n. Play this game [web] and track your performance.\n. Don’t forget to submit Writing Assignment 1.\nOct 13: Regression, Part 1\n. Read ch. 10 of FPP. Complete assigned exercises.\nOct 16: Regression, Part 2\n. Read ch. 11 of FPP. Complete assigned exercises.\n. Note that I accidentally assigned ch. 11 on the 13th as well. I meant to assign ch. 10. Make sure you’ve finished both ch. 10 and 11.\n. Begin Computing Assignment 4 [pdf].\nOct 18: Regression, Part 3\n. Read ch. 12 of FPP. Complete assigned exercises.\n. Submit Computing Assignment 4 [pdf].\n. Submit peer review for Writing Assignment 1. Details on eCampus.\nOct 20: Regression in R\n. Review Notes on Regression in R [pdf]. Complete exercises.\nOct 23: Multiple Regression, Part 1\nOct 25: Multiple Regression, Part 2\n. Review your notes on my lecture on econometric notation. Make sure you can explain the similarities and differences between FPP’s simple notation and the more complicated econometric notation. What are the two advantages of econometric notation?\n. Review your notes on my lecture [pdf] on regression for prediction.\n. Read these notes [pdf] for more detail on prediction and BIC.\n. Complete Computing Assignment 5 [pdf].\n. Read “5 Steps toward Constructing a Better Sentence” [web].\n. Read “5 Steps toward Writing an Effective Paragraph” [web].\n. Use this example response memo [pdf] when writing your own response memo.\nOct 27: “Breakfast with Ben”\nOct 30: Exam 2 Review\n. Study Guide [Google Doc]\nNov 1: Exam 2 (bring pencil, pocket calculator, and small green Scantron)\n. Final submission of Writing Assignment 1.\nNov 3: Probability, Part 1\n. We’ll look at the Federalist papers [web] in class.\n. Read ch. 13 of FPP. Complete assigned exercises.\nNov 6: Probability, Part 2\n. Read ch. 14 of FPP. Complete assigned exercises.\n. Submit peer review by noon on July 26.\nNov 8: Law of Averages\n. Read ch. 16 of FPP. Complete assigned exercises.\nNov 10: Expected Value and Standard Error\n. Read ch. 17 of FPP. Complete assigned exercises.\n. Begin Writing Assignment 2 [Dropbox]\nNov 13: Normal Approximation for Probability Histograms\n. Read ch. 18 of FPP. Complete assigned exercises. . In class, fill in this table [Google Sheet].\n. In class, use these slides as needed [Google Slides].\nNov 15: Sample Surveys, Part 1\n. Read ch. 19 of FPP. Complete assigned exercises.\nNov 17: Sample Surveys, Part 2\n. Read ch. 20 of FPP. Complete assigned exercises.\nNov 20: FSAB Panel Day\n. Catch-up on any review exercises you haven’t done.\n. Writing Assignment 2 due (postponed to Tuesday, Nov. 28).\nNov 22: No Class (Reading Day)\nNov 24: No Class (Thanksgiving Holiday)\nNov 27: Catch-up Day\n. Make sure you’ve read through ch. 20 of FPP and completed assigned exercises.\nNov 29: The Accuracy of Percentages\n. Read ch. 21 of FPP. Complete assigned exercises.\nDec 1: The Accuracy of Averages\n. Read ch. 23 of FPP. Complete assigned exercises.\nDec 4: Hypothesis Tests . Read ch. 26 of FPP. Complete assigned exercises.\nDec 6: Final Exam Review\n. Example problem for hypothesis test and 95% CI for percent [Dropbox].\n. Final submission of Writing Assignment 2 due.\n. Study Guide [Google Doc]\nDec 8 or 11: Final Exam (bring pencil, pocket calculator, and small green Scantron) . For 901 (8:35-9:25am), 10am-12pm on Dec 8\n. For 902 (9:45-10:35am), 8-10am on Dec 11"
  }
]