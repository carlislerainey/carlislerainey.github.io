[
  {
    "objectID": "teaching/pols-209/index.html",
    "href": "teaching/pols-209/index.html",
    "title": "POLS 209",
    "section": "",
    "text": "syllabus [pdf]\nassigned exercises from FPP [pdf]\ndata sets [zip]\n\nWriting Assignment 1 [Dropbox]\n\nRubric [Dropbox]\n\nWriting Assignment 2 [Dropbox] [checklist]\n\n\n\nAllen 2051, MW, 11am-12pm. Please reserve your slot here.\n\n\n\nTasks (bullets) below the date should be completed before the next class (unless a due-date is listed).\nAugust 30: Introduction\n. Read the syllabus carefully.\n. Install R and RStudio (complete by Sep 6).\n. Order textbook (have by Sep 11).\n. Obtain a pocket calculator (have by Sep 11).\nSep 1: Questions\n. Review Notes on Questions [pdf]. Complete exercises.\nSep 4: Models\n. Review Notes on Models [pdf] (and slides from lecture [pdf]). Complete exercises.\n. Install R and RStudio.\nSep 6: Model-Building Exercise\nSep 8: Computing in R\nSep 11: Computing in R, part 2\n. Read Notes on Computing in R [pdf]. Complete exercises.\nSep 13: Loading Data in R . Review your notes from my Lecture on Data Frames [pdf].\n. Review Notes on Loading Data in R [pdf]. Complete exercises.\n. Complete Computing Assignment 1 [pdf].\nSep 15: Causal Inference and Histograms\n. Have textbook and calculator.\n. Read chs. 1-2 of FPP. Complete assigned exercises (remember that assigned exercises are [at the top]).\n. Review your notes from my Lecture on Causal Inference [pdf]. Complete exercises at the end.\n. Read ch. 3 of FPP. Complete assigned exercises.\nSep 18: Histograms in R\n. Review Notes on Histograms in R [pdf]. Complete exercises.\n. Begin to work on Computing Assignment 2 [pdf] (due Sep 22).\nSep 20: Average and SD\n. Read ch. 4 of FPP. Complete assigned exercises.\n. Submit Computing Assignment 2 [pdf].\nSep 22: Catch-Up Day\n. Begin working on Writing Assignment 1 [Dropbox]. Come prepared with questions.\nSep 25: Average and SD in R\n. Review your notes from my Lecture on Average and SD in R [pdf].\n. Review Notes on Average and SD in R [pdf]. Complete exercises.\n. Begin working on Computing Assignment 3 [pdf].\nSep 27: Normal Approximation\n. Read ch. 5 of FPP. Complete assigned exercises.\n. Finish the leadership extremity exercise we began in class [pdf].\n. Submit Computing Assignment 3 [pdf].\nSep 29: Measurement\n. Read ch. 6 of FPP. Complete assigned exercises.\n. Prepare for Exam 1. Focus on the review exercises from notes, slides, and textbook.\nOct 2: Review for Exam 1\n. Study Guide [Google Doc]\nOct 4: Exam 1 (bring pencil, pocket calculator, and small green Scantron)\n. Read “Politics and the English Language” [pdf]. Expect a reading quiz.\nOct 6: Discussion of “Politics and the English Language”Scatterplots and Correlation, Part 1\n. Read chs. 7-8 of FPP. Complete assigned exercises.\n. See this sheet for p. 137, #9(a) [Google Sheet].\nOct 9: Measurement, Part 2\n. Review your notes from my Lecture on Measurement [pdf]. Complete exercises at the end.\nOct 11: Scatterplots and Correlation in R\n. Review Notes on Scatterplots and Correlation in R [pdf]. Complete exercises.\n. Read ch. 9 of FPP. Complete assigned exercises.\n. Play this game [web] and track your performance.\n. Don’t forget to submit Writing Assignment 1.\nOct 13: Regression, Part 1\n. Read ch. 10 of FPP. Complete assigned exercises.\nOct 16: Regression, Part 2\n. Read ch. 11 of FPP. Complete assigned exercises.\n. Note that I accidentally assigned ch. 11 on the 13th as well. I meant to assign ch. 10. Make sure you’ve finished both ch. 10 and 11.\n. Begin Computing Assignment 4 [pdf].\nOct 18: Regression, Part 3\n. Read ch. 12 of FPP. Complete assigned exercises.\n. Submit Computing Assignment 4 [pdf].\n. Submit peer review for Writing Assignment 1. Details on eCampus.\nOct 20: Regression in R\n. Review Notes on Regression in R [pdf]. Complete exercises.\nOct 23: Multiple Regression, Part 1\nOct 25: Multiple Regression, Part 2\n. Review your notes on my lecture on econometric notation. Make sure you can explain the similarities and differences between FPP’s simple notation and the more complicated econometric notation. What are the two advantages of econometric notation?\n. Review your notes on my lecture [pdf] on regression for prediction.\n. Read these notes [pdf] for more detail on prediction and BIC.\n. Complete Computing Assignment 5 [pdf].\n. Read “5 Steps toward Constructing a Better Sentence” [web].\n. Read “5 Steps toward Writing an Effective Paragraph” [web].\n. Use this example response memo [pdf] when writing your own response memo.\nOct 27: “Breakfast with Ben”\nOct 30: Exam 2 Review\n. Study Guide [Google Doc]\nNov 1: Exam 2 (bring pencil, pocket calculator, and small green Scantron)\n. Final submission of Writing Assignment 1.\nNov 3: Probability, Part 1\n. We’ll look at the Federalist papers [web] in class.\n. Read ch. 13 of FPP. Complete assigned exercises.\nNov 6: Probability, Part 2\n. Read ch. 14 of FPP. Complete assigned exercises.\n. Submit peer review by noon on July 26.\nNov 8: Law of Averages\n. Read ch. 16 of FPP. Complete assigned exercises.\nNov 10: Expected Value and Standard Error\n. Read ch. 17 of FPP. Complete assigned exercises.\n. Begin Writing Assignment 2 [Dropbox]\nNov 13: Normal Approximation for Probability Histograms\n. Read ch. 18 of FPP. Complete assigned exercises. . In class, fill in this table [Google Sheet].\n. In class, use these slides as needed [Google Slides].\nNov 15: Sample Surveys, Part 1\n. Read ch. 19 of FPP. Complete assigned exercises.\nNov 17: Sample Surveys, Part 2\n. Read ch. 20 of FPP. Complete assigned exercises.\nNov 20: FSAB Panel Day\n. Catch-up on any review exercises you haven’t done.\n. Writing Assignment 2 due (postponed to Tuesday, Nov. 28).\nNov 22: No Class (Reading Day)\nNov 24: No Class (Thanksgiving Holiday)\nNov 27: Catch-up Day\n. Make sure you’ve read through ch. 20 of FPP and completed assigned exercises.\nNov 29: The Accuracy of Percentages\n. Read ch. 21 of FPP. Complete assigned exercises.\nDec 1: The Accuracy of Averages\n. Read ch. 23 of FPP. Complete assigned exercises.\nDec 4: Hypothesis Tests . Read ch. 26 of FPP. Complete assigned exercises.\nDec 6: Final Exam Review\n. Example problem for hypothesis test and 95% CI for percent [Dropbox].\n. Final submission of Writing Assignment 2 due.\n. Study Guide [Google Doc]\nDec 8 or 11: Final Exam (bring pencil, pocket calculator, and small green Scantron) . For 901 (8:35-9:25am), 10am-12pm on Dec 8\n. For 902 (9:45-10:35am), 8-10am on Dec 11"
  },
  {
    "objectID": "teaching/pols-209/index.html#office-hours",
    "href": "teaching/pols-209/index.html#office-hours",
    "title": "POLS 209",
    "section": "",
    "text": "Allen 2051, MW, 11am-12pm. Please reserve your slot here."
  },
  {
    "objectID": "teaching/pols-209/index.html#schedule",
    "href": "teaching/pols-209/index.html#schedule",
    "title": "POLS 209",
    "section": "",
    "text": "Tasks (bullets) below the date should be completed before the next class (unless a due-date is listed).\nAugust 30: Introduction\n. Read the syllabus carefully.\n. Install R and RStudio (complete by Sep 6).\n. Order textbook (have by Sep 11).\n. Obtain a pocket calculator (have by Sep 11).\nSep 1: Questions\n. Review Notes on Questions [pdf]. Complete exercises.\nSep 4: Models\n. Review Notes on Models [pdf] (and slides from lecture [pdf]). Complete exercises.\n. Install R and RStudio.\nSep 6: Model-Building Exercise\nSep 8: Computing in R\nSep 11: Computing in R, part 2\n. Read Notes on Computing in R [pdf]. Complete exercises.\nSep 13: Loading Data in R . Review your notes from my Lecture on Data Frames [pdf].\n. Review Notes on Loading Data in R [pdf]. Complete exercises.\n. Complete Computing Assignment 1 [pdf].\nSep 15: Causal Inference and Histograms\n. Have textbook and calculator.\n. Read chs. 1-2 of FPP. Complete assigned exercises (remember that assigned exercises are [at the top]).\n. Review your notes from my Lecture on Causal Inference [pdf]. Complete exercises at the end.\n. Read ch. 3 of FPP. Complete assigned exercises.\nSep 18: Histograms in R\n. Review Notes on Histograms in R [pdf]. Complete exercises.\n. Begin to work on Computing Assignment 2 [pdf] (due Sep 22).\nSep 20: Average and SD\n. Read ch. 4 of FPP. Complete assigned exercises.\n. Submit Computing Assignment 2 [pdf].\nSep 22: Catch-Up Day\n. Begin working on Writing Assignment 1 [Dropbox]. Come prepared with questions.\nSep 25: Average and SD in R\n. Review your notes from my Lecture on Average and SD in R [pdf].\n. Review Notes on Average and SD in R [pdf]. Complete exercises.\n. Begin working on Computing Assignment 3 [pdf].\nSep 27: Normal Approximation\n. Read ch. 5 of FPP. Complete assigned exercises.\n. Finish the leadership extremity exercise we began in class [pdf].\n. Submit Computing Assignment 3 [pdf].\nSep 29: Measurement\n. Read ch. 6 of FPP. Complete assigned exercises.\n. Prepare for Exam 1. Focus on the review exercises from notes, slides, and textbook.\nOct 2: Review for Exam 1\n. Study Guide [Google Doc]\nOct 4: Exam 1 (bring pencil, pocket calculator, and small green Scantron)\n. Read “Politics and the English Language” [pdf]. Expect a reading quiz.\nOct 6: Discussion of “Politics and the English Language”Scatterplots and Correlation, Part 1\n. Read chs. 7-8 of FPP. Complete assigned exercises.\n. See this sheet for p. 137, #9(a) [Google Sheet].\nOct 9: Measurement, Part 2\n. Review your notes from my Lecture on Measurement [pdf]. Complete exercises at the end.\nOct 11: Scatterplots and Correlation in R\n. Review Notes on Scatterplots and Correlation in R [pdf]. Complete exercises.\n. Read ch. 9 of FPP. Complete assigned exercises.\n. Play this game [web] and track your performance.\n. Don’t forget to submit Writing Assignment 1.\nOct 13: Regression, Part 1\n. Read ch. 10 of FPP. Complete assigned exercises.\nOct 16: Regression, Part 2\n. Read ch. 11 of FPP. Complete assigned exercises.\n. Note that I accidentally assigned ch. 11 on the 13th as well. I meant to assign ch. 10. Make sure you’ve finished both ch. 10 and 11.\n. Begin Computing Assignment 4 [pdf].\nOct 18: Regression, Part 3\n. Read ch. 12 of FPP. Complete assigned exercises.\n. Submit Computing Assignment 4 [pdf].\n. Submit peer review for Writing Assignment 1. Details on eCampus.\nOct 20: Regression in R\n. Review Notes on Regression in R [pdf]. Complete exercises.\nOct 23: Multiple Regression, Part 1\nOct 25: Multiple Regression, Part 2\n. Review your notes on my lecture on econometric notation. Make sure you can explain the similarities and differences between FPP’s simple notation and the more complicated econometric notation. What are the two advantages of econometric notation?\n. Review your notes on my lecture [pdf] on regression for prediction.\n. Read these notes [pdf] for more detail on prediction and BIC.\n. Complete Computing Assignment 5 [pdf].\n. Read “5 Steps toward Constructing a Better Sentence” [web].\n. Read “5 Steps toward Writing an Effective Paragraph” [web].\n. Use this example response memo [pdf] when writing your own response memo.\nOct 27: “Breakfast with Ben”\nOct 30: Exam 2 Review\n. Study Guide [Google Doc]\nNov 1: Exam 2 (bring pencil, pocket calculator, and small green Scantron)\n. Final submission of Writing Assignment 1.\nNov 3: Probability, Part 1\n. We’ll look at the Federalist papers [web] in class.\n. Read ch. 13 of FPP. Complete assigned exercises.\nNov 6: Probability, Part 2\n. Read ch. 14 of FPP. Complete assigned exercises.\n. Submit peer review by noon on July 26.\nNov 8: Law of Averages\n. Read ch. 16 of FPP. Complete assigned exercises.\nNov 10: Expected Value and Standard Error\n. Read ch. 17 of FPP. Complete assigned exercises.\n. Begin Writing Assignment 2 [Dropbox]\nNov 13: Normal Approximation for Probability Histograms\n. Read ch. 18 of FPP. Complete assigned exercises. . In class, fill in this table [Google Sheet].\n. In class, use these slides as needed [Google Slides].\nNov 15: Sample Surveys, Part 1\n. Read ch. 19 of FPP. Complete assigned exercises.\nNov 17: Sample Surveys, Part 2\n. Read ch. 20 of FPP. Complete assigned exercises.\nNov 20: FSAB Panel Day\n. Catch-up on any review exercises you haven’t done.\n. Writing Assignment 2 due (postponed to Tuesday, Nov. 28).\nNov 22: No Class (Reading Day)\nNov 24: No Class (Thanksgiving Holiday)\nNov 27: Catch-up Day\n. Make sure you’ve read through ch. 20 of FPP and completed assigned exercises.\nNov 29: The Accuracy of Percentages\n. Read ch. 21 of FPP. Complete assigned exercises.\nDec 1: The Accuracy of Averages\n. Read ch. 23 of FPP. Complete assigned exercises.\nDec 4: Hypothesis Tests . Read ch. 26 of FPP. Complete assigned exercises.\nDec 6: Final Exam Review\n. Example problem for hypothesis test and 95% CI for percent [Dropbox].\n. Final submission of Writing Assignment 2 due.\n. Study Guide [Google Doc]\nDec 8 or 11: Final Exam (bring pencil, pocket calculator, and small green Scantron) . For 901 (8:35-9:25am), 10am-12pm on Dec 8\n. For 902 (9:45-10:35am), 8-10am on Dec 11"
  },
  {
    "objectID": "talks/index.html",
    "href": "talks/index.html",
    "title": "Talks",
    "section": "",
    "text": "Topic Sampling @ EPOVB 2024\n\n\nA talk at the 2024 meeting of the Election, Public Opinion, and Voting Behavior section of APSA.\n\n\n\nCarlisle Rainey\n\n\nMar 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTopic Sampling @ SPSA 2024\n\n\nA talk at the 2024 meeting of the Southern Political Science Association.\n\n\n\nCarlisle Rainey\n\n\nJan 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTopic Sampling @ EPOVB 2023\n\n\nA talk at the 2023 meeting of the Election, Public Opinion, and Voting Behavior section of APSA.\n\n\n\nCarlisle Rainey\n\n\nMar 4, 2023\n\n\n\n\n\n\n\n\nNo matching items\n\n\nFor older talks, see Speaker Deck."
  },
  {
    "objectID": "talks/2024-03-02-topic-sampling/index.html",
    "href": "talks/2024-03-02-topic-sampling/index.html",
    "title": "Topic Sampling @ EPOVB 2024",
    "section": "",
    "text": "The talk will be an amalgamation of several recent papers, but focus on two in particular:\n\nClifford, Scott and Carlisle Rainey. “The Limits (and Strengths) of Single-Topic Experiments.” [Preprint]\nThorson, Emily and Carlisle Rainey. “The Policy Longevity Bonus: Pre-Analysis Plan.” [Pre-Analysis Plan]\n\nThe two other relevant papers are below.\n\nClifford, Scott, Thomas Leeper, and Carlisle Rainey. “Generalizing Survey Experiments Using Topic Sampling: An Application to Party Cues.” Forthcoming in Political Behavior. [Journal] [Ungated]\nClifford, Scott and Carlisle Rainey. “Estimators for Topic-Sampling Designs.” Conditionally accepted at Political Analysis. [Preprint]"
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Papers",
    "section": "",
    "text": "If want to stay up-to-date on my work, follow me on Google Scholar."
  },
  {
    "objectID": "research/index.html#working-papers",
    "href": "research/index.html#working-papers",
    "title": "Papers",
    "section": "Working Papers",
    "text": "Working Papers\n\n\n\n\n    \n      The Data Availability Policies of Political Science Journals \n      \n      \n      \n        \n          Feb 15, 2024. Carlisle Rainey, Harley Roe. Working paper.\n        \n      \n      \n         \n           Preprint \n         \n         \n           Dataverse \n        \n      \n      \n      \n    \n    \n      Data and Code Availability in Political Science Publications from 1995 to 2022 \n      \n      \n      \n        \n          Jan 21, 2024. Carlisle Rainey, Harley Roe, Qing Wang, Hao Zhou. Working paper.\n        \n      \n      \n         \n           Preprint \n         \n         \n           Dataverse \n        \n      \n      \n      \n    \n    \n      Overt Consequences of Covert Actions: Success, Failure, and Voters’ Preferences for Legislative Oversight \n      \n      \n      \n        \n          Jan 8, 2024. Caroline Robbins, Alessandro Brunelli, José Castro, Ainsley Coty, Andrew Louis, Bryanna Major, María Alemán Martínez, Yadianis Lara Ojeda, Larissa Pontes, Elke Schumacher, Omer Turkomer, Luzmi Valenzuela, Valeria Veras, Carlisle Rainey. Working paper.\n        \n      \n      \n         \n           Preprint \n         \n         \n           OSF\n        \n         \n           Preregistration \n         \n      \n      \n      \n    \n    \n      The Dissent Score: Using Events Data to Measure Dissent \n      \n      \n      \n        \n          Dec 23, 2023. Carlisle Rainey, Harley Roe, Qing Wang, Nick Dietrich. Working paper.\n        \n      \n      \n         \n           Project Website\n        \n         \n           Preprint \n         \n      \n        \n          The dissent score data set is available on Dataverse.    \n        \n      \n      \n    \n    \n      Estimators for Topic-Sampling Designs \n      \n      \n      \n        \n          Dec 18, 2023. Scott Clifford, Carlisle Rainey. Conditionally accepted at Political Analysis.\n        \n      \n      \n         \n           Preprint \n         \n      \n      \n      \n    \n    \n      Generational Differences in Abortion Attitudes in the United States: Will Dobbs v. Jackson Become Starkly Counter-Majoritarian? \n      \n      \n      \n        \n          Oct 13, 2023. Carlisle Rainey, Robert Jackson. Working paper.\n        \n      \n      \n         \n           Preprint \n         \n         \n           Dataverse \n        \n      \n      \n      \n    \n    \n      The Limits of Single-Topic Designs \n      \n      \n      \n        \n          Oct 10, 2023. Scott Clifford, Carlisle Rainey. Working paper.\n        \n      \n      \n         \n           Preprint \n         \n      \n      \n      \n    \n    \n      Generation Effects on Americans' Symbolic Ideology and Attitudes Toward the Economic Role of Government \n      \n      \n      \n        \n          Aug 2, 2023. Robert Jackson, Carlisle Rainey. Working paper.\n        \n      \n      \n         \n           Preprint \n         \n      \n      \n      \n    \n\n\nNo matching items"
  },
  {
    "objectID": "research/index.html#forthcoming",
    "href": "research/index.html#forthcoming",
    "title": "Papers",
    "section": "Forthcoming",
    "text": "Forthcoming\n\n\n\n\n    \n      Hypothesis Tests Under Separation \n      \n      \n      \n        \n          Sep 7, 2023. Carlisle Rainey. Forthcoming at Political Analysis.\n        \n      \n      \n         \n           Open Access\n        \n         \n           Preprint \n         \n         \n           Dataverse \n        \n         \n           OSF\n        \n      \n        \n          See also Rainey (2016)    \n        \n      \n      \n    \n    \n      Generalizing Survey Experiments Using Topic Sampling: An Application to Party Cues \n      \n      \n      \n        \n          Mar 26, 2023. Scott Clifford, Thomas Leeper, Carlisle Rainey. Forthcoming at Political Behavior.\n        \n      \n      \n         \n           PDF \n         \n         \n           Publisher's Version \n        \n         \n           Dataverse \n        \n      \n      \n      \n    \n    \n      A Careful Consideration of CLARIFY: Simulation-Induced Bias in Point Estimates of Quantities of Interest \n      \n      \n      \n        \n          Mar 26, 2023. Carlisle Rainey. Forthcoming at Political Science Research and Methods.\n        \n      \n      \n         \n           Errata\n         \n         \n           PDF \n         \n         \n           Open Access\n        \n         \n           Dataverse \n        \n         \n           GitHub \n        \n      \n      \n      \n    \n\n\nNo matching items"
  },
  {
    "objectID": "research/index.html#published-papers",
    "href": "research/index.html#published-papers",
    "title": "Papers",
    "section": "Published Papers",
    "text": "Published Papers\n\n\n\n\n    \n      Estimating Logit Models with Small Samples \n      \n      \n      \n      \n        \n          2021. Carlisle Rainey, Kelly McCaskey. Political Science Research and Methods.\n        \n      \n         \n           PDF \n         \n         \n           Open Access\n        \n         \n           Dataverse \n        \n      \n      \n      \n    \n    \n      When BLUE Is Not Best: Non-Normal Errors and the Linear Model \n      \n      \n      \n      \n        \n          2020. Daniel Baissa, Carlisle Rainey. Political Science Research and Methods.\n        \n      \n         \n           PDF \n         \n         \n           Publisher's Version \n        \n         \n           Dataverse \n        \n      \n      \n      \n    \n    \n      Unreliable Inferences about Unobservable Processes: A Critique of Partial Observability Models \n      \n      \n      \n      \n        \n          2018. Carlisle Rainey, Robert Jackson. Political Science Research and Methods.\n        \n      \n         \n           PDF \n         \n         \n           Publisher's Version \n        \n         \n           Dataverse \n        \n      \n      \n      \n    \n    \n      Transformation-Induced Bias: Unbiased Coefficients Do Not Imply Unbiased Quantities of Interest \n      \n      \n      \n      \n        \n          2017. Carlisle Rainey. Political Analysis.\n        \n      \n         \n           PDF \n         \n         \n           Publisher's Version \n        \n         \n           Dataverse \n        \n      \n      \n      \n    \n    \n      Compression and Conditional Effects: A Product Term Is Essential When Using Logistic Regression to Test for Interaction \n      \n      \n      \n      \n        \n          2016. Carlisle Rainey. Political Science Research and Methods.\n        \n      \n         \n           PDF \n         \n         \n           Publisher's Version \n        \n         \n           Appendix \n        \n         \n           Dataverse \n        \n      \n      \n      \n    \n    \n      Dealing with Separation in Logistic Regression Models \n      \n      \n      \n      \n        \n          2016. Carlisle Rainey. Political Analysis.\n        \n      \n         \n           PDF \n         \n         \n           Publisher's Version \n        \n         \n           Appendix \n        \n         \n           Dataverse \n        \n      \n      \n      \n    \n    \n      Does District Magnitude Matter: The Case of Taiwan \n      \n      \n      \n      \n        \n          2016. Carlisle Rainey. Electoral Studies.\n        \n      \n         \n           PDF \n         \n         \n           Publisher's Version \n        \n         \n           Dataverse \n        \n      \n      \n      \n    \n    \n      Strategic Mobilization: Why Proportional Representation Decreases Voter Mobilization \n      \n      \n      \n      \n        \n          2015. Carlisle Rainey. Electoral Studies.\n        \n      \n         \n           PDF \n         \n         \n           Publisher's Version \n        \n         \n           Appendix \n        \n         \n           Dataverse \n        \n      \n      \n      \n    \n    \n      Substantive Importance and the Veil of Statistical Significance \n      \n      \n      \n      \n        \n          2015. Kelly McCaskey, Carlisle Rainey. Statistics, Politics, and Policy.\n        \n      \n         \n           PDF \n         \n         \n           Publisher's Version \n        \n         \n           GitHub \n        \n      \n      \n      \n    \n    \n      Moral Concerns and Policy Attitudes: Investigating the Influence of Elite Rhetoric \n      \n      \n      \n      \n        \n          2015. Scott Clifford, Jennifer Jerit, Carlisle Rainey, Matt Motyl. Political Communication.\n        \n      \n         \n           PDF \n         \n         \n           Publisher's Version \n        \n      \n      \n      \n    \n    \n      The Politics of Need: Examining Governors' Decisions to Oppose the 'Obamacare' Medicaid Expansion \n      \n      \n      \n      \n        \n          2014. Charles Barrillleaux, Carlisle Rainey. State Politics and Policy Quarterly.\n        \n      \n         \n           PDF \n         \n         \n           Publisher's Version \n        \n         \n           Appendix \n        \n         \n           Dataverse \n        \n      \n      \n      \n    \n    \n      The Question(s) of Political Knowledge \n      \n      \n      \n      \n        \n          2014. Jason Barabas, Jennifer Jerit, William Pollock, Carlisle Rainey. American Political Science Review.\n        \n      \n         \n           PDF \n         \n         \n           Publisher's Version \n        \n         \n           Dataverse \n        \n      \n      \n      \n    \n    \n      Arguing for a Negligible Effect \n      \n      \n      \n      \n        \n          2014. Carlisle Rainey. American Journal of Political Science.\n        \n      \n         \n           PDF \n         \n         \n           Publisher's Version \n        \n         \n           Appendix \n        \n         \n           Dataverse \n        \n      \n      \n      \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/2023-08-30-firth-references/index.html",
    "href": "blog/2023-08-30-firth-references/index.html",
    "title": "Firth’s Logit: Some References",
    "section": "",
    "text": "In Rainey and McCaskey (2021), Kelly McCaskey and I offer a accessible and practical (re)introduction to Firth’s penalized maximum likelihood estimator that (1) corrects the small sample bias and (2) reduces the excessive variance of the usual maximum likelihood estimator.\nBelow, I bookmark other references that might be helpful.\n\n\n\n\n\n\nI’m sure there are embarrassing omissions. If you see an omission, please let me know (self-promotion is encouraged, especially not-yet-published papers).\n\n\n\nThis Stack Exchange answer gives a brief, but careful explanation of Firth’s logit. If you’re looking for a quick explanation, start here.\n\nThe Two Main Papers\n\nFirth (1993) originally introduced the idea. Kelly and I draw mostly on this paper—it’s a wonderful paper.\nKosmidis and Firth (2021) follow-up with additional theoretical results that are relevant for the estimator as used in practice since 1993. This happened to come out while our paper was working its way through the publication process. Most importantly, they discuss the shrinkage property of the estimator, which is what Kelly and I highlight as under-appreciated (and really important!).\n\nFrom my perpective, these are the two main papers to refer to if you’re concerned about small sample bias in logistic regression models.\n\n\nExtensions\nBeyond these two main papers, there have been a few extensions. Zietkiewicz and Kosmidis (2023) talk about Firth’s logit in very large data sets. Cook, Hays, and Franzese (2018) make a good argument for using Firth’s estimator in panel data sets with binary outcomes and fixed effects. Sterzinger and Kosmidis (2023) apply these ideas to mixed models (or random effects models). Šinkovec et al. (2021) compare Firth’s approach to ridge regression, and suggest that Firth’s is superior in small or sparse data sets. Puhr et al. (2017) study Firth’s logit in the context of rare events and propose FLIC and FLAC as alternatives.\n\n\nApplications\n\nRöver et al. (2022) offer an application of Firth’s logit to clinical trials.\nTurner and Firth (2012) offer an application to Bradley-Terry models with the {BradleyTerry2} R package.\n\n\n\nSeparation and Finiteness\nI learned about Firth’s estimator from Zorn (2005), who follows Heinze and Schemper (2002) in suggesting it as a solution to separation. According to David Firth in this blog post, this is the application that stimulated interest in the approach after it went relatively unnoticed for a few years. (Great post, I highly recommend reading it!) This application piqued my interest in Firth’s estimator. Briefly, I think Firth’s default penalty might not be substantively reasonable in a given application (Rainey 2016) (see also Beiser-McGrath (2020)) and the usual likelihood ratio and score tests work well without the penalty (Rainey 2023).\n\n\n\n\n\n\nFor more on Firth’s logit, see Ioannis Kosmidis’ research page and Georg Heinz Google Scholar page.\n\n\n\n\n\n\n\n\nReferences\n\nBeiser-McGrath, Liam F. 2020. “Separation and Rare Events.” Political Science Research and Methods 10 (2): 428–37. https://doi.org/10.1017/psrm.2020.46.\n\n\nCook, Scott J., Jude C. Hays, and Robert J. Franzese. 2018. “Fixed Effects in Rare Events Data: A Penalized Maximum Likelihood Solution.” Political Science Research and Methods 8 (1): 92–105. https://doi.org/10.1017/psrm.2018.40.\n\n\nFirth, David. 1993. “Bias Reduction of Maximum Likelihood Estimates.” Biometrika 80 (1): 27–38. https://doi.org/10.1093/biomet/80.1.27.\n\n\nHeinze, Georg, and Michael Schemper. 2002. “A Solution to the Problem of Separation in Logistic Regression.” Statistics in Medicine 21 (16): 2409–19. https://doi.org/10.1002/sim.1047.\n\n\nKosmidis, Ioannis, and David Firth. 2021. “Jeffreys-Prior Penalty, Finiteness and Shrinkage in Binomial-Response Generalized Linear Models.” Biometrika 108 (1): 71–82. https://doi.org/10.1093/biomet/asaa052.\n\n\nPuhr, Rainer, Georg Heinze, Mariana Nold, Lara Lusa, and Angelika Geroldinger. 2017. “Firth’s Logistic Regression with Rare Events: Accurate Effect Estimates and Predictions?” Statistics in Medicine. https://doi.org/10.1002/sim.7273.\n\n\nRainey, Carlisle. 2016. “Dealing with Separation in Logistic Regression Models.” Political Analysis 24 (3): 339–55. https://doi.org/10.1093/pan/mpw014.\n\n\n———. 2023. “Hypothesis Tests Under Separation.” http://dx.doi.org/10.31235/osf.io/bmvnu.\n\n\nRainey, Carlisle, and Kelly McCaskey. 2021. “Estimating Logit Models with Small Samples.” Political Science Research and Methods 9 (3): 549–64. https://doi.org/10.1017/psrm.2021.9.\n\n\nRöver, Christian, Moreno Ursino, Tim Friede, and Sarah Zohar. 2022. “A Straightforward Meta-Analysis Approach for Oncology Phase I Dose-Finding Studies.” Statistics in Medicine 41 (20): 3915–40. https://doi.org/10.1002/sim.9484.\n\n\nŠinkovec, Hana, Georg Heinze, Rok Blagus, and Angelika Geroldinger. 2021. “To Tune or Not to Tune, a Case Study of Ridge Logistic Regression in Small or Sparse Datasets.” BMC Medical Research Methodology 21 (1). https://doi.org/10.1186/s12874-021-01374-y.\n\n\nSterzinger, Philipp, and Ioannis Kosmidis. 2023. “Maximum Softly-Penalized Likelihood for Mixed Effects Logistic Regression.” Statistics and Computing 33 (2). https://doi.org/10.1007/s11222-023-10217-3.\n\n\nTurner, Heather, and David Firth. 2012. “Bradley-Terry Models inR: TheBradleyTerry2Package.” Journal of Statistical Software 48 (9). https://doi.org/10.18637/jss.v048.i09.\n\n\nZietkiewicz, Patrick, and Ioannis Kosmidis. 2023. “Bounded-Memory Adjusted Scores Estimation in Generalized Linear Models with Large Data Sets.” https://doi.org/10.48550/ARXIV.2307.07342.\n\n\nZorn, Christopher. 2005. “A Solution to Separation in Binary Response Models.” Political Analysis 13 (2): 157–70. https://doi.org/10.1093/pan/mpi009."
  },
  {
    "objectID": "blog/2023-08-15-daily-writing/index.html",
    "href": "blog/2023-08-15-daily-writing/index.html",
    "title": "Daily Writing",
    "section": "",
    "text": "I try to write everyday.11 See lots of caveats below!\nBy “writing,” I mean “pushing the paper closest to publication just a little bit closer.” I want to think about the next step on the journey to the published paper and do it. According to this loose definition of writing, it might involve data collection, data analysis, creating slides, or even writing and polishing text. It might involve organization, planning, or learning new skills. It excludes any tasks that aren’t necessary to complete the project.\nBy “everyday,” I mean at least every weekday, probably at the same time every day and probably first thing in the morning. For better or worse, academics are evaluated by their research productivity."
  },
  {
    "objectID": "blog/2023-08-15-daily-writing/index.html#urgency-and-importance",
    "href": "blog/2023-08-15-daily-writing/index.html#urgency-and-importance",
    "title": "Daily Writing",
    "section": "Urgency and Importance",
    "text": "Urgency and Importance\nPresident Eisenhower famously characterized his duties: “I have two kinds of problems, the urgent and the important. The urgent are not important, and the important are never urgent.”\nFollowing Eisenhower’s Box, we might assign degrees of urgency and importance to tasks in academic tasks. In graduate school, I had teaching responsibilities, RA duties, readings for seminars, homework for methods classes, preliminary exams, and administrative tasks. All of these tasks are important. They must be completed. They must be completed well. Yet I was evaluated largely on my papers. As a faculty member, little has changed.\nWriting is important, but writing never quite becomes urgent. It’s easy to put off writing to prepare a lecture (or write a blog post)."
  },
  {
    "objectID": "blog/2023-08-15-daily-writing/index.html#the-evidence",
    "href": "blog/2023-08-15-daily-writing/index.html#the-evidence",
    "title": "Daily Writing",
    "section": "The Evidence",
    "text": "The Evidence\nRobert Boice studied academic productivity carefully. A couple of his studies provide some evidence for my strategy to write every day.\nFirst, he assessed how early-career academics spend their time. The figure below shows the results. Notice that these faculty spend more time in committee meetings (2 hrs.) than writing (1.5 hours).\n\n\n\n\n\nSecond, Boice conducted an experiment to assess the effect of writing strategies.\nBoice randomly divided 27 academics into three groups:22 This is a small sample, but it supports my claim so it’s okay.\n\nThe control group agreed to defer all but the most urgent writing for ten weeks.\nThe spontaneous group agreed write when they felt like it.\nThe contingency group agreed to donate to an anti-charity if they failed to write every day.\n\nThe figure below shows that regular writing routine increase production of both pages and ideas. Notice that the spontaneous writers barely produced more ideas and pages than the group trying to avoid writing.\n\n\n\n\n\nI find these results compelling, but note that Helen Sword urges some caution."
  },
  {
    "objectID": "blog/2023-08-15-daily-writing/index.html#how-i-do-it",
    "href": "blog/2023-08-15-daily-writing/index.html#how-i-do-it",
    "title": "Daily Writing",
    "section": "How I Do It",
    "text": "How I Do It\nEveryone is different, and my own approach has evolved over time. Here are the key ingredients (for me):\n\nWrite for two hours at a regular time. Consistency is key.3\nAvoid writing outside this window. Set your window so that your window is “enough.”\nTake breaks. I take long breaks from writing. But these are intentional and planned.4\nFamily permitting, I think it’s helpful to spend a little while pushing the projects forward on the weekends, just to keep the momentum up.5\n\n3 Two hours works really well for me. My productivity degrades quickly after two hours, so it’s best to move on to less taxing tasks. But it takes me a while to get warmed up, so I need to keep moving while I’ve got momentum.4 An unfortunate outcome is not writing and being stressed about not writing.5 Just 15 minutes is great. This slot is perfect for proof-reading.I admit that I deviate from the strategy above (and not always intentionally). But I’ve been at this long enough to know that a regular routine works really well for me."
  },
  {
    "objectID": "blog/2023-08-15-daily-writing/index.html#what-if-youre-not-ready-to-write-yet",
    "href": "blog/2023-08-15-daily-writing/index.html#what-if-youre-not-ready-to-write-yet",
    "title": "Daily Writing",
    "section": "What if you’re not ready to write yet?",
    "text": "What if you’re not ready to write yet?\nIt’s my view that PhD students should write every day, from the first day of their first semester (remember that I have a broad definition of “write”). Most students need some time before they’re ready to jump into the technical details a solo project, but there are always things to do.\nIf you can’t identify a specific task to work on, here are some resources to help you brainstorm.\n\nPlan and organize. Start by reading How to Write a Lot. Perhaps read Getting Things Done. Perhaps read Air & Light & Time & Space or Writing for Social Scientists.\nRead “Publication, Publication” and the updates.\nBefore you can jump into a project, you need to know the literature. Spend some writing time exploring literatures that you might want to contribute to. What interests you most? The Annual Review of Political Science is a valuable resource.\nOnce you have a specific topic of interest, you need to learn that literature. You can spend dozens of “writing” sessions reading and taking notes. I strongly encourage you to read and take notes systematically, as Raul Pacheco-Vega suggests using a spreadsheet, Elaine Campbell suggests a similar method, and Katherine Firth suggests a using Cornell notes.\nTanya Golash-Boza lists ten ways to write everyday if you’ve got a paper in-progress."
  },
  {
    "objectID": "blog/2023-06-12-power-3-rule-of-364/index.html",
    "href": "blog/2023-06-12-power-3-rule-of-364/index.html",
    "title": "Power, Part III: The Rule of 3.64 for Statistical Power",
    "section": "",
    "text": "I’ve wrapped up the argument that you should pursue statistical power in your experiments. In sum, you should do it for you (not a future Reviewer 2) and you shouldn’t see confidence intervals nestle consistently against zero.\n\n“Power Is For You, Not For Reviewer 2”\n“What Do Confidence Intervals From Well-Powered Studies Look Like?”\n\nIn this post, I’d like to develop the intuition for power calculations, two helpful guidelines, and one implication.\nMain takeaway: You need the ratio of the true effect and the standard error to be more than 3.64."
  },
  {
    "objectID": "blog/2023-06-12-power-3-rule-of-364/index.html#background",
    "href": "blog/2023-06-12-power-3-rule-of-364/index.html#background",
    "title": "Power, Part III: The Rule of 3.64 for Statistical Power",
    "section": "",
    "text": "I’ve wrapped up the argument that you should pursue statistical power in your experiments. In sum, you should do it for you (not a future Reviewer 2) and you shouldn’t see confidence intervals nestle consistently against zero.\n\n“Power Is For You, Not For Reviewer 2”\n“What Do Confidence Intervals From Well-Powered Studies Look Like?”\n\nIn this post, I’d like to develop the intuition for power calculations, two helpful guidelines, and one implication.\nMain takeaway: You need the ratio of the true effect and the standard error to be more than 3.64."
  },
  {
    "objectID": "blog/2023-06-12-power-3-rule-of-364/index.html#starting-point-the-sampling-distribution",
    "href": "blog/2023-06-12-power-3-rule-of-364/index.html#starting-point-the-sampling-distribution",
    "title": "Power, Part III: The Rule of 3.64 for Statistical Power",
    "section": "Starting Point: The Sampling Distribution",
    "text": "Starting Point: The Sampling Distribution\nWhen you run one experiment, you realize one of many possible patterns of randomization. This particular realization produces a single estimate of the treatment effect from a distribution of possible estimates. The distribution of possibilities is called a “sampling distribution.”\nThus, we can think of the estimate as a random variable and its distribution as the sampling distribution. The sampling distribution is key to everything I do in this post, so let’s spend some time with it.\nLet’s imagine that we did the exact same study 50 times. Let’s say that we computed a difference-in-means in dollars ($) donated.1 I refer to this difference-in-means as the estimated treatment effect. It is the estimate of the average treatment effect (in $). This estimate will vary across the many possible patterns of randomization because each pattern puts different respondents in the treatment and control group.1 I just want an easy-to-use unit here, and dollars meets that criteria. Other units work fine, too.\nWe can visualize this with ggnaminate. We can imagine each iteration of the study as producing a particular estimate. We continue to repeat the study and collect the estimates. Eventually, we can produce a histogram from this collection of estimates. This histogram represents the sampling distribution and is fundamental to the calculations that follow. The figure belows shows how we might collect the points into a histogram.\n\n\nCode\nlibrary(tidyverse)\nlibrary(gganimate)\nlibrary(magick)\n\n# gif pars\nduration &lt;- 24 # must be even\nfps &lt;- 25\nnframes &lt;- duration*fps\nscale &lt;- 2.5\nwidth &lt;- 8\nheight &lt;- 6\nres &lt;- 125\n\n# study parameters\ntrue_effect &lt;- 1\nse &lt;- 0.4\n\n# number of times to repeat the study\nn_studies &lt;- 50 # nframes\n\n# create a data frame of confidence intervals\nests &lt;- tibble(study_id = 1:n_studies, \n               est = c(rnorm(n_studies, true_effect, se))) %&gt;%\n  mutate(reject_null = ifelse(est - 1.64*se &gt; 0, \"Yes\", \"No\"))\n\n# add two things to the data frame of confidence intervals\n# 1. an initial row with study_id = 1 and est = NA so that \n#    the plot starts empty (gganimate would start with the \n#    first observation in place otherwise).\n# 2. a group variable that defines the row. This is the same\n#    as the study_id, except the dummy row from (1) and the \n#    actual first row have different groups.\nanimate_data &lt;- bind_rows(\n  tibble(study_id = 1, est = NA),  # study_id = 1, est = NA\n  ests                             # actual cis\n) %&gt;%\n  mutate(group = 1:n()) \n\nsplit_animate_data &lt;- animate_data %&gt;%        # group (row index)\n  split(.$group) %&gt;%\n  accumulate(~ bind_rows(.x, .y)) %&gt;% \n  bind_rows(.id = \"frame\") %&gt;% \n  mutate(frame = as.integer(frame))\n\n\nse_lines &lt;- tribble(\n  ~se_, ~label, ~chance, ~ch_loc_,  # trailing _ means not rescaeld to study se\n  0, \"True Effect\", NA, NA,\n  1, \"+1 SE\", scales::percent(pnorm(1) - pnorm(0), accuracy = 1), 0.5,\n  2, \"+2 SE\", scales::percent(pnorm(2) - pnorm(1), accuracy = 1), 1.5,\n  3, \"+3 SE\", scales::percent(pnorm(3) - pnorm(2), accuracy = 1), 2.5,\n  -1, \"-1 SE\", scales::percent(pnorm(0) - pnorm(-1), accuracy = 1), -0.5,\n  -2, \"-2 SE\", scales::percent(pnorm(-1) - pnorm(-2), accuracy = 1), -1.5,\n  -3, \"-3 SE\", scales::percent(pnorm(-2) - pnorm(-3), accuracy = 1), -2.5,\n) %&gt;%\n  mutate(ch_loc = ch_loc_*se + true_effect,\n         se = se_*se + true_effect)\n\n# start with a ggplot\ngg1 &lt;- ggplot(animate_data, aes(x = est, \n                               y = study_id,\n                               group = group)) + \n  geom_vline(data = se_lines, aes(xintercept = se, \n                                  color = -dnorm(se_)), linetype = \"dashed\") + \n  geom_label(data = se_lines, aes(x = se, y = n_studies + 2, label = label, group = NULL, color = -dnorm(se_))) + \n  geom_text(data = se_lines, aes(x = ch_loc, y = 4, label = chance, group = NULL)) + \n  geom_point(aes(color = -dnorm((est- true_effect)/se)),\n             size = 3) + \n  geom_rug(sides = \"b\", \n           aes(x = est, \n               color = -dnorm((est- true_effect)/se)), \n           alpha = 0.5, \n           length = unit(0.025, \"npc\")) + \n  theme_bw() + \n  theme(panel.grid.minor.y = element_blank()) + \n  labs(x = \"Estimate of Effect\",\n       y = \"Study Number\") +\n  theme(legend.position = \"none\")\n\n# add dyamics to the plot\ngg1_anim &lt;- gg1 +  \n  transition_states(states = group) + \n  # how points enter\n  enter_drift(y_mod = 10) +\n  enter_grow() +\n  enter_fade() + \n  # how points exit/remain\n  exit_fade(alpha = 0.5) +\n  exit_shrink(size = 1) + \n  shadow_mark(alpha = 0.5) \n\ngg1_gif&lt;- animate(gg1_anim, nframes = nframes, duration = duration, width = width, height = height, units = \"in\", res = res)\nanim_save(\"gg1.gif\")\ngg1_mgif &lt;- image_read(\"gg1.gif\")\n\n\n## plot 2: histogram\n# start with a ggplot\ngg2 &lt;- ggplot(split_animate_data, aes(x = est, group = frame)) + \n  geom_histogram(binwidth = se, boundary = true_effect, fill = \"grey\") + \n  geom_vline(data = se_lines, aes(xintercept = se, \n                                  color = -dnorm(se_)), linetype = \"dashed\") + \n  geom_label(data = se_lines, aes(x = se, y = Inf, label = label, group = NULL, color = -dnorm(se_)), vjust = 1.5) + \n  geom_label(data = se_lines, aes(x = ch_loc, y = 0, label = chance, group = NULL), vjust = -1) + \n  #geom_density(linewidth = 2) + \n  geom_rug(sides = \"b\", \n           aes(x = est, \n               color = -dnorm((est- true_effect)/se)), \n           alpha = 0.5, \n           length = unit(0.025, \"npc\")) + \n  theme_bw() + \n  labs(x = \"Estimate of Effect\",\n       y = \"Count\") + \n  theme(legend.position = \"none\")\n\n# add dyamics to the plot\ngg2_anim &lt;- gg2 +  \n  transition_states(states = frame)\n\ngg2_gif&lt;- animate(gg2_anim, nframes = nframes, duration = duration, width = width, height = height, units = \"in\", res = res)\nanim_save(\"gg2.gif\")\ngg2_mgif &lt;- image_read(\"gg2.gif\")\n\n\n\nnew_gif &lt;- image_append(c(gg1_mgif[1], gg2_mgif[1]), stack = FALSE)\nfor(i in 2:nframes){\n  combined_gif &lt;- image_append(c(gg1_mgif[i], gg2_mgif[i]), stack = FALSE)\n  new_gif &lt;- c(new_gif, combined_gif)\n}\nnew_gif\n\n\n\n\n\nBefore the study, we can predict two features of this sampling distribution.\n\nFirst, it will usually have a bell-shaped, normal distribution.\nSecond, we can predict the standard deviation of this distribution with good accuracy before conducting a single study and excellent accuracy after just one study. We call the standard deviation of the sampling distribution the standard error or SE.\n\nFor our purposes, then, we can describe the sampling distribution as normally distributed with an assumed mean and SD. The mean is the assumed true effect and the SD is the well-predicted SE.\nThis is the key claim: In order to build power into your experiment, you must build certain properties into the sampling distribution.\nThe design of your experiment will not affect the normality of the sampling distribution, but it will change the true effect and the SE. Changing the true effect and the SE will change the power."
  },
  {
    "objectID": "blog/2023-06-12-power-3-rule-of-364/index.html#thinking-about-true-effect-and-standard-error-as-targets",
    "href": "blog/2023-06-12-power-3-rule-of-364/index.html#thinking-about-true-effect-and-standard-error-as-targets",
    "title": "Power, Part III: The Rule of 3.64 for Statistical Power",
    "section": "Thinking about True Effect and Standard Error as Targets",
    "text": "Thinking about True Effect and Standard Error as Targets\nI’m leaving aside how to predict the standard error of the experiment or choose the true effect. This post is about the target standard error and true effect.\nPredicting the standard error is a mechanical process mixed with a little guesswork.2 Choose a true effect is a mostly substantive decision.32 I’ll suggest two methods I like. First, use \\(\\text{predicted SE} = \\frac{\\text{SD of same outcome in different data set}}{0.5 \\sqrt{\\text{sample size}}}\\). I like to confirm this estimate with a small pilot. I sample observations from this pilot data set, run the full analysis, and confirm that my prediction is close, and make any needed adjustments.3 Cyrus Samii likes to use the MME, I like a conservatively choosen guess of what the effect actually is.\nBut instead of talking about a target power, I like to talk about a target standard error (given a true effect) or a target true effect (given a standard error)—power is just not a very intuitive quantity.\nTo understand how the true effect and the SE relate to power, we need to introduce the confidence interval."
  },
  {
    "objectID": "blog/2023-06-12-power-3-rule-of-364/index.html#testing-with-confidence-intervals",
    "href": "blog/2023-06-12-power-3-rule-of-364/index.html#testing-with-confidence-intervals",
    "title": "Power, Part III: The Rule of 3.64 for Statistical Power",
    "section": "Testing with Confidence Intervals",
    "text": "Testing with Confidence Intervals\nI like to use 90% confidence intervals to test hypotheses (see Rainey 2014 and Rainey 2015). In short, 90% confidence intervals correspond to one-tailed tests with size 0.05 and equivalence tests with size 0.05.] The formula for a 90% CI is \\(\\text{estimate} \\pm 1.64\\text{SE}\\). That is, we put “arms” around our estimate—one to the left and another to the right. Each arm is 1.64 standard errors long.\nI’ll assume we have a one-sided research hypothesis that suggests a positive effect. If the lower bound (\\(\\text{estimate} - 1.64\\text{SE}\\)) is less than zero, then we fail to reject the null hypothesis. If this lower bound is greater than zero, then we reject the null hypothesis.44 This is equivalent to a z-test in a standard hypothesis testing framework using a p-value of less than 0.05 as the threshold for rejecting the null hypothesis.\nThis focus on testing rather than estimation changes the nature of the sampling distribution. Rather than an estimate along a continuous range, we get a binary outcome: either (1) reject the null hypothesis or (2) fail to reject the null hypothesis.\nBut the sampling distribution of estimates and the associated outcomes of tests are closely related. In particular, the logic of the test implies that *if the estimate falls less than 1.64 SEs above zero, we cannot reject the null.\nWe can reconstruct the figure above using this logic. Rather than plot the points continuously along the x-axis, we can color the points (and now error bars) according to whether the lower bound falls above zero or not. And we can use a bar plot showing the number of rejections and non-rejections.\n\n\nCode\nlibrary(tidyverse)\nlibrary(gganimate)\nlibrary(magick)\n\n# gif pars\nduration &lt;- 24 # must be even\nfps &lt;- 25\nnframes &lt;- duration*fps\nscale &lt;- 2.5\nwidth &lt;- 8\nheight &lt;- 6\nres &lt;- 125\n\n# study parameters\ntrue_effect &lt;- 1\nse &lt;- 0.4\n\n# number of times to repeat the study\nn_studies &lt;- 50 # nframes\n\n# create a data frame of confidence intervals\nests &lt;- tibble(study_id = 1:n_studies, \n               est = c(rnorm(n_studies, true_effect, se))) %&gt;%\n  mutate(reject_null = ifelse(est - 1.64*se &gt; 0, \"Yes\", \"No\"),\n         lwr = est - 1.64*se,\n         upr = est + 1.64*se)\n\n# add two things to the data frame of confidence intervals\n# 1. an initial row with study_id = 1 and est = NA so that \n#    the plot starts empty (gganimate would start with the \n#    first observation in place otherwise).\n# 2. a group variable that defines the row. This is the same\n#    as the study_id, except the dummy row from (1) and the \n#    actual first row have different groups.\nanimate_data &lt;- bind_rows(\n  tibble(study_id = 1, est = NA),  # study_id = 1, est = NA\n  ests                             # actual cis\n) %&gt;%\n  mutate(group = 1:n()) \n\nsplit_animate_data &lt;- animate_data %&gt;%        # group (row index)\n  split(.$group) %&gt;%\n  accumulate(~ bind_rows(.x, .y)) %&gt;% \n  bind_rows(.id = \"frame\") %&gt;% \n  mutate(frame = as.integer(frame))\n\n\nse_lines &lt;- tribble(\n  ~se_, ~label, ~chance, ~ch_loc_,  # trailing _ means not rescaeld to study se\n  0, \"True Effect\", NA, NA,\n  1, \"+1 SE\", scales::percent(pnorm(1) - pnorm(0), accuracy = 1), 0.5,\n  2, \"+2 SE\", scales::percent(pnorm(2) - pnorm(1), accuracy = 1), 1.5,\n  3, \"+3 SE\", scales::percent(pnorm(3) - pnorm(2), accuracy = 1), 2.5,\n  -1, \"-1 SE\", scales::percent(pnorm(0) - pnorm(-1), accuracy = 1), -0.5,\n  -2, \"-2 SE\", scales::percent(pnorm(-1) - pnorm(-2), accuracy = 1), -1.5,\n  -3, \"-3 SE\", scales::percent(pnorm(-2) - pnorm(-3), accuracy = 1), -2.5,\n) %&gt;%\n  mutate(ch_loc = ch_loc_*se + true_effect,\n         se = se_*se + true_effect)\n\n# start with a ggplot\ngg1 &lt;- ggplot(animate_data, aes(x = est, \n                                y = study_id,\n                                group = group)) + \n  geom_vline(xintercept = 1.64*se) + \n  annotate(\"label\", x = 1.64*se, y = 5, label = \"1.64 SEs\") + \n  geom_errorbarh(height = 0, aes(xmin = lwr, xmax = upr, color = reject_null)) + \n  geom_point(aes(color = reject_null),\n             size = 3) + \n  geom_rug(sides = \"b\", \n           aes(x = est, \n               color = reject_null), \n           alpha = 0.5, \n           length = unit(0.025, \"npc\")) + \n  theme_bw() + \n  theme(panel.grid.minor.y = element_blank()) + \n  labs(x = \"Estimate of Effect\",\n       y = \"Study Number\") +\n  theme(legend.position = \"none\") + \n  scale_color_manual(values = c(\"Yes\" = \"#1b9e77\", \"No\" = \"#d95f02\"))\n\n# add dyamics to the plot\ngg1_anim &lt;- gg1 +  \n  transition_states(states = group) + \n  # how points enter\n  enter_drift(y_mod = 10) +\n  enter_grow() +\n  enter_fade() + \n  # how points exit/remain\n  exit_fade(alpha = 0.5) +\n  exit_shrink(size = 1) + \n  shadow_mark(alpha = 0.5) \n\ngg1_gif&lt;- animate(gg1_anim, nframes = nframes, duration = duration, width = width, height = height, units = \"in\", res = res)\nanim_save(\"gg1.gif\")\ngg1_mgif &lt;- image_read(\"gg1.gif\")\n\n\n## plot 2: histogram\n# start with a ggplot\ngg2 &lt;- ggplot(split_animate_data, aes(x = reject_null, fill = reject_null),  na.rm = TRUE) + \n  geom_bar(na.rm = TRUE) + \n  theme_bw() + \n  labs(x = \"Reject Null\",\n       y = \"Count\") + \n  theme(legend.position = \"none\") + \n  scale_x_discrete(na.translate = FALSE) + \n  scale_fill_manual(values = c(\"Yes\" = \"#1b9e77\", \"No\" = \"#d95f02\"))\n\n# add dyamics to the plot\ngg2_anim &lt;- gg2 +  \n  transition_states(states = frame)\n\ngg2_gif&lt;- animate(gg2_anim, nframes = nframes, duration = duration, width = width, height = height, units = \"in\", res = res)\nanim_save(\"gg2.gif\")\ngg2_mgif &lt;- image_read(\"gg2.gif\")\n\n\n\nnew_gif &lt;- image_append(c(gg1_mgif[1], gg2_mgif[1]), stack = FALSE)\nfor(i in 2:nframes){\n  combined_gif &lt;- image_append(c(gg1_mgif[i], gg2_mgif[i]), stack = FALSE)\n  new_gif &lt;- c(new_gif, combined_gif)\n}\nnew_gif"
  },
  {
    "objectID": "blog/2023-06-12-power-3-rule-of-364/index.html#the-key-ratio",
    "href": "blog/2023-06-12-power-3-rule-of-364/index.html#the-key-ratio",
    "title": "Power, Part III: The Rule of 3.64 for Statistical Power",
    "section": "The Key Ratio",
    "text": "The Key Ratio\nThe key to building statistical power into your experiment is to get “almost all” of the sampling distribution above 1.64 standard errors above zero. The portion of the sampling distribution that falls below 1.64 standard errors above zero does not allow the researcher to reject the null.\n\n\nCode\nlibrary(tidyverse)\n\n# study parameters\ntrue_effect &lt;- 1\nse &lt;- 0.4\n\n\nse_lines &lt;- tribble(\n  ~se_, ~label, ~chance, ~ch_loc_,  # trailing _ means not rescaeld to study se\n  0, \"True Effect\", NA, NA,\n  1, \"+1 SE\", scales::percent(pnorm(1) - pnorm(0), accuracy = 1), 0.5,\n  2, \"+2 SE\", scales::percent(pnorm(2) - pnorm(1), accuracy = 1), 1.5,\n  3, \"+3 SE\", scales::percent(pnorm(3) - pnorm(2), accuracy = 1), 2.5,\n  -1, \"-1 SE\", scales::percent(pnorm(0) - pnorm(-1), accuracy = 1), -0.5,\n  -2, \"-2 SE\", scales::percent(pnorm(-1) - pnorm(-2), accuracy = 1), -1.5,\n  -3, \"-3 SE\", scales::percent(pnorm(-2) - pnorm(-3), accuracy = 1), -2.5,\n) %&gt;%\n  mutate(ch_loc = ch_loc_*se + true_effect,\n         se = se_*se + true_effect) \n\nx &lt;- rnorm(500000, mean = true_effect, sd = se)\ndf &lt;- data.frame(x)\n\nbg_alpha &lt;- 0.3\nggplot() + \n  geom_histogram(data = df, \n                 aes(x = x, y = after_stat(density)), binwidth = se, boundary = true_effect, fill = \"grey\", alpha = bg_alpha) + \n  geom_vline(data = se_lines, aes(xintercept = se, \n                                  color = -dnorm(se_)), linetype = \"dashed\", alpha = bg_alpha) + \n  geom_label(data = se_lines, aes(x = se, y = Inf, label = label, group = NULL), vjust = 1.5, color = alpha('black', bg_alpha)) + \n  geom_label(data = se_lines, aes(x = ch_loc, y = 0, label = chance, group = NULL), vjust = -1, color = alpha('black', bg_alpha)) + \n  geom_vline(xintercept = 0) + \n  geom_function(fun = dnorm, args = list(mean = true_effect, sd = se), size = 1) + \n  theme_bw() + \n  labs(x = \"Estimate of Effect\",\n       y = \"Density\") + \n  geom_area(data = tibble(x = seq(1.64*se, 3*se + true_effect, by = 0.1)), aes(x = x), \n            stat = \"function\", fun = dnorm, args = list(mean = true_effect, sd = se),\n            fill = \"#d95f02\", alpha = 0.1, xlim = c(1.64*se, 3**se + true_effect)) + \n  annotate(\"label\", x = 1.3, y = .1, label = \"fraction rejected\", color = \"#d95f02\", size = 6) +\n  annotate(\"segment\", x = 1.64*se, xend = 1.64*se, y = 0, yend = dnorm(1.64*se, mean = true_effect, sd = se), color = \"#1b9e77\", size = 1) + \n  annotate(\"label\", x = 1.64*se, y = dnorm(1.64*se, mean = true_effect, sd = se)/2, label = \"1.64 SEs above zero\", color = \"#1b9e77\") + \n  annotate(\"segment\", x = 0, xend = 1.64*se, \n           y = dnorm(1.64*se, mean = true_effect, sd = se), \n           yend = dnorm(1.64*se, mean = true_effect, sd = se), \n           color = \"#7570b3\", size = 1) + \n  annotate(\"label\", x = 0.5*1.64*se, y = dnorm(1.64*se, mean = true_effect, sd = se), label = \"width of 90% CI\", color = \"#7570b3\") + \n  theme(legend.position = \"none\") + \n  xlim(-1, 3)\n\n\n\n\n\nYou’ll remember that “almost all” of the normal distribution falls within two standard errors of its mean. So as a starting point, let’s use this rule: get the sampling distribution two standard errors above 1.64 standard errors above zero. We can just add 1.64 and 2 together to get a sampling distribution 3.64 standard errors above zero. That is, we need \\(\\frac{\\text{true effect}}{\\text{standard error}} &gt; 3.64\\). If the true effect is larger than 3.64 standard errors, then the confidence interval will “rarely” overlap zero (about 2% of the time).\n\n\nCode\nlibrary(tidyverse)\n\n# study parameters\ntrue_effect &lt;- 1\nse &lt;- 1/3.84\n\nx &lt;- rnorm(500000, mean = true_effect, sd = se)\ndf &lt;- data.frame(x)\n\nggplot() + \n  geom_histogram(data = df, \n                 aes(x = x, y = after_stat(density)), binwidth = se, boundary = true_effect, fill = \"grey\", alpha = bg_alpha) + \n  geom_vline(data = se_lines, aes(xintercept = se, \n                                  color = -dnorm(se_)), linetype = \"dashed\", alpha = bg_alpha) + \n  geom_label(data = se_lines, aes(x = se, y = Inf, label = label, group = NULL), vjust = 1.5, color = alpha('black', bg_alpha)) + \n  geom_label(data = se_lines, aes(x = ch_loc, y = 0, label = chance, group = NULL), vjust = -1, color = alpha('black', bg_alpha)) + \n  geom_vline(xintercept = 0) + \n  geom_function(fun = dnorm, args = list(mean = true_effect, sd = se), size = 1) + \n  theme_bw() + \n  labs(x = \"Estimate of Effect\",\n       y = \"Density\") + \n  geom_area(data = tibble(x = seq(1.64*se, 3*se + true_effect, by = 0.1)), aes(x = x), \n            stat = \"function\", fun = dnorm, args = list(mean = true_effect, sd = se),\n            fill = \"#d95f02\", alpha = 0.1, xlim = c(1.64*se, 3**se + true_effect)) + \n  #annotate(\"label\", x = 1.3, y = .1, label = \"fraction rejected\", color = \"#d95f02\", size = 6) +\n  annotate(\"segment\", x = true_effect, xend = true_effect, y = 0, yend = dnorm(true_effect, mean = true_effect, sd = se), color = \"#d95f02\", size = 1) + \n  annotate(\"label\", x = true_effect, y = dnorm(true_effect, mean = true_effect, sd = se)/2, label = \"true effect\", color = \"#d95f02\") +   \n  annotate(\"segment\", x = 1.64*se, xend = 1.64*se, y = 0, yend = .75, color = \"#1b9e77\", size = 1) + \n  annotate(\"label\", x = 1.64*se, y = .75, label = \"1.64 SEs above zero\", color = \"#1b9e77\") + \n  annotate(\"segment\", x = 0, xend = 1.64*se, \n           y = .125, \n           yend = .125, \n           color = \"#7570b3\", size = 1,\n           lineend = \"round\", linejoin = \"round\", arrow = arrow(length = unit(0.1, \"inches\"), ends = \"both\")) + \n  annotate(\"label\", x = 0.5*1.64*se, y = .125, label = \"1.64 SEs\", color = \"#7570b3\", size = 4) + \n  annotate(\"segment\", x = true_effect, xend = 1.64*se, \n           y = .125, \n           yend = .125, \n           color = \"black\", size = 1,\n           lineend = \"round\", linejoin = \"round\", arrow = arrow(length = unit(0.1, \"inches\"), ends = \"both\")) + \n  annotate(\"label\", x = 1.64*se + (true_effect - 1.64*se)/2, y = .125\n  \n  , label = \"ideally 2 SEs\", color = \"black\", size = 4) + \n  annotate(\"segment\", x = true_effect, xend = 0, \n           y = 1, yend = 1, \n           color = \"black\", size = 1,\n           lineend = \"round\", linejoin = \"round\", arrow = arrow(length = unit(0.1, \"inches\"), ends = \"both\")) + \n  annotate(\"label\", x = true_effect/2, y = 1, label = \"ideally 3.64 SEs\", color = \"black\", size = 4) + \n  theme(legend.position = \"none\") + \n  xlim(-0.1, 2.0)"
  },
  {
    "objectID": "blog/2023-06-12-power-3-rule-of-364/index.html#the-resulting-guidelines",
    "href": "blog/2023-06-12-power-3-rule-of-364/index.html#the-resulting-guidelines",
    "title": "Power, Part III: The Rule of 3.64 for Statistical Power",
    "section": "The Resulting Guidelines",
    "text": "The Resulting Guidelines\nThere are a few ways to write the ratio:\n\n\\(\\frac{\\text{true effect}}{\\text{standard error}} &gt; 3.64\\)\n\\(\\text{true effect} &gt; 3.64 \\times \\text{standard error}\\)\n\\(\\text{standard error} &lt; \\frac{1}{3.64} \\times \\text{true effect} \\approx 0.27 \\times \\text{true effect}\\)\n\nThese are all the same goals. I prefer the first, but they are all equivalent targets for power.\nI like this ratio because it points to strategies to increase power. You can do two things:\n\nIncrease the true effect.\nDecrease the standard error.\n\nThis ratio gets you thinking not what your power is, but how to increase it. Again, power isn’t a task for the experimenter, it’s the task.5 Kane (2023)6 suggests several ways researchers can increase the true effect or decrease the standard error.5 I’m leaving aside the question of how to predict the standard error or choose a true effect for a paricular design.6 Kane, John V. 2023. “More Than Meets the ITT: A Guide for Investigating Null Results.” APSA Preprints. doi: 10.33774/apsa-2023-h4p0q-v2.\nI’ll highlight a few examples here. To increase the treatment effect, you can:\n\nIncrease attentiveness.\nEnsure that respondents are not pre-treated.\nWrite strong treatments or “hit them between the eyes” (Kuklinski et al. 2000).\n\nTo decrease the standard error, you can:\n\nIncrease the sample size.\nUse multiple measurements for the outcome.\nUse a pre-post design (Clifford, Sheagley, and Piston 2022)."
  },
  {
    "objectID": "blog/2023-06-12-power-3-rule-of-364/index.html#implication",
    "href": "blog/2023-06-12-power-3-rule-of-364/index.html#implication",
    "title": "Power, Part III: The Rule of 3.64 for Statistical Power",
    "section": "Implication",
    "text": "Implication\nIf you get the ratio to 3.64, then you’ll have 98%. This is much higher than the cutoff of 80% I hear suggested most often. If you want 80% power, rather than 98% power, you can change the 3.64 to 2.48.7 But notice that 3.64 and 2.48 are not very different. This has an important implication.7 Find this by adding 1.64 to -qnorm(0.2).\nA researcher can lower the risk of a failed study from about 1 in 5 (80% power) to about 1 in 50 (98% power) by increasing the ratio from 2.48 to 3.64. This requires increasing the treatment effect by about 50% or shrinking the standard error by about 33%. Stated differently, if you are careless and let your treatment effect fall by 33% or let your standard error increase by 50%, then your risk of a failed study increases 10-fold! Small differences can matter a lot.\nThe implication is this: when you are on the cusp of a well-powered study (about 80% power), then small increases in the ratio have a disproportionate impact on your risk of a failed study."
  },
  {
    "objectID": "blog/2023-06-12-power-3-rule-of-364/index.html#computing-power-from-the-ratio",
    "href": "blog/2023-06-12-power-3-rule-of-364/index.html#computing-power-from-the-ratio",
    "title": "Power, Part III: The Rule of 3.64 for Statistical Power",
    "section": "Computing Power from the Ratio",
    "text": "Computing Power from the Ratio\nFollowing the logic of the picture above, we need to compute the fraction of the sampling distributin that lies above 1.64 SEs. The pnorm() function returns normal probabilities, but below specfic thresholds. By supplying the argument lower.tail = FALSE, we can get the probabilities of falling above a specific threshold.\n\ntrue_effect &lt;- 1.00\nse &lt;- 0.4\n\n# compute power\npnorm(1.64*se,   # want fraction above 1.64 SE\n      mean = true_effect,  # mean of sampling distribution\n      sd = se,  # sd of sampling distribution\n      lower.tail = FALSE)  # fraction above, not below\n\n[1] 0.8051055"
  },
  {
    "objectID": "blog/2023-06-12-power-3-rule-of-364/index.html#summary",
    "href": "blog/2023-06-12-power-3-rule-of-364/index.html#summary",
    "title": "Power, Part III: The Rule of 3.64 for Statistical Power",
    "section": "Summary",
    "text": "Summary\nStatistical power is an abstract quantity. It’s easy to understand what it means, but harder to think about how to manipulate it. I explain why the target \\(\\frac{\\text{true effect}}{\\text{standard error}} &gt; 2.48\\) is equivalent to a target power of 80%. But you want to “almost always” reject the null, so you should shoot for \\(\\frac{\\text{true effect}}{\\text{standard error}} &gt; 3.64\\). Hopefully these guidelines help you build a bit of actionable intuition about statistical power."
  },
  {
    "objectID": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html",
    "href": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html",
    "title": "Power, Part II: What Do Confidence Intervals from Well-Powered Studies Look Like?",
    "section": "",
    "text": "In this post, I address confidence intervals that are nestled right up against zero.1 These intervals indicate that an estimate is “barely” significant. I want to be clear: “barely significant” is still significant, so you should still reject the null hypothesis.21 This is the second post in a series. In my previous post, I mentioned two new papers that have me thinking about power: Arel-Bundock et al.’s “Quantitative Political Science Research Is Greatly Underpowered” and Kane’s “More Than Meets the ITT: A Guide for Investigating Null Results”. Go check out that post and those papers if you haven’t.2 I’m focusing on confidence intervals here because inference from confidence intervals is a bit more intuitive (see Rainey 2014 and Rainey 2015. In the cases I discuss, whether one checks whether the p-value is less than 0.05 or checks that confidence interval contains zero are equivalent.\nBut I want to address a feeling that can come along with a confidence interval nestled right up against zero. A feeling of victory. It seems like a perfectly designed study. You rejected the null and collected just enough data to do it.\nBut instead, it should feel like a near-miss. Like an accident narrowly avoided. A confidence interval nestled right up against zero indicates that one of two things has happened: either you were (1) unlucky or (2) under-powered.\nBecause “unlucky” is always a possibility, we can’t learn much from a particular confidence interval, but we can learn a lot from a literature. A literature with well-powered studies produces confidence intervals that often fall far from zero. A well-powered literature does not produce confidence intervals that consistently nestle up against zero. Under-powered studies, though, do tend to produce confidence intervals that nestle right up against zero."
  },
  {
    "objectID": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#background",
    "href": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#background",
    "title": "Power, Part II: What Do Confidence Intervals from Well-Powered Studies Look Like?",
    "section": "",
    "text": "In this post, I address confidence intervals that are nestled right up against zero.1 These intervals indicate that an estimate is “barely” significant. I want to be clear: “barely significant” is still significant, so you should still reject the null hypothesis.21 This is the second post in a series. In my previous post, I mentioned two new papers that have me thinking about power: Arel-Bundock et al.’s “Quantitative Political Science Research Is Greatly Underpowered” and Kane’s “More Than Meets the ITT: A Guide for Investigating Null Results”. Go check out that post and those papers if you haven’t.2 I’m focusing on confidence intervals here because inference from confidence intervals is a bit more intuitive (see Rainey 2014 and Rainey 2015. In the cases I discuss, whether one checks whether the p-value is less than 0.05 or checks that confidence interval contains zero are equivalent.\nBut I want to address a feeling that can come along with a confidence interval nestled right up against zero. A feeling of victory. It seems like a perfectly designed study. You rejected the null and collected just enough data to do it.\nBut instead, it should feel like a near-miss. Like an accident narrowly avoided. A confidence interval nestled right up against zero indicates that one of two things has happened: either you were (1) unlucky or (2) under-powered.\nBecause “unlucky” is always a possibility, we can’t learn much from a particular confidence interval, but we can learn a lot from a literature. A literature with well-powered studies produces confidence intervals that often fall far from zero. A well-powered literature does not produce confidence intervals that consistently nestle up against zero. Under-powered studies, though, do tend to produce confidence intervals that nestle right up against zero."
  },
  {
    "objectID": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#a-simulation",
    "href": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#a-simulation",
    "title": "Power, Part II: What Do Confidence Intervals from Well-Powered Studies Look Like?",
    "section": "A Simulation",
    "text": "A Simulation\nI’m going to explore the behavior of confidence intervals with a little simulation. In this simulation, I’m going to assert a standard error rather than create the standard error endogenously through sample size, etc. I use a true effect size of 0.5 and standard errors of 0.5, 0.3, 0.2, and 0.15 to create studies with 25%, 50%, 80%, and 95% power, respectively.3 I think of 80% as “minimally-powered” and 95% as “well-powered.”3 I’m ignoring how to choose the true effect, estimate the standard error, and compute power. For now, I’m placing all this behind the curtain. See Daniël Lakens’ book [Improving Your Statistical Inferences] for discussion (h/t Bermond Scoggins).\nI’m using a one-sided test (hypothesizing a positive effect), so I’ll use 90% confidence intervals with arms that are 1.64 standard errors wide. Let’s simulate some estimates from each of our four studies and compute their confidence intervals. I simulate 5,000 confidence intervals to explore below.\n\n\nCode\n# load packages\nlibrary(tidyverse)\n\n# create a parameter for the true effect\ntrue_effect &lt;- 0.5 # just assumed by me\n\n# create a data frame of standard errors (with approximate power)\nse_df &lt;- tribble(\n  ~se,    ~pwr,\n  0.5,    \"about 25% power\",\n  0.3,    \"about 50% power\",\n  0.2,    \"about 80% power\",\n  0.15,   \"about 95% power\"\n)\n\n# create function to simulate estimates for each standard error\nsimulate_estimates &lt;- function(se, pwr) {\n  tibble(\n    est = rnorm(n_cis, mean = true_effect, sd = se),\n    se = se,\n    pwr = pwr\n  )\n}\n\n# simulate the estimates, compute the confidence intervals, and wrangle\nn_cis &lt;- 5000  # the number of cis to create\nci_df &lt;- se_df %&gt;% \n  # simulate estimates\n  pmap_dfr(simulate_estimates) %&gt;%\n  # compute confidence intervals\n  mutate(lwr = est - 1.64*se, \n         upr = est + 1.64*se) %&gt;%\n  # summarize the location of the confidence interval\n  mutate(result = case_when(lwr &lt; 0 ~ \"Not significant\",\n                            lwr &lt; se ~ \"Nestled against zero\",\n                            lwr &gt;= se~ \"Not nestled against zero\"))\n\n\nNow let’s quickly confirm my power calculations by computing the proportion of confidence intervals to the right of zero. These are about right. In a later post, I’ll describe how I think about computing these quantities.\n\n\nCode\n# confirm power calculations\nci_df %&gt;%\n  group_by(se, pwr) %&gt;%\n  summarize(sim_pwr = 1 - mean(result == \"Not significant\"),\n            sim_pwr = scales::percent(sim_pwr, accuracy = 1)) %&gt;%\n    select(SE = se, \n         Power = pwr,\n         `Percent Significant` = sim_pwr) %&gt;%\n  kableExtra::kable(format = \"markdown\")\n\n\n\n\n\nSE\nPower\nPercent Significant\n\n\n\n\n0.15\nabout 95% power\n95%\n\n\n0.20\nabout 80% power\n80%\n\n\n0.30\nabout 50% power\n51%\n\n\n0.50\nabout 25% power\n25%"
  },
  {
    "objectID": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#what-do-confidence-intervals-from-well-powered-studies-look-like",
    "href": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#what-do-confidence-intervals-from-well-powered-studies-look-like",
    "title": "Power, Part II: What Do Confidence Intervals from Well-Powered Studies Look Like?",
    "section": "What Do Confidence Intervals from Well-Powered Studies Look Like?",
    "text": "What Do Confidence Intervals from Well-Powered Studies Look Like?\nNow let’s see what these confidence intervals look like. 5,000 is too many to plot, so I sample 25. But I applied the statistical significance filter first. This mimics the publication process and makes the plots a little easier to compare. My argument doesn’t depend on this filter, though.\nI plotted these 100 intervals below44 4 studies x 25 simulated intervals per study = 100 intervals.\nThere are three important vertical lines in these plots.\n\nThe solid line indicates zero. All confidence intervals are above zero because I applied the significance filter.\nThe dotted line indicates one standard error above zero. This varies across panels because the standard error varies across panels.\nThe dashed line indicates the true effect of 0.5. Because I applied the significance filter, the lower-powered studies are consistently over-estimating the true effect.\n\nThe intervals are green when the lower bound of the 90% confidence interval falls within one standard error of zero—that’s my definition of “nestled up against zero.” The intervals are orange when the lower bound falls further than one standard error above zero.\nNotice how low-powered studies tend to nestle their confidence intervals right up against zero. Almost all of the confidence intervals from the study with 25% power are nestled right up against zero. Very few of the confidence intervals from the study with 95% power are nestled up against zero.\nAgain, you should apply this standard to a literature. You should not apply this standard to a particular study because even well-powered studies sometimes produce confidence intervals that nestle up against zero. But when you start to see confidence intervals consistently falling close to zero, you should start to assume that the literature uses under-powered studies and that the estimates in that literature are inflated due to Type M errors (Gelman and Stern 2014).\n\n\nCode\ngg_df &lt;- ci_df %&gt;%\n  filter(lwr &gt; 0) %&gt;% # apply significance filter \n  # sample 25 intervals (from those that are significant)\n  group_by(se, pwr) %&gt;%\n  sample_n(25) %&gt;%\n  # create id (ordered by estimate value)\n  group_by(se, pwr) %&gt;%\n  arrange(est) %&gt;%\n  mutate(ci_id = 1:n())\n  \nggplot(gg_df, aes(x = est, xmin = lwr, xmax = upr, y = ci_id,\n                    color = result)) + \n  facet_wrap(vars(pwr), ncol = 1, scales = \"free_x\") + \n  geom_vline(data = se_df, aes(xintercept = se), linetype = \"dotted\") +\n  geom_vline(xintercept = 0) + \n  geom_vline(xintercept = true_effect, linetype = \"dashed\") + \n  geom_errorbarh(height = 0) + \n  geom_point() + \n  scale_color_brewer(type = \"qual\", palette = 2) + \n  theme_bw() + \n  labs(x = \"Estimate and 90% CI\",\n       y = NULL,\n       color = \"Result\")"
  },
  {
    "objectID": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#showing-this-another-way-density-of-the-lower-bounds",
    "href": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#showing-this-another-way-density-of-the-lower-bounds",
    "title": "Power, Part II: What Do Confidence Intervals from Well-Powered Studies Look Like?",
    "section": "Showing This Another Way: Density of the Lower Bounds",
    "text": "Showing This Another Way: Density of the Lower Bounds\nWe can also plot the density of the lower bounds of these 5,000 intervals. This approach shows the “nestling” most clearly. The plots below show that the lower bounds of confidence intervals tend to nestle close to zero when the power is low, and lie further from zero when the power is high.\n\n\nCode\ngg_df &lt;- ci_df %&gt;%\n  filter(lwr &gt; 0) # apply significance filter \nggplot(gg_df, aes(x = lwr)) + \n  facet_wrap(vars(pwr), scales = \"free_x\") + \n  geom_density(fill = \"grey50\") + \n  geom_vline(data = se_df, aes(xintercept = se), linetype = \"dotted\") + \n  theme_bw() + \n  labs(x = \"Location of Lower Bound of 90% CI\",\n       y = \"Density\",\n       color = \"Power\")"
  },
  {
    "objectID": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#showing-this-another-way-frequency-of-nestling",
    "href": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#showing-this-another-way-frequency-of-nestling",
    "title": "Power, Part II: What Do Confidence Intervals from Well-Powered Studies Look Like?",
    "section": "Showing This Another Way: Frequency of Nestling",
    "text": "Showing This Another Way: Frequency of Nestling\nLastly, I compute the percent of confidence intervals that are nestled right up against zero. For a well-powered study with 95% power, only about 1 in 5 confidence intervals nestle up against zero. For a poorly-powered study with 25% power, about 4 in 5 of confidence intervals nestle up against zero (among those that are above zero). The table below shows the remaining frequencies.\n\n\nCode\nci_df %&gt;%\n  group_by(se, pwr, result) %&gt;%\n  summarize(frac = n()/n_cis, .groups = \"drop\") %&gt;%\n  pivot_wider(names_from = result, values_from = frac) %&gt;%\n  mutate(`Nestled, given significant` = `Nestled against zero`/(1 - `Not significant`),\n         `Not nestled, given significant` = `Not nestled against zero`/(1 - `Not significant`)) %&gt;%\n  select(SE = se, \n         Power = pwr,\n         `Not significant`,\n         `Nestled against zero`,\n         `Not nestled against zero`,\n         `Nestled, given significant`,\n         `Not nestled, given significant`) %&gt;%\n  mutate(across(`Not significant`:`Not nestled, given significant`, ~ scales::percent(., accuracy = 1))) %&gt;%\n  kableExtra::kable()\n\n\n\n\n\nSE\nPower\nNot significant\nNestled against zero\nNot nestled against zero\nNestled, given significant\nNot nestled, given significant\n\n\n\n\n0.15\nabout 95% power\n5%\n20%\n75%\n21%\n79%\n\n\n0.20\nabout 80% power\n20%\n37%\n44%\n46%\n54%\n\n\n0.30\nabout 50% power\n49%\n34%\n17%\n67%\n33%\n\n\n0.50\nabout 25% power\n75%\n21%\n5%\n81%\n19%"
  },
  {
    "objectID": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#summary",
    "href": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#summary",
    "title": "Power, Part II: What Do Confidence Intervals from Well-Powered Studies Look Like?",
    "section": "Summary",
    "text": "Summary\nIn this post, I address confidence intervals that are nestled right up against zero. These intervals can suggest a perfectly powered study—not too much, not too little. But instead, a confidence interval nestled right up against zero indicates that one of two things has happened: either you were (1) unlucky or (2) under-powered.\nBecause “unlucky” is always a possibility, we can’t learn much from a particular confidence interval, but we can learn a lot from a literature. A literature with well-powered studies produces confidence intervals that often fall far from zero. A well-powered literature does not produce confidence intervals that consistently nestle up against zero. Under-powered studies, though, do tend to produce confidence intervals that nestle right up against zero.\n\n\n\n\n\n\nKey Takeaway\n\n\n\nUnder-powered studies tend to produce confidence intervals that are nestled right up against zero. Well-powered studies tend to produce confidence intervals that fall further away. A literature that produces confidence intervals that consistently nestle right up against zero is likely a collection of under-powered studies."
  },
  {
    "objectID": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html",
    "href": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html",
    "title": "Power, Part I: Power Is for You, Not for Reviewer Two",
    "section": "",
    "text": "There’s been some really good work lately on statistical power. I’ll point you to two really great papers.\n\nArel-Bundock, Vincent, Ryan C. Briggs, Hristos Doucouliagos, Marco Mendoza Aviña, and T.D. Stanley. 2022. “Quantitative Political Science Research Is Greatly Underpowered.” OSF Preprints. July 5. doi: 10.31219/osf.io/7vy2f.\nKane, John V. 2023. “More Than Meets the ITT: A Guide for Investigating Null Results .” APSA Preprints. doi: 10.33774/apsa-2023-h4p0q-v2.\n\nI’ve been long interested in statistical power (see Rainey 20141 and Rainey 20152), and these new papers have me thinking even more about the importance of power.1 Rainey, Carlisle. 2014. “Arguing for a Negligible Effect.” American Journal of Political Science 58(4): 1083-1091.2 McCaskey, Kelly and Carlisle Rainey. 2015. “Substantive Importance and the Veil of Statistical Significance.” Statistics, Politics, and Policy 6(1-2): 77-96.\nIn this post, I argue that statistical power isn’t something ancillary. Power is primary. I also argue that power isn’t something you–the researcher–build to satisfy an especially cranky Reviewer 2, it’s something you do for yourself, to make sure that your study succeeds."
  },
  {
    "objectID": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#background",
    "href": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#background",
    "title": "Power, Part I: Power Is for You, Not for Reviewer Two",
    "section": "",
    "text": "There’s been some really good work lately on statistical power. I’ll point you to two really great papers.\n\nArel-Bundock, Vincent, Ryan C. Briggs, Hristos Doucouliagos, Marco Mendoza Aviña, and T.D. Stanley. 2022. “Quantitative Political Science Research Is Greatly Underpowered.” OSF Preprints. July 5. doi: 10.31219/osf.io/7vy2f.\nKane, John V. 2023. “More Than Meets the ITT: A Guide for Investigating Null Results .” APSA Preprints. doi: 10.33774/apsa-2023-h4p0q-v2.\n\nI’ve been long interested in statistical power (see Rainey 20141 and Rainey 20152), and these new papers have me thinking even more about the importance of power.1 Rainey, Carlisle. 2014. “Arguing for a Negligible Effect.” American Journal of Political Science 58(4): 1083-1091.2 McCaskey, Kelly and Carlisle Rainey. 2015. “Substantive Importance and the Veil of Statistical Significance.” Statistics, Politics, and Policy 6(1-2): 77-96.\nIn this post, I argue that statistical power isn’t something ancillary. Power is primary. I also argue that power isn’t something you–the researcher–build to satisfy an especially cranky Reviewer 2, it’s something you do for yourself, to make sure that your study succeeds."
  },
  {
    "objectID": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#the-hypothesis-testing-framework",
    "href": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#the-hypothesis-testing-framework",
    "title": "Power, Part I: Power Is for You, Not for Reviewer Two",
    "section": "The Hypothesis Testing Framework",
    "text": "The Hypothesis Testing Framework\nIn the hypothesis testing framework, you consider two hypotheses: the null hypothesis and the alternative hypothesis.\nThe hypothesis test is all about arguing against the null hypothesis \\(H_0\\) (leaving the alternative \\(H_A\\) as the only remaining possibility). You will (try to) show that your data would be “unusual” if the null hypothesis were correct.33 When hypothesizing about the average treatment effect (ATE), this can take a variety of forms. The form doesn’t really matter.\nIf the data would NOT be unusual under the null hypothesis, then you do not reject the null hypothesis."
  },
  {
    "objectID": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#intepreting-a-failure-to-reject",
    "href": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#intepreting-a-failure-to-reject",
    "title": "Power, Part I: Power Is for You, Not for Reviewer Two",
    "section": "Intepreting a Failure to Reject",
    "text": "Intepreting a Failure to Reject\nA failure to reject means that the data “would not be unusual under the null hypothesis.” This does not imply that you should conclude the data are only consistent with the null. Indeed, there is a sharp asymmetry in hypothesis testing. I describe this in my 2014 AJPS:\n\nPolitical scientists commonly interpret a lack of statistical significance (i.e., a failure to reject the null) as evidence for a negligible effect (Gill 1999), but this approach acts as a broken compass… If the sample size is too small, the researcher often concludes that the effect is negligible even though the data are also consistent with large, meaningful effects. This occurs because the small sample leads to a large confidence interval, which is likely to contain both “no effect” and large effects.\n\nGill (1999)4 describes this more forcefully:4 Gill, Jeff. 1999. “The Insignificance of Null Hypothesis Significance Testing.” Political Research Quarterly 52(3): 647-674.\n\nWe teach graduate students to be very careful when describing the occurrence of not rejecting the null hypothesis. This is because failing to reject the null hypothesis does not rule out an infinite number of other competing research hypotheses. Null hypothesis significance testing is asymmetric: if the test statistic is sufficiently atypical given the null hypothesis then the null hypothesis is rejected, but if the test statistic is insufficiently atypical given the null hypothesis then the null hypothesis is not accepted. This is a double standard: H1 is held innocent until proven guilty and Ho is held guilty until proven innocent (Rozeboom 1960)…\n\n\nThere are two problems that develop as a result of asymmetry. The first is a misinterpretation of the asymmetry to assert that finding a non-statistically significant difference or effect is evidence that it is equal to zero or nearly zero. Regarding the impact of this acceptance error Schmidt (1996: 126) asserts that this: “belief held by many researchers is the most devastating of all to the research enterprise.” This acceptance of the null hypothesis is damaging because it inhibits the exploration of competing research hypotheses. The second problem pertains to the correct interpretation of failing to reject the null hypotheses. Failing to reject the null hypothesis essentially provides almost no information about the state of the world. It simply means that given the evidence at hand one cannot make an assertion about some relationship: all you can conclude is that you can’t conclude that the null was false (Cohen 1962).\n\nThere are many incorrect, but somewhat innocent interpretations of p-values. Interpreting a lack of statistical significance as evidence for the null is incorrect and wildly misleading in many cases.\n\n\n\n\n\n\nImportant Point\n\n\n\nA non-statistically significant difference is not evidence that an effect is equal to zero or nearly zero. Interpreting a non-statistically significant effect otherwise is “devastating.”"
  },
  {
    "objectID": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#the-implication-of-a-non-conclusion",
    "href": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#the-implication-of-a-non-conclusion",
    "title": "Power, Part I: Power Is for You, Not for Reviewer Two",
    "section": "The Implication of a Non-Conclusion",
    "text": "The Implication of a Non-Conclusion\nIf you cannot draw a conclusion then, what exactly has happened? Obtaining \\(p &gt; 0.05\\) will not be an “error” because you won’t make a strong claim that the research hypothesis is wrong. Instead, you will simply admit that you failed to uncover evidence against the null. Failing to uncover evidence isn’t an error.\nIndeed, Jones and Tukey (2000)5 write:5 Jones, Lyle V., and John W. Tukey. 2000. “A Sensible Formulation of the Significance Test.” Psychological Methods 5(4): 411-414.\n\nA conclusion is in error only when it is “a reversal,” when it asserts one direction while the (unknown) truth is the other direction. Asserting that the direction is not yet established may constitute a wasted opportunity, but it is not an error.\n\n\n\n\n\n\n\nImportant Point\n\n\n\nFailing to uncover evidence isn’t an “error,” it is a “wasted effort.”\n\n\nThis is worth emphasizing in a different way. Tests are not magical tools that tell you which hypothesis is correct. Instead, tests summarize the evidence against the null. There are two critical pieces to “evidence against the null”: (1) the amount of evidence and (2) whether the evidence is against the null. If you buy your own argument that the null is false (surely you do!), then (2) is taken care of. Only the amount of evidence remains, and you–the researcher–choose the amount of evidence to supply."
  },
  {
    "objectID": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#the-implication-for-power-calculations",
    "href": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#the-implication-for-power-calculations",
    "title": "Power, Part I: Power Is for You, Not for Reviewer Two",
    "section": "The Implication for Power Calculations",
    "text": "The Implication for Power Calculations\nThis perspective helps motivate power calculations. By their design, tests control the error rate in certain situations (when then null is correct). You do not need to worry about Type I errors. First, the test controls the error rate under the null. Second, you are pretty sure the null is wrong (see your theory section).\n\n\n\n\n\n\nImportant Point\n\n\n\nThe hypothesis test takes care of the the Type I error rate. If you choose a properly-sized test, you don’t need to worry about those errors any more.\n\n\nIf you aren’t worried about Type I errors, what are you worried about? They only thing left to worry about is wasting your time and money. Statistical power is the chance not of wasting your time and money.\nPower isn’t a secondary quantity that you compute for thoroughness or in anticipation of a comment from Reviewer 2. Power is something that you build for yourself.\nStatisticians talk a lot about Type I errors because that’s their contribution. It’s your job to bring the power.\nAnd importantly, power is under your control. Kane provides a rich summary of ways to increase the power of your experiment. At a minimum, you have brute force control through sample size.\nPower isn’t an ancillary concern, it’s the entire game from the very beginning of the planning stage. It should be at the forefront of the researchers mind from the very beginning. You should want the power as high as possible.66 I hear that 80% is the standard, but I’m pretty uncomfortable spending dozens of hours and thousands of dollars running for a 1 in 5 chance of wasting my time. I want that chance as close to zero as I can get it. I want power close to 100%. 99% power and 80% power might both seem “high” or “acceptable,” but these are not the same. 80% power means 1 in 5 studies fail. 99% power means that 1 in 100 studies fail.\nYou have to supply a test overwhelming evidence to consistently reject the null. Careful power calculations help you make sure you succeed in this war against the null.\nPower isn’t about Type S and M errors (Gelman and Carlin 2014)7. Power is about you protecting yourself from a failed study. And that seems like a protection worth pursuing carefully.87 Gelman, Andrew, and John Carlin. “Beyond Power Calculations: Assessing Type S (Sign) and Type M (Magnitude) Errors.” Perspectives on Psychological Science 9(6): 641-651.8 Of course it’s also about Type S and M errors, but those are discipline-level concerns. I’m talking about your incentives as a researcher."
  },
  {
    "objectID": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#summary",
    "href": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#summary",
    "title": "Power, Part I: Power Is for You, Not for Reviewer Two",
    "section": "Summary",
    "text": "Summary\nHere are the takeaways:\n\nStatistical power is the chance of using your time and money productively (i.e., not wasting it).\nStatistical power is under your control (see Kane).\nYour power might be (much) lower than you think–you should check (see Arel-Bundock et al.).\nPower should be a primary concern throughout the design. The researcher should care deeply about power, perhaps more than anything else.\n\n\n\n\n\n\n\nImportant Point\n\n\n\nThe hypothesis test is no oracle. It will not consistently reject the null (even when the null is wrong) unless you supply overwhelming evidence. In experimental design, that’s not a task, that’s the task."
  },
  {
    "objectID": "blog/2023-06-01-teaching-confidence-intervals-with-gganimate/index.html",
    "href": "blog/2023-06-01-teaching-confidence-intervals-with-gganimate/index.html",
    "title": "Teaching Confidence Intervals and Hypothesis Testing with gganimate",
    "section": "",
    "text": "When I give students formula for confidence intervals, I find that students don’t have a sharp concept of how those confidence intervals work—even if I explain the components of the formula well.\nEven though they understand—seemingly very well—that the point estimate is noisy, they struggle to conceptualize that a confidence interval can often include values on the incorrect side of zero. Stated differently, they have a hard time understanding how a hypothesis test can fail to reject the null (when the null is incorrect). Their intuition suggests that a “test” should give you the correct answer.\nBecause their instincts are wrong, I want to undermine their trust in hypothesis tests. I want them to feel the riskiness of poorly-powered experiments that consistently nestle confidence intervals right up against zero. I want them ready and eager to work hard to avoid that risk—to make sure they have adequate statistical power.\nTo help undermine their confidence in confidence intervals, I like three exercises that mimic a test with 80% power. In each case, we are assuming that we have formulated a correct hypothesis and designed an excellent experiment with 80% power.\n\nFirst, I have students roll a six-sided die. If the die produces a , then their study fails—they wasted their opportunity. See this post for details on this perspective. This simulates the riskiness of an experiment with 80% power quite well. They get lots of failed experiments in a short period of time. They become well-aware of the possibility of a failed study and grow increasingly interested in reducing this risk.\nSecond, I use a computer to produce a plot of many (about 50 seems right) confidence intervals from the same repeated study. I explain that the interval we will get in the study we actually conduct is like a random draw from this collection. (See below for an example of this figure.)\nThird, I make the plot dynamic. I find that dynamics make the plot more memorable and convey the (appropriate) idea that hypothesis tests and confidence intervals are chaotic, noisy quantities.\n\nThese exercises make it clear that failed studies are real possibilities. Hopefully they clearly see and “feel”:\n\nThe hypothesis test is no oracle. It will not consistently reject the null (even when the null is wrong) unless you supply overwhelming evidence. In experimental design, that’s not a task, that’s the task.\n\nBelow, I walk through the plots I use in parts 2 and 3."
  },
  {
    "objectID": "blog/2023-06-01-teaching-confidence-intervals-with-gganimate/index.html#background",
    "href": "blog/2023-06-01-teaching-confidence-intervals-with-gganimate/index.html#background",
    "title": "Teaching Confidence Intervals and Hypothesis Testing with gganimate",
    "section": "",
    "text": "When I give students formula for confidence intervals, I find that students don’t have a sharp concept of how those confidence intervals work—even if I explain the components of the formula well.\nEven though they understand—seemingly very well—that the point estimate is noisy, they struggle to conceptualize that a confidence interval can often include values on the incorrect side of zero. Stated differently, they have a hard time understanding how a hypothesis test can fail to reject the null (when the null is incorrect). Their intuition suggests that a “test” should give you the correct answer.\nBecause their instincts are wrong, I want to undermine their trust in hypothesis tests. I want them to feel the riskiness of poorly-powered experiments that consistently nestle confidence intervals right up against zero. I want them ready and eager to work hard to avoid that risk—to make sure they have adequate statistical power.\nTo help undermine their confidence in confidence intervals, I like three exercises that mimic a test with 80% power. In each case, we are assuming that we have formulated a correct hypothesis and designed an excellent experiment with 80% power.\n\nFirst, I have students roll a six-sided die. If the die produces a , then their study fails—they wasted their opportunity. See this post for details on this perspective. This simulates the riskiness of an experiment with 80% power quite well. They get lots of failed experiments in a short period of time. They become well-aware of the possibility of a failed study and grow increasingly interested in reducing this risk.\nSecond, I use a computer to produce a plot of many (about 50 seems right) confidence intervals from the same repeated study. I explain that the interval we will get in the study we actually conduct is like a random draw from this collection. (See below for an example of this figure.)\nThird, I make the plot dynamic. I find that dynamics make the plot more memorable and convey the (appropriate) idea that hypothesis tests and confidence intervals are chaotic, noisy quantities.\n\nThese exercises make it clear that failed studies are real possibilities. Hopefully they clearly see and “feel”:\n\nThe hypothesis test is no oracle. It will not consistently reject the null (even when the null is wrong) unless you supply overwhelming evidence. In experimental design, that’s not a task, that’s the task.\n\nBelow, I walk through the plots I use in parts 2 and 3."
  },
  {
    "objectID": "blog/2023-06-01-teaching-confidence-intervals-with-gganimate/index.html#an-experiment-that-we-can-repeat",
    "href": "blog/2023-06-01-teaching-confidence-intervals-with-gganimate/index.html#an-experiment-that-we-can-repeat",
    "title": "Teaching Confidence Intervals and Hypothesis Testing with gganimate",
    "section": "An Experiment that We Can Repeat",
    "text": "An Experiment that We Can Repeat\nFirst, let’s choose to conduct an experiment with 80% power. To get this, we’ll suppose that the true effect is 1 and the standard error is 0.4. To obtain 80% power, you can use the guideline that the standard error should be about 40% of the true effect (or the true effect divided by 2.48). I’ll show where these guidelines come from in a future post. With the true effect and standard error in hand, we can compute the long-run properties of the experiment.\n\n\nCode\nlibrary(tidyverse)\n\n# study parameters\ntrue_effect &lt;- 1\nse &lt;- 0.40  # 1/2.48\n\n# identify effects of interest\neoi &lt;- tribble(\n  ~Effect, ~Description,\n  true_effect, \"True Effect (known in this exercise)\"\n) \n\n# compute quantities of interest regarding power\neoi %&gt;%\n  mutate(Power = 1 - pnorm(1.64*se, Effect, se),\n         Power = scales::percent(Power, accuracy = 1),\n         `Type S` = retrodesign::type_s(Effect, se)$type_s,\n         `Type S` = scales::number(`Type S`, accuracy = 0.01),\n         `Type M` = retrodesign::type_m(Effect, se)$type_m,\n         `Type M` = scales::number(`Type M`, accuracy = 0.01),\n         Effect = scales::number(Effect, accuracy = 0.01)) %&gt;% \n  pivot_longer(cols = Effect:`Type M`) %&gt;%\n  kableExtra::kable(format = \"markdown\", col.names = NULL)\n\n\n\n\n\nEffect\n1.00\n\n\nDescription\nTrue Effect (known in this exercise)\n\n\nPower\n81%\n\n\nType S\n0.00\n\n\nType M\n1.20"
  },
  {
    "objectID": "blog/2023-06-01-teaching-confidence-intervals-with-gganimate/index.html#a-static-plot",
    "href": "blog/2023-06-01-teaching-confidence-intervals-with-gganimate/index.html#a-static-plot",
    "title": "Teaching Confidence Intervals and Hypothesis Testing with gganimate",
    "section": "A Static Plot",
    "text": "A Static Plot\nBut these long-run properties remain a bit abstract and seem “distant” from the practical implications of our particular experiment. This is where the three exercises above come in handy.\nThe second exercise is a static plot. The plot shows 50 intervals; we can see that several include zero. These are wasted opporunities. We set out to reject a null hypothesis that the effect was less than or equal to zero, and we failed to do that.\n\n\nCode\n# number of studies to simulate\nn_studies &lt;- 50\n\n# a data frame of studies \nset.seed(123)\nests &lt;- tibble(study_id = 1:n_studies,\n               est = c(rnorm(n_studies, true_effect, se))) %&gt;%\n  mutate(reject_null = ifelse(est - 1.64*se &gt; 0, \"Yes\", \"No\")) \n\n# plot the confidence intervals for each study\ngg &lt;- ggplot(ests, aes(x = est, \n                         y = study_id, \n                         xmin = est - 1.64*se, \n                         xmax = est + 1.64*se, \n                         color = reject_null)) + \n  geom_vline(xintercept = true_effect, \n             linetype = \"dotted\") + \n  geom_vline(xintercept = 0) + \n  geom_point() + \n  geom_errorbarh(height = 0) + \n  geom_rug(sides = \"b\", \n           aes(x = est - 1.64*se, color = NULL), \n           alpha = 0.5, \n           length = unit(0.025, \"npc\")) + \n  scale_color_manual(values = c(\"No\" = \"#d95f02\", \"Yes\" = \"#1b9e77\")) +  # from https://colorbrewer2.org/#type=qualitative&scheme=Dark2&n=3\n  theme_bw() +\n    theme(panel.grid = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.title.y = element_blank()) + \n  labs(x = \"Estimate and 90% Confidence Interval\",\n       y = \"Study ID\", \n       color = \"Reject Null?\", \n       caption = \"Rug shows distribution of lower bound of confidence interval.\") \n\n# print plot\nprint(gg)"
  },
  {
    "objectID": "blog/2023-06-01-teaching-confidence-intervals-with-gganimate/index.html#dynamic-plot",
    "href": "blog/2023-06-01-teaching-confidence-intervals-with-gganimate/index.html#dynamic-plot",
    "title": "Teaching Confidence Intervals and Hypothesis Testing with gganimate",
    "section": "Dynamic Plot",
    "text": "Dynamic Plot\nThe static plot above is nice, but doesn’t convey an appropriate sense of randomness or “you don’t know what you’ll get this time.” To convey this feeling, I like to add dynamics. In particular, I like a confidence interval that’s moving and you’re not sure if it will cover zero or not. This conveys the sense that “something bad might happen” with each draw. Even though only about 10 of the 50 confidence intervals will cross zero, you feel the danger on all 50 simulations.\nIn designing this plot, two features are in tension:\n\nThe plot conveys the ideas.\nThe code is easy to update and understand.\n\nIn the past, I’ve prioritized making the plot look exactly like I want. But there are concrete downsides to using hacky solutions—using functions in ways not intended. The code is brittle and difficult. For examples that clearly convey the ideas, see Presidential Plinko and this “raindrop” plot. These are great ways to convey the randomness, but producing these plots requires some “tedious” coding.\nWith this code, I tried to illustrate the concept well while avoid hacky solutions to minor problems. This makes the code easier to understand, change, and update.\nFirst, let’s start by creating the data frame to plot. We need to make two small changes to the data frame above. These are both hacks, but worth it.\n\nAdd a dummy row to the data frame to trick gganimate into starting with an empty plot.\nAdd a grouping variable to indicate the states for the transitions. This is simply a row ID variable.\n\n\n\nCode\n# load packages\nlibrary(gganimate)\n\n# add two things to the data frame of confidence intervals\n# 1. an initial row with study_id = 1 and est = NA so that \n#    the plot starts empty (gganimate would start with the \n#    first observation in place otherwise).\n# 2. a group variable that defines the row. This is the same\n#    as the study_id, except the dummy row from (1) and the \n#    actual first row have different groups.\nanimate_data &lt;- bind_rows(\n  tibble(study_id = 1, est = NA),  # study_id = 1, est = NA\n  ests                             # combine dummy row with ests data frame from above\n  ) %&gt;%\n  mutate(group = 1:n())            # group (row index)\n\n\nNow let’s plot the confidence intervals much like above, except with expanded scales to give some more room for movement.\n\n\nCode\n# same ggplot, except three annotated changes\n1gg_exp &lt;- ggplot(animate_data, aes(x = est,\n                         y = study_id, \n                         xmin = est - 1.64*se, \n                         xmax = est + 1.64*se, \n                         color = reject_null, \n2                         group = group)) +\n  geom_vline(xintercept = true_effect, \n             linetype = \"dotted\") + \n  geom_vline(xintercept = 0) + \n  geom_point() + \n  geom_errorbarh(height = 0) + \n  geom_rug(sides = \"b\", \n           aes(x = est - 1.64*se, color = NULL), \n           alpha = 0.5, \n           length = unit(0.025, \"npc\")) + \n  scale_color_manual(values = c(\"No\" = \"#d95f02\", \"Yes\" = \"#1b9e77\")) +  \n  theme_bw() +\n    theme(panel.grid = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.title.y = element_blank()) + \n  labs(x = \"Estimate and 90% Confidence Interval\",\n       y = \"Study ID\", \n       color = \"Reject Null?\", \n       caption = \"Rug shows distribution of lower bound of confidence interval.\n                  Solid lines shows zero.\n                  Dotted line shows the true effect.\") + \n  # new scales here\n3  scale_x_continuous(expand = expansion(add = c(0.6, 1))) +\n  scale_y_continuous(expand = expansion(add = 2))\n  \n\n# print plot\nprint(gg_exp)\n\n\n\n1\n\nUse dataset with group variable.\n\n2\n\nSet group explicitly.\n\n3\n\nExpand x- and y-axis.\n\n\n\n\n\n\n\nNow let’s add the animation. I like the confidence intervals to be shooting toward zero. This gives the feeling that “it might cross!” and makes the fear of a failed study real for each simulation.\n\n\nCode\n# add dyamics to the plot\n1anim &lt;- gg_exp +\n2  transition_states(states = group) +\n  # how points enter\n3  enter_drift(x_mod = 2) +\n4  enter_grow() +\n5  enter_recolor(color = \"black\") +\n6  ease_aes(color = \"exponential-in\") +\n  # how points exit/remain\n7  exit_fade(alpha = 0.3) +\n8  shadow_mark(alpha = 0.3)\n\n# make magic happen!\nanimate(anim, duration = n_studies, fps = 10, \n        height = 6, width = 8, units = \"in\", res = 150)\n\n\n\n1\n\n‘gg’ is created earlier. It’s the plot we want to make dynamic.\n\n2\n\nThe transition_states() function from gganimate creates an animation transitioning between different states of the data. In this case, the argument states is set to group. This makes each group (each row in the data frame animate_data) appear in the plot, one at a time.\n\n3\n\nThe enter_drift() function describes how new data points enter the frame. x_mod = 2 makes new data enter by drifting along the x-axis 2 points from the right of their ending position.\n\n4\n\nThe enter_grow() function describes how new data points enter the frame. In this case, it makes them will grow from a size of 0 to their ending size.\n\n5\n\nThe enter_recolor() function again describes how new data points should enter the frame. Here, it makes the points and CIs change the color from black to their ending color as they appear. I want the final color to be a bit of a surprise, so I start them as black.\n\n6\n\nThe ease_aes() function determines how the aesthetics of the points change over the transitions. color = \"exponential-in\" means the color change will be at an exponential rate at the start of the transition. This makes the color change really fast at the end of the transition to maintain the surprise of the result.\n\n7\n\nThe exit_fade() function describes how data points leave the frame. alpha = 0.3 specifies that points will fade out to 30% transparency when they exit (when the next data point reaches its position).\n\n8\n\nshadow_mark() function keeps past data points in the frame. alpha = 0.3 sets the transparency of the shadow marks to 30% to match the exit_fade() transparency."
  },
  {
    "objectID": "blog/2023-06-01-teaching-confidence-intervals-with-gganimate/index.html#summary",
    "href": "blog/2023-06-01-teaching-confidence-intervals-with-gganimate/index.html#summary",
    "title": "Teaching Confidence Intervals and Hypothesis Testing with gganimate",
    "section": "Summary",
    "text": "Summary\nThe dynamic plot above is a good tool to help students understand that confidence intervals can quite easily include values on the wrong side of zero. Stated differently, it’s easy for a hypothesis test to fail to reject the null (when the null is wrong). Their intuition suggests that the test should tell you the correct answer.\nThe exercises above (appropriately) undermine their trust in hypothesis tests. I want them to feel the riskiness of poorly-powered experiments that consistently nestle confidence intervals right up against zero. I want them ready to work hard to avoid that risk—to make sure they have statistical power. Hopefully, they see that carefully building power into your experiment isn’t a task, it’s the task of experimental design.\nAs a concluding example, here’s the same dynamic plot for a study with about 98% power. Notice how “safe” this study feels compared to the one above with 80% power. I think this plot does a good job a translating probabilities into an appropriate sense of “danger.”\n\n\nCode\n# study parameters\nse &lt;- 0.27  # 1/3.64\n\n# identify effects of interest\neoi &lt;- tribble(\n  ~Effect, ~Description,\n  true_effect, \"True Effect (known in this exercise)\"\n) \n\n# compute quantities of interest regarding power\neoi %&gt;%\n  mutate(Power = 1 - pnorm(1.64*se, Effect, se),\n         Power = scales::percent(Power, accuracy = 1),\n         `Type S` = retrodesign::type_s(Effect, se)$type_s,\n         `Type S` = scales::number(`Type S`, accuracy = 0.01),\n         `Type M` = retrodesign::type_m(Effect, se)$type_m,\n         `Type M` = scales::number(`Type M`, accuracy = 0.01),\n         Effect = scales::number(Effect, accuracy = 0.01)) %&gt;% \n  pivot_longer(cols = Effect:`Type M`) %&gt;%\n  kableExtra::kable(format = \"markdown\", col.names = NULL)\n\n# a data frame of studies \nests &lt;- tibble(study_id = 1:n_studies,\n               est = c(rnorm(n_studies, true_effect, se))) %&gt;%\n  mutate(reject_null = ifelse(est - 1.64*se &gt; 0, \"Yes\", \"No\")) \n\nanimate_data &lt;- bind_rows(\n  tibble(study_id = 1, est = NA),  # study_id = 1, est = NA\n  ests                             # combine dummy row with ests data frame from above\n  ) %&gt;%\n  mutate(group = 1:n())            # group (row index)\n\n# same ggplot, except three annotated changes\ngg_exp &lt;- ggplot(animate_data, aes(x = est,\n                         y = study_id, \n                         xmin = est - 1.64*se, \n                         xmax = est + 1.64*se, \n                         color = reject_null, \n                         group = group)) +\n  geom_vline(xintercept = true_effect, \n             linetype = \"dotted\") + \n  geom_vline(xintercept = 0) + \n  geom_point() + \n  geom_errorbarh(height = 0) + \n  geom_rug(sides = \"b\", \n           aes(x = est - 1.64*se, color = NULL), \n           alpha = 0.5, \n           length = unit(0.025, \"npc\")) + \n  scale_color_manual(values = c(\"No\" = \"#d95f02\", \"Yes\" = \"#1b9e77\")) +  \n  theme_bw() +\n    theme(panel.grid = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.title.y = element_blank()) + \n  labs(x = \"Estimate and 90% Confidence Interval\",\n       y = \"Study ID\", \n       color = \"Reject Null?\", \n       caption = \"Rug shows distribution of lower bound of confidence interval.\n                  Solid line shows zero.\n                  Dotted line shows the true effect.\") + \n  # new scales here\n  scale_x_continuous(expand = expansion(add = c(0.6, 1))) +\n  scale_y_continuous(expand = expansion(add = 2))\n\n# add dyamics to the plot\nanim &lt;- gg_exp +\n  transition_states(states = group) +\n  # how points enter\n  enter_drift(x_mod = 2) +\n  enter_grow() +\n  enter_recolor(color = \"black\") +\n  ease_aes(color = \"exponential-in\") +\n  # how points exit/remain\n  exit_fade(alpha = 0.3) +\n  shadow_mark(alpha = 0.3)\n\n# make magic happen!\nanimate(anim, duration = n_studies, fps = 10, \n        height = 6, width = 8, units = \"in\", res = 150)\n\n\n\n\n\nEffect\n1.00\n\n\nDescription\nTrue Effect (known in this exercise)\n\n\nPower\n98%\n\n\nType S\n0.00\n\n\nType M\n1.02"
  },
  {
    "objectID": "blog/2023-08-11-benchmarking-firth/index.html",
    "href": "blog/2023-08-11-benchmarking-firth/index.html",
    "title": "Benchmarking Firth’s Logit: {brglm2} versus {logistf}",
    "section": "",
    "text": "I like Firth’s logistic regression model (Firth 1993). I talk about that in Rainey and McCaskey (2021) and this Twitter thread. Kosmidis and Firth (2021) offer an excellent, recent follow-up as well.\nI’ll refer you to the papers for a careful discussion of the benefits, but Firth’s penalty reduces the bias and variance of the logit coefficients."
  },
  {
    "objectID": "blog/2023-08-11-benchmarking-firth/index.html#firths-logit",
    "href": "blog/2023-08-11-benchmarking-firth/index.html#firths-logit",
    "title": "Benchmarking Firth’s Logit: {brglm2} versus {logistf}",
    "section": "",
    "text": "I like Firth’s logistic regression model (Firth 1993). I talk about that in Rainey and McCaskey (2021) and this Twitter thread. Kosmidis and Firth (2021) offer an excellent, recent follow-up as well.\nI’ll refer you to the papers for a careful discussion of the benefits, but Firth’s penalty reduces the bias and variance of the logit coefficients."
  },
  {
    "objectID": "blog/2023-08-11-benchmarking-firth/index.html#goals-for-benchmarking",
    "href": "blog/2023-08-11-benchmarking-firth/index.html#goals-for-benchmarking",
    "title": "Benchmarking Firth’s Logit: {brglm2} versus {logistf}",
    "section": "Goals for Benchmarking",
    "text": "Goals for Benchmarking\nIn this post, I want to compare the brglm2 and logistf packages. Which fits logistic regression models with Firth’s penalty the fastest?\nThese packages both fit the models almost instantly, so there is no practical difference when fitting just one model. But large Monte Carlo simulations (or perhaps bootstraps), small differences might add up to a substantial time difference.\nHere, I benchmark the two packages for fitting logistic regression models with Firth’s penalty in a small sample–the results might not generalize to a larger sample. The data set comes from Weisiger (2014) (see ?crdata::weisiger2014). It has only 35 observations.\nYou can find the benchmarking code as a GitHub Gist."
  },
  {
    "objectID": "blog/2023-08-11-benchmarking-firth/index.html#benchmarking",
    "href": "blog/2023-08-11-benchmarking-firth/index.html#benchmarking",
    "title": "Benchmarking Firth’s Logit: {brglm2} versus {logistf}",
    "section": "Benchmarking",
    "text": "Benchmarking\nI benchmark four methods here.\n\nA vanilla glm() logit model.\nA Firth’s logit via brglm2 by supplying method = brglm2::brglmFit to glm().\nA Firth’s logit via logistf via logistf() using the default settings.\nA Firth’s logit via logistf via logistf() with the argument pl = FALSE. This argument is important because it skips hypothesis testing using profile likelihoods, which are computationally costly.\n\n\n# install crdata package to egt weisiger2014 data set\nremotes::install_github(\"carlislerainey/crdata\")\n\n# load packages\nlibrary(tidyverse)\nlibrary(brglm2)\nlibrary(logistf)\nlibrary(microbenchmark)\n\n\n# load data\nweis &lt;- crdata::weisiger2014\n\n# rescale weisiger2014 explanatory variables using arm::rescale()\nrs_weis &lt;- weis %&gt;%\n  mutate(across(polity_conq:coord, arm::rescale)) \n\n# create functions to fit models\nf &lt;- resist ~ polity_conq + lndist + terrain + soldperterr + gdppc2 + coord\nf1 &lt;- function() {\n  glm(f, data = rs_weis, family = \"binomial\")\n}\nf2 &lt;- function() {\n  glm(f, data = rs_weis, family = \"binomial\", method = brglmFit)\n}\nf3 &lt;- function() {\n  logistf(f, data = rs_weis)\n}\nf4 &lt;- function() {\n  logistf(f, data = rs_weis, pl = FALSE)\n}\n\n# do benchmarking\nbm &lt;- microbenchmark(\"regular glm()\" = f1(), \n               \"brglm2\" = f2(), \n               \"logistf (default)\" = f3(),\n               \"logistf (w/ pl = FALSE)\" = f4(),\n               times = 100) \n\nWarning in microbenchmark(`regular glm()` = f1(), brglm2 = f2(), `logistf\n(default)` = f3(), : less accurate nanosecond times to avoid potential integer\noverflows\n\nprint(bm)\n\nUnit: microseconds\n                    expr      min        lq      mean    median        uq\n           regular glm()  500.651  547.0425  591.5304  560.0805  584.4550\n                  brglm2 2469.266 2546.9405 2735.6438 2591.5690 2672.3800\n       logistf (default) 3303.329 3432.6635 3942.9122 3481.6585 3558.1235\n logistf (w/ pl = FALSE)  513.197  553.3155  594.6205  570.8225  608.9115\n       max neval\n  2713.708   100\n  9735.983   100\n 17672.148   100\n  1698.343   100\n\n\nIn short, logistf is slower than brglm2, but only because it computes the profile likelihood p-values by default. Once we skip those calculations using pl = FALSE, logistf is much faster. On average, it’s faster than glm(), because glm() has the occasional really slow computation.\nHere’s a plot showing the computation times of the four fits. Remember that all of these are computed practically instantly, so it only makes a difference when the fits are done thousands of times, like in a Monte Carlo simulation.\n\n# plot times\nbm %&gt;%\n  group_by(expr) %&gt;%\n  summarize(avg_time = mean(time)*10e-5) %&gt;%  # convert to milliseconds\n  ggplot(aes(x = fct_rev(expr), y = avg_time)) + \n  geom_col() + \n  labs(x = \"Method\", \n       y = \"Avg. Time (in milliseconds)\") + \n  coord_flip()"
  },
  {
    "objectID": "blog/2023-08-11-benchmarking-firth/index.html#computer",
    "href": "blog/2023-08-11-benchmarking-firth/index.html#computer",
    "title": "Benchmarking Firth’s Logit: {brglm2} versus {logistf}",
    "section": "Computer",
    "text": "Computer\nHere’s the info on my machine.\n\nsystem(\"sysctl -n machdep.cpu.brand_string\", intern = TRUE)\n\n[1] \"Apple M2 Max\""
  },
  {
    "objectID": "blog/2023-08-18-equivalence-tests/index.html",
    "href": "blog/2023-08-18-equivalence-tests/index.html",
    "title": "Equivalence Tests Using {marginaleffects}",
    "section": "",
    "text": "First, a bit of background on the paper and the idea of hypothesizing that a variable “has no effect.”\nI remember sitting in a talk as a first-year graduate student, and the speaker said something like: “I expect no effect here, and, just as I expected, the difference is not statistically significant.” I was a little bit taken aback—of course, that’s not a compelling argument for a null effect. But I saw this approach taken again and again in published work.\nMy first publication was an AJPS article (Rainey 2014) explaining why this doesn’t work well and how to do it better.\nHere’s what I wrote in that paper:\n\nHypothesis testing is a powerful empirical argument not because it shows that the data are consistent with the research hypothesis, but because it shows that the data are inconsistent with other hypotheses (i.e., the null hypothesis). However, researchers sometimes reverse this logic when arguing for a negligible effect, showing only that the data are consistent with “no effect” and failing to show that the data are inconsistent with meaningful effects. When researchers argue that a variable has “no effect” because its confidence interval contains zero, they take no steps to rule out large, meaningful effects, making the empirical claim considerably less persuasive (Altman and Bland 1995; Gill 1999; Nickerson 2000).\n\nBut here’s a critical point, it’s impossible to reject every hypothesis except exactly no effect. Instead, the researcher must define a range of substantively “negligible” effects. The researcher can reject the null hypothesis that the effect falls outside this range of negligible effects. However, this requires a substantive judgement about those effects that are negligible and those that are not.\nHere’s what I wrote:\n\nResearchers who wish to argue for a negligible effect must precisely define the set of effects that are deemed “negligible” as well as the set of effects that are “meaningful.” This requires defining the smallest substantively meaningful effect, which I denote as \\(m\\). The definition must be debated by substantive scholars for any given context because the appropriate \\(m\\) varies widely across applications."
  },
  {
    "objectID": "blog/2023-08-18-equivalence-tests/index.html#background-on-arguing-for-a-negligible-effect",
    "href": "blog/2023-08-18-equivalence-tests/index.html#background-on-arguing-for-a-negligible-effect",
    "title": "Equivalence Tests Using {marginaleffects}",
    "section": "",
    "text": "First, a bit of background on the paper and the idea of hypothesizing that a variable “has no effect.”\nI remember sitting in a talk as a first-year graduate student, and the speaker said something like: “I expect no effect here, and, just as I expected, the difference is not statistically significant.” I was a little bit taken aback—of course, that’s not a compelling argument for a null effect. But I saw this approach taken again and again in published work.\nMy first publication was an AJPS article (Rainey 2014) explaining why this doesn’t work well and how to do it better.\nHere’s what I wrote in that paper:\n\nHypothesis testing is a powerful empirical argument not because it shows that the data are consistent with the research hypothesis, but because it shows that the data are inconsistent with other hypotheses (i.e., the null hypothesis). However, researchers sometimes reverse this logic when arguing for a negligible effect, showing only that the data are consistent with “no effect” and failing to show that the data are inconsistent with meaningful effects. When researchers argue that a variable has “no effect” because its confidence interval contains zero, they take no steps to rule out large, meaningful effects, making the empirical claim considerably less persuasive (Altman and Bland 1995; Gill 1999; Nickerson 2000).\n\nBut here’s a critical point, it’s impossible to reject every hypothesis except exactly no effect. Instead, the researcher must define a range of substantively “negligible” effects. The researcher can reject the null hypothesis that the effect falls outside this range of negligible effects. However, this requires a substantive judgement about those effects that are negligible and those that are not.\nHere’s what I wrote:\n\nResearchers who wish to argue for a negligible effect must precisely define the set of effects that are deemed “negligible” as well as the set of effects that are “meaningful.” This requires defining the smallest substantively meaningful effect, which I denote as \\(m\\). The definition must be debated by substantive scholars for any given context because the appropriate \\(m\\) varies widely across applications."
  },
  {
    "objectID": "blog/2023-08-18-equivalence-tests/index.html#clark-and-golder-2006",
    "href": "blog/2023-08-18-equivalence-tests/index.html#clark-and-golder-2006",
    "title": "Equivalence Tests Using {marginaleffects}",
    "section": "Clark and Golder (2006)",
    "text": "Clark and Golder (2006)\nClark and Golder (2006) offer a nice example of this sort of hypothesis. I’ll refer you there and to Rainey (2014) for a complete discussion of their idea, but I’ll motivate it briefly here.\nExplaining why a country might have only a few (i.e., two) parties, Clark and Golder write:\n\nFirst, it could be the case that the demand for parties is low because there are few social cleavages. In this situation, there would be few parties whether the electoral institutions were permissive or not. Second, it could be the case that the electoral system is not permissive. In this situation, there would be a small number of parties even if the demand for political parties were high. Only a polity characterized by both a high degree of social heterogeneity and a highly permissive electoral system is expected to produce a large number of parties. (p. 683)\n\nThus, they expect that electoral institutions won’t matter in socially homogenous systems. And they expect that social heterogeneity won’t matter in electoral systems that are not permissive."
  },
  {
    "objectID": "blog/2023-08-18-equivalence-tests/index.html#reproducing-clark-and-golder-2006",
    "href": "blog/2023-08-18-equivalence-tests/index.html#reproducing-clark-and-golder-2006",
    "title": "Equivalence Tests Using {marginaleffects}",
    "section": "Reproducing Clark and Golder (2006)",
    "text": "Reproducing Clark and Golder (2006)\nBefore computing their specific quantities of interest, let’s reproduce their regression model. Here’s their table that we’re trying to reproduce.\n\nAnd here’s a reproduction of their estimates using the cg2006 data from the {crdata} package on GitHub.11 Run ?crdata::cg2006 for detailed documentation of this data set.\n\n# load packages\nlibrary(sandwich)\nlibrary(modelsummary)\n\n# install my data packages from github\ndevtools::install_github(\"carlislerainey/crdata\")  # only updates if newer version available\n\n# load clark and golder's data set\ncg &lt;- crdata::cg2006\n\n# reproduce their estimates\nf &lt;- enep ~ eneg*log(average_magnitude) + eneg*upper_tier + en_pres*proximity\nfit &lt;- lm(f, data = cg)\n\n# cluster-robust standard errors\nSigma_hat &lt;- vcovCL(fit, cluster = ~ country, type = \"HC1\")\n\n# regression table\nmodelsummary(fit, vcov = Sigma_hat, fmt = 2, shape = term ~ model + statistic)\n\n\n\n\n\n\n\n\n\n\n\n(1)\n\n\n\n\nEst.\nS.E.\n\n\n\n\n(Intercept)\n2.92\n0.35\n\n\neneg\n0.11\n0.14\n\n\nlog(average_magnitude)\n0.08\n0.23\n\n\nupper_tier\n−0.06\n0.03\n\n\nen_pres\n0.26\n0.15\n\n\nproximity\n−3.10\n0.46\n\n\neneg × log(average_magnitude)\n0.26\n0.17\n\n\neneg × upper_tier\n0.06\n0.02\n\n\nen_pres × proximity\n0.68\n0.23\n\n\n\n\n\n\n\nSuccess!\nThey use averge_magnitude to measure the permissiveness of the electoral system and eneg to measure social heterogeneity."
  },
  {
    "objectID": "blog/2023-08-18-equivalence-tests/index.html#using-comparisons-to-compute-the-effects",
    "href": "blog/2023-08-18-equivalence-tests/index.html#using-comparisons-to-compute-the-effects",
    "title": "Equivalence Tests Using {marginaleffects}",
    "section": "Using comparisons() to compute the effects",
    "text": "Using comparisons() to compute the effects\nNow let’s compute the two quantities of interest. Clark and Golder argue for two negligible effects, which I make really concrete below.\n\nHypothesis 1 Increasing the effective number of ethnic groups from the 10th percentile (1.06) to the 90th percentile (2.48) will not lead to a substantively meaningful change in the effective number of political parties when the district magnitude is one.\nHypothesis 2 Increasing the district magnitude from one to seven will not lead to a substantively meaningful change in the effective number of political parties when the effective number of ethnic groups is one.\n\nAnd comparing the U.S. and the U.K., I argue that the smallest substantively interesting effect is 0.62. In Rainey (2014), I made the plot below. I want to reproduce it with {marginaleffects}.\n\nThese differences (and the 90% CIs) are really easy to compute using {marginaleffects}!22 I’m only doing Clark and Golder’s original results, not any of the robustness checks.\n\n# load packages\nlibrary(marginaleffects)\n\n# the smallest substantively interesting effect\nm &lt;- 0.62\n\n# a data frame setting the values of the \"other\" variables\nX_c &lt;- data.frame(\n  eneg = 1.06,  # low value\n  average_magnitude = 1,  # low value\n  upper_tier = 0,\n  en_pres = 0, \n  proximity = 0\n)\n\n# compute the comparison for eneg\neneg_comp &lt;- comparisons(fit,\n                         vcov = Sigma_hat,\n                         newdata = X_c, \n                         variables = list(\"eneg\" = c(1.06, 2.48)), # low to high value\n                         conf_level = 0.90)\n\n# compute the comparison for average magnitude\nmag_comp &lt;- comparisons(fit,\n                        vcov = Sigma_hat,\n                        newdata = X_c, \n                        variables = list(\"average_magnitude\" = c(1, 7)), # low to high value\n                        conf_level = 0.90)\n\nNow we can just plot the 90% CIs with ggplot() and check whether the entire interval falls inside the bounds.\n\n# load packages\nlibrary(tidyverse)\n\n# bind the comparisons together and plot\ncomp &lt;- bind_rows(eneg_comp, mag_comp)\nggplot(comp, aes(x = estimate,\n                 xmin = conf.low,\n                 xmax = conf.high, \n                 y = term)) + \n  geom_vline(xintercept = c(-m, m), linetype = \"dashed\") + \n  geom_errorbarh() + \n  geom_point() \n\n\n\n\nIn this case, we conclude that social heterogeneity (eneg) has a negligible effect because the 90% CI only contains substantively negligible values. However, the 90% CI for district magnitude (average_magnitude) contains substantively negligible and meaningful values, so we cannot reject the null hypothesis of a meaningful effect."
  },
  {
    "objectID": "blog/2023-08-18-equivalence-tests/index.html#computing-the-tost-p-values-using-hypotheses",
    "href": "blog/2023-08-18-equivalence-tests/index.html#computing-the-tost-p-values-using-hypotheses",
    "title": "Equivalence Tests Using {marginaleffects}",
    "section": "Computing the TOST p-values using hypotheses()",
    "text": "Computing the TOST p-values using hypotheses()\nIt’s then almost trivial to use the hypotheses() function to compute the TOST p-values.33 Note this warning from ?hypotheses(): Warning #2: For hypothesis tests on objects produced by the marginaleffects package, it is safer to use the hypothesis argument of the original function. Using hypotheses() may not work in certain environments, in lists, or when working programmatically with apply style functions.\n\n# hypothesis tests\nhypotheses(eneg_comp, equivalence = c(-m, m))\n\n\n Term    Contrast Estimate Std. Error    z Pr(&gt;|z|)   S   2.5 % 97.5 %\n eneg 2.48 - 1.06    0.158      0.101 1.56    0.118 3.1 -0.0402  0.357\n p (NonSup) p (NonInf) p (Equiv) eneg average_magnitude upper_tier en_pres\n     &lt;0.001     &lt;0.001    &lt;0.001 1.06                 1          0       0\n proximity\n         0\n\nColumns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, eneg, average_magnitude, upper_tier, en_pres, proximity, enep, statistic.noninf, statistic.nonsup, p.value.noninf, p.value.nonsup, p.value.equiv \n\nhypotheses(mag_comp, equivalence = c(-m, m))\n\n\n              Term Contrast Estimate Std. Error    z Pr(&gt;|z|)    S 2.5 % 97.5 %\n average_magnitude    7 - 1    0.696      0.113 6.16   &lt;0.001 30.3 0.474  0.917\n p (NonSup) p (NonInf) p (Equiv) eneg average_magnitude upper_tier en_pres\n      0.748     &lt;0.001     0.748 1.06                 1          0       0\n proximity\n         0\n\nColumns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted_lo, predicted_hi, predicted, eneg, average_magnitude, upper_tier, en_pres, proximity, enep, statistic.noninf, statistic.nonsup, p.value.noninf, p.value.nonsup, p.value.equiv \n\n\nChecking that the 90% CIs fall within the bounds created by the smallest substantively-meaningful effect is equivalent to checking whether the TOST p-value (i.e., the p(Equiv) column) is less than 0.05, so our conclusions are (and must be) identical."
  },
  {
    "objectID": "blog/2023-08-18-equivalence-tests/index.html#final-thoughts",
    "href": "blog/2023-08-18-equivalence-tests/index.html#final-thoughts",
    "title": "Equivalence Tests Using {marginaleffects}",
    "section": "Final thoughts",
    "text": "Final thoughts\n\n{marginaleffects} is a great package. I think it’s the first package in which the syntax matches the way I think about computing quantities of interest. That said, this is just my first try at it. But I’m very impressed so far.\nThe {marginaleffects} book has a whole chapter on equivalence tests. My only caution is that there is a mismatch between 95% confidence intervals and equivalence tests. By default, {marginaleffects} reports a 95% CI, even when producing a p-value for an equivalence test. However, the 90% confidence interval correspondents to a size-5% equivalence test. So if you’re using {marginaleffects} to do equivalence tests, I recommend setting conf_level = 0.90.4\nFor a more recent example, Jake Jares and Neil Malhotra have a new paper that discusses negligible effects and hypothesis tests in a way that I find clear and compelling. It’s an excellent model to follow. See pp. 26-31. They “show that improved compensation outcomes had negligible impacts on Republican farmers’ midterm turnout and campaign contributions, even though such variation in benefits significantly affected whether farmers viewed the intervention as helpful.”\n\n4 I would make a similar point about one-sided tests as well, but that’s less correct, because it should be a one-sided 95% CI."
  },
  {
    "objectID": "blog/2023-08-18-equivalence-tests/index.html#complete-code",
    "href": "blog/2023-08-18-equivalence-tests/index.html#complete-code",
    "title": "Equivalence Tests Using {marginaleffects}",
    "section": "Complete code",
    "text": "Complete code\nTo make this easy to reproduce, here’s the complete code:\n\n# load packages\nlibrary(tidyverse)\nlibrary(sandwich)\nlibrary(modelsummary)\nlibrary(marginaleffects)\n\n# install my data packages from github\ndevtools::install_github(\"carlislerainey/crdata\")  # only updates if newer version available\n\n# load clark and golder's data set\ncg &lt;- crdata::cg2006\n\n# reproduce their estimates\nf &lt;- enep ~ eneg*log(average_magnitude) + eneg*upper_tier + en_pres*proximity\nfit &lt;- lm(f, data = cg)\n\n# cluster-robust standard errors\nSigma_hat &lt;- vcovCL(fit, cluster = ~ country, type = \"HC1\")\n\n# regression table\nmodelsummary(fit, vcov = Sigma_hat, fmt = 2, shape = term ~ model + statistic)\n\n# the smallest substantively interesting effect\nm &lt;- 0.62\n\n# a data frame setting the values of the \"other\" variables\nX_c &lt;- data.frame(\n  eneg = 1.06,  # low value\n  average_magnitude = 1,  # low value\n  upper_tier = 0,\n  en_pres = 0, \n  proximity = 0\n)\n\n# compute the comparison for eneg\neneg_comp &lt;- comparisons(fit,\n                         vcov = Sigma_hat,\n                         newdata = X_c, \n                         variables = list(\"eneg\" = c(1.06, 2.48)), # low to high value\n                         conf_level = 0.90)\n\n# compute the comparison for average magnitude\nmag_comp&lt;- comparisons(fit,\n                       vcov = Sigma_hat,\n                       newdata = X_c, \n                       variables = list(\"average_magnitude\" = c(1, 7)), # low to high value\n                       conf_level = 0.90)\n\n# bind the comparisons together and plot\ncomp &lt;- bind_rows(eneg_comp, mag_comp)\nggplot(comp, aes(x = estimate,\n                 xmin = conf.low,\n                 xmax = conf.high, \n                 y = term)) + \n  geom_vline(xintercept = c(-m, m), linetype = \"dashed\") + \n  geom_errorbarh() + \n  geom_point() \n\n# hypothesis tests (TOSTs)\nhypotheses(eneg_comp, equivalence = c(-m, m))\nhypotheses(mag_comp, equivalence = c(-m, m))"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\nAug 30, 2023\n\n\nFirth’s Logit: Some References\n\n\n3 min\n\n\n\n\nAug 18, 2023\n\n\nEquivalence Tests Using {marginaleffects}\n\n\n11 min\n\n\n\n\nAug 15, 2023\n\n\nDaily Writing\n\n\n7 min\n\n\n\n\nAug 11, 2023\n\n\nBenchmarking Firth’s Logit: {brglm2} versus {logistf}\n\n\n4 min\n\n\n\n\nJun 12, 2023\n\n\nPower, Part III: The Rule of 3.64 for Statistical Power\n\n\n24 min\n\n\n\n\nJun 1, 2023\n\n\nTeaching Confidence Intervals and Hypothesis Testing with gganimate\n\n\n20 min\n\n\n\n\nMay 25, 2023\n\n\nPower, Part II: What Do Confidence Intervals from Well-Powered Studies Look Like?\n\n\n9 min\n\n\n\n\nMay 21, 2023\n\n\nPower, Part I: Power Is for You, Not for Reviewer Two\n\n\n8 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Carlisle Rainey",
    "section": "",
    "text": "email\n  \n  \n    \n     CV\n  \n\n  \n  \nI’m an Associate Professor in the Department of Political Science at Florida State University. My research has appeared in the American Political Science Review, the American Journal of Political Science, Political Analysis, and other peer-reviewed journals. I teach courses in American politics, comparative politics, and political methodology.\n\nFollow My Research\nIf want to stay up-to-date on my work, follow me on Google Scholar.\n\n\nConnect in Other Places\nI’m on Twitter, Bluesky, and Mastodon. I publicly version-control some of research projects and courses on GitHub. I upload my talks to Speaker Deck. I’m on Google Scholar, ORCID and OSF."
  },
  {
    "objectID": "talks/2023-topic-sampling/index.html",
    "href": "talks/2023-topic-sampling/index.html",
    "title": "Topic Sampling @ EPOVB 2023",
    "section": "",
    "text": "The two papers are below.\n\nClifford, Scott, Thomas Leeper, and Carlisle Rainey. “Generalizing Survey Experiments Using Topic Sampling: An Application to Party Cues.” Forthcoming in Political Behavior. [Journal] [Ungated]\nClifford, Scott and Carlisle Rainey. “Estimators for Topic-Sampling Designs.” [Preprint]\n\nSlides, with transitions [Dropbox] and without [Dropbox]"
  },
  {
    "objectID": "talks/2024-topic-sampling/index.html",
    "href": "talks/2024-topic-sampling/index.html",
    "title": "Topic Sampling @ SPSA 2024",
    "section": "",
    "text": "The three relevant papers are below.\n\nClifford, Scott, Thomas Leeper, and Carlisle Rainey. “Generalizing Survey Experiments Using Topic Sampling: An Application to Party Cues.” Forthcoming in Political Behavior. [Journal] [Ungated]\nClifford, Scott and Carlisle Rainey. “Estimators for Topic-Sampling Designs.” [Preprint]\nClifford, Scott and Carlisle Rainey. “The Limits of Single-Topic Experiments.” [Preprint]\n\nSlides, with transitions [Dropbox] and without [Dropbox]"
  },
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Teaching",
    "section": "",
    "text": "Advanced Quantitative Methods"
  },
  {
    "objectID": "teaching/index.html#current-classes",
    "href": "teaching/index.html#current-classes",
    "title": "Teaching",
    "section": "",
    "text": "Advanced Quantitative Methods"
  },
  {
    "objectID": "teaching/index.html#archived-classes",
    "href": "teaching/index.html#archived-classes",
    "title": "Teaching",
    "section": "Archived Classes",
    "text": "Archived Classes\n\nPOS 3713: Introduction to Political Science Research Methods\nPOS 5737: Introduction to Data Analysis\nPOLS 209 at TAMU: Research Methods"
  }
]