---
title: "Firth's Logit: Some References"
author: "Carlisle Rainey"
date: "2023-08-30"
categories: [methodology, logistic regression, small samples, Firth]
description: "In this note, I collect some older and more recent work on Firth's penalized maximum likelihood estimator."
toc: false
code-annotations: hover
reference-location: margin
website:
  twitter-card:
    image: "/profile-tw.png"
    card-style: "summary"
draft: false
bibliography: references.bib
---

In @rainey2021, Kelly McCaskey and I offer a accessible and practical (re)introduction to Firth's penalized maximum likelihood estimator that (1) corrects the small sample bias and (2) reduces the excessive variance of the usual maximum likelihood estimator.

Below, I bookmark other references that might be helpful.

::: {.callout-note appearance="minimal"}
I'm sure there are embarrassing omissions. If you see an omission, please let me know (self-promotion is encouraged, especially not-yet-published papers).
:::

[This Stack Exchange answer](https://stats.stackexchange.com/a/88743) gives a brief, but careful explanation of Firth's logit. If you're looking for a quick explanation, start here.

### The Two Main Papers

1. @firth1993 originally introduced the idea. Kelly and I draw mostly on this paper---it's a wonderful paper. 
1. @kosmidis2021 follow-up with additional theoretical results that are relevant for the estimator as used in practice since 1993. This happened to come out while our paper was working its way through the publication process. Most importantly, they discuss the shrinkage property of the estimator, which is what Kelly and I highlight as under-appreciated (and really important!).

From my perpective, these are the two main papers to refer to if you're concerned about small sample bias in logistic regression models.

### Extensions

Beyond these two main papers, there have been a few extensions. @zietkiewicz2023 talk about Firth's logit in *very* large data sets. @cook2018 make a good argument for using Firth's estimator in panel data sets with binary outcomes and fixed effects. @sterzinger2023 apply these ideas to mixed models (or random effects models). @sinkovec2021 compare Firth's approach to ridge regression, and suggest that Firth's is superior in small or sparse data sets. @puhr2017 study Firth's logit in the context of rare events and propose FLIC and FLAC as alternatives.

### Applications

- @r√∂ver2022 offer an application of Firth's logit to clinical trials.
- @turner2012 offer an application to Bradley-Terry models with the {BradleyTerry2} R package.

### Separation and Finiteness

I learned about Firth's estimator from @zorn2005, who follows @heinze2002 in suggesting it as a solution to separation. According to David Firth in [this blog post](https://davidfirth.github.io/blog/2023/01/05/f93-citations-and-history/), this is the application that stimulated interest in the approach after it went relatively unnoticed for a few years. *(Great post, I highly recommend reading it!)* This application piqued my interest in Firth's estimator. Briefly, I think Firth's default penalty might not be substantively reasonable in a given application [@rainey2016] (see also @beiser-mcgrath2020) and the usual likelihood ratio and score tests work well without the penalty [@rainey2023].

::: {.callout-note appearance="minimal"}
For more on Firth's logit, see [Ioannis Kosmidis](https://www.ikosmidis.com/research/)' research page and Georg Heinz [Google Scholar page](https://scholar.google.com/citations?user=HhfU2UQAAAAJ).
:::
