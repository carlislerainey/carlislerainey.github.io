[
  {
    "objectID": "teaching/pols-209/index.html",
    "href": "teaching/pols-209/index.html",
    "title": "POLS 209",
    "section": "",
    "text": "syllabus [pdf]\nassigned exercises from FPP [pdf]\ndata sets [zip]\n\nWriting Assignment 1 [Dropbox]\n\nRubric [Dropbox]\n\nWriting Assignment 2 [Dropbox] [checklist]\n\n\n\nAllen 2051, MW, 11am-12pm. Please reserve your slot here.\n\n\n\nTasks (bullets) below the date should be completed before the next class (unless a due-date is listed).\nAugust 30: Introduction\n. Read the syllabus carefully.\n. Install R and RStudio (complete by Sep 6).\n. Order textbook (have by Sep 11).\n. Obtain a pocket calculator (have by Sep 11).\nSep 1: Questions\n. Review Notes on Questions [pdf]. Complete exercises.\nSep 4: Models\n. Review Notes on Models [pdf] (and slides from lecture [pdf]). Complete exercises.\n. Install R and RStudio.\nSep 6: Model-Building Exercise\nSep 8: Computing in R\nSep 11: Computing in R, part 2\n. Read Notes on Computing in R [pdf]. Complete exercises.\nSep 13: Loading Data in R . Review your notes from my Lecture on Data Frames [pdf].\n. Review Notes on Loading Data in R [pdf]. Complete exercises.\n. Complete Computing Assignment 1 [pdf].\nSep 15: Causal Inference and Histograms\n. Have textbook and calculator.\n. Read chs. 1-2 of FPP. Complete assigned exercises (remember that assigned exercises are [at the top]).\n. Review your notes from my Lecture on Causal Inference [pdf]. Complete exercises at the end.\n. Read ch. 3 of FPP. Complete assigned exercises.\nSep 18: Histograms in R\n. Review Notes on Histograms in R [pdf]. Complete exercises.\n. Begin to work on Computing Assignment 2 [pdf] (due Sep 22).\nSep 20: Average and SD\n. Read ch. 4 of FPP. Complete assigned exercises.\n. Submit Computing Assignment 2 [pdf].\nSep 22: Catch-Up Day\n. Begin working on Writing Assignment 1 [Dropbox]. Come prepared with questions.\nSep 25: Average and SD in R\n. Review your notes from my Lecture on Average and SD in R [pdf].\n. Review Notes on Average and SD in R [pdf]. Complete exercises.\n. Begin working on Computing Assignment 3 [pdf].\nSep 27: Normal Approximation\n. Read ch. 5 of FPP. Complete assigned exercises.\n. Finish the leadership extremity exercise we began in class [pdf].\n. Submit Computing Assignment 3 [pdf].\nSep 29: Measurement\n. Read ch. 6 of FPP. Complete assigned exercises.\n. Prepare for Exam 1. Focus on the review exercises from notes, slides, and textbook.\nOct 2: Review for Exam 1\n. Study Guide [Google Doc]\nOct 4: Exam 1 (bring pencil, pocket calculator, and small green Scantron)\n. Read “Politics and the English Language” [pdf]. Expect a reading quiz.\nOct 6: Discussion of “Politics and the English Language”Scatterplots and Correlation, Part 1\n. Read chs. 7-8 of FPP. Complete assigned exercises.\n. See this sheet for p. 137, #9(a) [Google Sheet].\nOct 9: Measurement, Part 2\n. Review your notes from my Lecture on Measurement [pdf]. Complete exercises at the end.\nOct 11: Scatterplots and Correlation in R\n. Review Notes on Scatterplots and Correlation in R [pdf]. Complete exercises.\n. Read ch. 9 of FPP. Complete assigned exercises.\n. Play this game [web] and track your performance.\n. Don’t forget to submit Writing Assignment 1.\nOct 13: Regression, Part 1\n. Read ch. 10 of FPP. Complete assigned exercises.\nOct 16: Regression, Part 2\n. Read ch. 11 of FPP. Complete assigned exercises.\n. Note that I accidentally assigned ch. 11 on the 13th as well. I meant to assign ch. 10. Make sure you’ve finished both ch. 10 and 11.\n. Begin Computing Assignment 4 [pdf].\nOct 18: Regression, Part 3\n. Read ch. 12 of FPP. Complete assigned exercises.\n. Submit Computing Assignment 4 [pdf].\n. Submit peer review for Writing Assignment 1. Details on eCampus.\nOct 20: Regression in R\n. Review Notes on Regression in R [pdf]. Complete exercises.\nOct 23: Multiple Regression, Part 1\nOct 25: Multiple Regression, Part 2\n. Review your notes on my lecture on econometric notation. Make sure you can explain the similarities and differences between FPP’s simple notation and the more complicated econometric notation. What are the two advantages of econometric notation?\n. Review your notes on my lecture [pdf] on regression for prediction.\n. Read these notes [pdf] for more detail on prediction and BIC.\n. Complete Computing Assignment 5 [pdf].\n. Read “5 Steps toward Constructing a Better Sentence” [web].\n. Read “5 Steps toward Writing an Effective Paragraph” [web].\n. Use this example response memo [pdf] when writing your own response memo.\nOct 27: “Breakfast with Ben”\nOct 30: Exam 2 Review\n. Study Guide [Google Doc]\nNov 1: Exam 2 (bring pencil, pocket calculator, and small green Scantron)\n. Final submission of Writing Assignment 1.\nNov 3: Probability, Part 1\n. We’ll look at the Federalist papers [web] in class.\n. Read ch. 13 of FPP. Complete assigned exercises.\nNov 6: Probability, Part 2\n. Read ch. 14 of FPP. Complete assigned exercises.\n. Submit peer review by noon on July 26.\nNov 8: Law of Averages\n. Read ch. 16 of FPP. Complete assigned exercises.\nNov 10: Expected Value and Standard Error\n. Read ch. 17 of FPP. Complete assigned exercises.\n. Begin Writing Assignment 2 [Dropbox]\nNov 13: Normal Approximation for Probability Histograms\n. Read ch. 18 of FPP. Complete assigned exercises. . In class, fill in this table [Google Sheet].\n. In class, use these slides as needed [Google Slides].\nNov 15: Sample Surveys, Part 1\n. Read ch. 19 of FPP. Complete assigned exercises.\nNov 17: Sample Surveys, Part 2\n. Read ch. 20 of FPP. Complete assigned exercises.\nNov 20: FSAB Panel Day\n. Catch-up on any review exercises you haven’t done.\n. Writing Assignment 2 due (postponed to Tuesday, Nov. 28).\nNov 22: No Class (Reading Day)\nNov 24: No Class (Thanksgiving Holiday)\nNov 27: Catch-up Day\n. Make sure you’ve read through ch. 20 of FPP and completed assigned exercises.\nNov 29: The Accuracy of Percentages\n. Read ch. 21 of FPP. Complete assigned exercises.\nDec 1: The Accuracy of Averages\n. Read ch. 23 of FPP. Complete assigned exercises.\nDec 4: Hypothesis Tests . Read ch. 26 of FPP. Complete assigned exercises.\nDec 6: Final Exam Review\n. Example problem for hypothesis test and 95% CI for percent [Dropbox].\n. Final submission of Writing Assignment 2 due.\n. Study Guide [Google Doc]\nDec 8 or 11: Final Exam (bring pencil, pocket calculator, and small green Scantron) . For 901 (8:35-9:25am), 10am-12pm on Dec 8\n. For 902 (9:45-10:35am), 8-10am on Dec 11"
  },
  {
    "objectID": "teaching/pols-209/index.html#office-hours",
    "href": "teaching/pols-209/index.html#office-hours",
    "title": "POLS 209",
    "section": "",
    "text": "Allen 2051, MW, 11am-12pm. Please reserve your slot here."
  },
  {
    "objectID": "teaching/pols-209/index.html#schedule",
    "href": "teaching/pols-209/index.html#schedule",
    "title": "POLS 209",
    "section": "",
    "text": "Tasks (bullets) below the date should be completed before the next class (unless a due-date is listed).\nAugust 30: Introduction\n. Read the syllabus carefully.\n. Install R and RStudio (complete by Sep 6).\n. Order textbook (have by Sep 11).\n. Obtain a pocket calculator (have by Sep 11).\nSep 1: Questions\n. Review Notes on Questions [pdf]. Complete exercises.\nSep 4: Models\n. Review Notes on Models [pdf] (and slides from lecture [pdf]). Complete exercises.\n. Install R and RStudio.\nSep 6: Model-Building Exercise\nSep 8: Computing in R\nSep 11: Computing in R, part 2\n. Read Notes on Computing in R [pdf]. Complete exercises.\nSep 13: Loading Data in R . Review your notes from my Lecture on Data Frames [pdf].\n. Review Notes on Loading Data in R [pdf]. Complete exercises.\n. Complete Computing Assignment 1 [pdf].\nSep 15: Causal Inference and Histograms\n. Have textbook and calculator.\n. Read chs. 1-2 of FPP. Complete assigned exercises (remember that assigned exercises are [at the top]).\n. Review your notes from my Lecture on Causal Inference [pdf]. Complete exercises at the end.\n. Read ch. 3 of FPP. Complete assigned exercises.\nSep 18: Histograms in R\n. Review Notes on Histograms in R [pdf]. Complete exercises.\n. Begin to work on Computing Assignment 2 [pdf] (due Sep 22).\nSep 20: Average and SD\n. Read ch. 4 of FPP. Complete assigned exercises.\n. Submit Computing Assignment 2 [pdf].\nSep 22: Catch-Up Day\n. Begin working on Writing Assignment 1 [Dropbox]. Come prepared with questions.\nSep 25: Average and SD in R\n. Review your notes from my Lecture on Average and SD in R [pdf].\n. Review Notes on Average and SD in R [pdf]. Complete exercises.\n. Begin working on Computing Assignment 3 [pdf].\nSep 27: Normal Approximation\n. Read ch. 5 of FPP. Complete assigned exercises.\n. Finish the leadership extremity exercise we began in class [pdf].\n. Submit Computing Assignment 3 [pdf].\nSep 29: Measurement\n. Read ch. 6 of FPP. Complete assigned exercises.\n. Prepare for Exam 1. Focus on the review exercises from notes, slides, and textbook.\nOct 2: Review for Exam 1\n. Study Guide [Google Doc]\nOct 4: Exam 1 (bring pencil, pocket calculator, and small green Scantron)\n. Read “Politics and the English Language” [pdf]. Expect a reading quiz.\nOct 6: Discussion of “Politics and the English Language”Scatterplots and Correlation, Part 1\n. Read chs. 7-8 of FPP. Complete assigned exercises.\n. See this sheet for p. 137, #9(a) [Google Sheet].\nOct 9: Measurement, Part 2\n. Review your notes from my Lecture on Measurement [pdf]. Complete exercises at the end.\nOct 11: Scatterplots and Correlation in R\n. Review Notes on Scatterplots and Correlation in R [pdf]. Complete exercises.\n. Read ch. 9 of FPP. Complete assigned exercises.\n. Play this game [web] and track your performance.\n. Don’t forget to submit Writing Assignment 1.\nOct 13: Regression, Part 1\n. Read ch. 10 of FPP. Complete assigned exercises.\nOct 16: Regression, Part 2\n. Read ch. 11 of FPP. Complete assigned exercises.\n. Note that I accidentally assigned ch. 11 on the 13th as well. I meant to assign ch. 10. Make sure you’ve finished both ch. 10 and 11.\n. Begin Computing Assignment 4 [pdf].\nOct 18: Regression, Part 3\n. Read ch. 12 of FPP. Complete assigned exercises.\n. Submit Computing Assignment 4 [pdf].\n. Submit peer review for Writing Assignment 1. Details on eCampus.\nOct 20: Regression in R\n. Review Notes on Regression in R [pdf]. Complete exercises.\nOct 23: Multiple Regression, Part 1\nOct 25: Multiple Regression, Part 2\n. Review your notes on my lecture on econometric notation. Make sure you can explain the similarities and differences between FPP’s simple notation and the more complicated econometric notation. What are the two advantages of econometric notation?\n. Review your notes on my lecture [pdf] on regression for prediction.\n. Read these notes [pdf] for more detail on prediction and BIC.\n. Complete Computing Assignment 5 [pdf].\n. Read “5 Steps toward Constructing a Better Sentence” [web].\n. Read “5 Steps toward Writing an Effective Paragraph” [web].\n. Use this example response memo [pdf] when writing your own response memo.\nOct 27: “Breakfast with Ben”\nOct 30: Exam 2 Review\n. Study Guide [Google Doc]\nNov 1: Exam 2 (bring pencil, pocket calculator, and small green Scantron)\n. Final submission of Writing Assignment 1.\nNov 3: Probability, Part 1\n. We’ll look at the Federalist papers [web] in class.\n. Read ch. 13 of FPP. Complete assigned exercises.\nNov 6: Probability, Part 2\n. Read ch. 14 of FPP. Complete assigned exercises.\n. Submit peer review by noon on July 26.\nNov 8: Law of Averages\n. Read ch. 16 of FPP. Complete assigned exercises.\nNov 10: Expected Value and Standard Error\n. Read ch. 17 of FPP. Complete assigned exercises.\n. Begin Writing Assignment 2 [Dropbox]\nNov 13: Normal Approximation for Probability Histograms\n. Read ch. 18 of FPP. Complete assigned exercises. . In class, fill in this table [Google Sheet].\n. In class, use these slides as needed [Google Slides].\nNov 15: Sample Surveys, Part 1\n. Read ch. 19 of FPP. Complete assigned exercises.\nNov 17: Sample Surveys, Part 2\n. Read ch. 20 of FPP. Complete assigned exercises.\nNov 20: FSAB Panel Day\n. Catch-up on any review exercises you haven’t done.\n. Writing Assignment 2 due (postponed to Tuesday, Nov. 28).\nNov 22: No Class (Reading Day)\nNov 24: No Class (Thanksgiving Holiday)\nNov 27: Catch-up Day\n. Make sure you’ve read through ch. 20 of FPP and completed assigned exercises.\nNov 29: The Accuracy of Percentages\n. Read ch. 21 of FPP. Complete assigned exercises.\nDec 1: The Accuracy of Averages\n. Read ch. 23 of FPP. Complete assigned exercises.\nDec 4: Hypothesis Tests . Read ch. 26 of FPP. Complete assigned exercises.\nDec 6: Final Exam Review\n. Example problem for hypothesis test and 95% CI for percent [Dropbox].\n. Final submission of Writing Assignment 2 due.\n. Study Guide [Google Doc]\nDec 8 or 11: Final Exam (bring pencil, pocket calculator, and small green Scantron) . For 901 (8:35-9:25am), 10am-12pm on Dec 8\n. For 902 (9:45-10:35am), 8-10am on Dec 11"
  },
  {
    "objectID": "talks/index.html",
    "href": "talks/index.html",
    "title": "Talks",
    "section": "",
    "text": "Power Rules @ SPSA\n\n\nA talk at the 2025 SPSA Annual Conference.\n\n\n\nCarlisle Rainey\n\n\nJan 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPower Rules @ UGA\n\n\nA talk at the University of Georgia.\n\n\n\nCarlisle Rainey\n\n\nOct 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDissent Scores @ Connected_Politics\n\n\nA talk at the March 6, 2024, meeting of the Connected_Politics Lab.\n\n\n\nCarlisle Rainey\n\n\nMar 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTopic Sampling @ EPOVB 2024\n\n\nA talk at the 2024 meeting of the Election, Public Opinion, and Voting Behavior section of APSA.\n\n\n\nCarlisle Rainey\n\n\nMar 2, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTopic Sampling @ SPSA 2024\n\n\nA talk at the 2024 meeting of the Southern Political Science Association.\n\n\n\nCarlisle Rainey\n\n\nJan 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTopic Sampling @ EPOVB 2023\n\n\nA talk at the 2023 meeting of the Election, Public Opinion, and Voting Behavior section of APSA.\n\n\n\nCarlisle Rainey\n\n\nMar 4, 2023\n\n\n\n\n\n\n\n\nNo matching items\n\n\nFor older talks, see Speaker Deck."
  },
  {
    "objectID": "talks/2024-topic-sampling/index.html",
    "href": "talks/2024-topic-sampling/index.html",
    "title": "Topic Sampling @ SPSA 2024",
    "section": "",
    "text": "The three relevant papers are below.\n\nClifford, Scott, Thomas Leeper, and Carlisle Rainey. “Generalizing Survey Experiments Using Topic Sampling: An Application to Party Cues.” Forthcoming in Political Behavior. [Journal] [Ungated]\nClifford, Scott and Carlisle Rainey. “Estimators for Topic-Sampling Designs.” [Preprint]\nClifford, Scott and Carlisle Rainey. “The Limits of Single-Topic Experiments.” [Preprint]\n\nSlides, with transitions [Dropbox] and without [Dropbox]"
  },
  {
    "objectID": "talks/2024-03-06-dissent-scores/index.html",
    "href": "talks/2024-03-06-dissent-scores/index.html",
    "title": "Dissent Scores @ Connected_Politics",
    "section": "",
    "text": "slides\npreprint\nproject website\ndata set"
  },
  {
    "objectID": "talks/2024-03-06-dissent-scores/index.html#most-relevant-links",
    "href": "talks/2024-03-06-dissent-scores/index.html#most-relevant-links",
    "title": "Dissent Scores @ Connected_Politics",
    "section": "",
    "text": "slides\npreprint\nproject website\ndata set"
  },
  {
    "objectID": "talks/2024-03-06-dissent-scores/index.html#code-to-download-data",
    "href": "talks/2024-03-06-dissent-scores/index.html#code-to-download-data",
    "title": "Dissent Scores @ Connected_Politics",
    "section": "Code to download data",
    "text": "Code to download data\n\n# load packages\nlibrary(tidyverse)\nlibrary(dataverse)\n\n# get the dissent score data set from dataverse\ndissent &lt;- get_dataframe_by_name(\n  filename = \"dissent-scores.tab\",\n  dataset  = \"doi:10.7910/DVN/CL4CA8\",\n  server   = \"dataverse.harvard.edu\", \n  original = TRUE, \n  .f = readr::read_csv) |&gt; \n  glimpse()"
  },
  {
    "objectID": "talks/2024-03-06-dissent-scores/index.html#other-links",
    "href": "talks/2024-03-06-dissent-scores/index.html#other-links",
    "title": "Dissent Scores @ Connected_Politics",
    "section": "Other links",
    "text": "Other links\n\ngamson.csv is available here, via the data sets for my undergraduate research methods class.\n“Data and Code Availability in Political Science Publications from 1995 to 2022” [DOI]\n“The Data Availability Policies of Political Science Journals” [DOI]"
  },
  {
    "objectID": "talks/2023-topic-sampling/index.html",
    "href": "talks/2023-topic-sampling/index.html",
    "title": "Topic Sampling @ EPOVB 2023",
    "section": "",
    "text": "The two papers are below.\n\nClifford, Scott, Thomas Leeper, and Carlisle Rainey. “Generalizing Survey Experiments Using Topic Sampling: An Application to Party Cues.” Forthcoming in Political Behavior. [Journal] [Ungated]\nClifford, Scott and Carlisle Rainey. “Estimators for Topic-Sampling Designs.” [Preprint]\n\nSlides, with transitions [Dropbox] and without [Dropbox]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Carlisle Rainey",
    "section": "",
    "text": "email\n  \n  \n    \n     CV\n  \n\n  \n  \nI’m an Associate Professor in the Department of Political Science at Florida State University. My research has appeared in the American Political Science Review, the American Journal of Political Science, Political Analysis, and other peer-reviewed journals. I teach courses in American politics, comparative politics, and political methodology.\n\nFollow My Research\nIf want to stay up-to-date on my work, follow me on Google Scholar.\n\n\n\n\n\n\nLatest: A Paper on Statistical Power\n\n\n\nMy latest working paper is “Power Rules.” It provides a practical guide to power analysis aimed at political scientists, including advice about using pilot data. You can find the preprint here, a talk here, and a blog post here.\n\n\n\n\nConnect in Other Places\nI’m on Twitter, Bluesky, and Mastodon. I publicly version-control some of research projects and courses on GitHub. I upload my talks to Speaker Deck. I’m on Google Scholar, ORCID and OSF."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\nAug 18, 2025\n\n\nFor Your Syllabus: Statistical Power\n\n\n3 min\n\n\n\n\nMar 31, 2025\n\n\nCustomize Your Terminal Prompt on macOS\n\n\n1 min\n\n\n\n\nMar 19, 2025\n\n\nKane (2025) as a Power Paper\n\n\n6 min\n\n\n\n\nJun 10, 2024\n\n\nStatistical Power from Pilot Data: An Example\n\n\n11 min\n\n\n\n\nJun 3, 2024\n\n\nStatistical Power from Pilot Data: Simulations to Illustrate\n\n\n11 min\n\n\n\n\nAug 30, 2023\n\n\nFirth’s Logit: Some References\n\n\n3 min\n\n\n\n\nAug 18, 2023\n\n\nEquivalence Tests Using {marginaleffects}\n\n\n8 min\n\n\n\n\nAug 15, 2023\n\n\nDaily Writing\n\n\n7 min\n\n\n\n\nAug 11, 2023\n\n\nBenchmarking Firth’s Logit: {brglm2} versus {logistf}\n\n\n4 min\n\n\n\n\nJun 12, 2023\n\n\nPower, Part III: The Rule of 3.64 for Statistical Power\n\n\n24 min\n\n\n\n\nJun 1, 2023\n\n\nTeaching Confidence Intervals and Hypothesis Testing with gganimate\n\n\n20 min\n\n\n\n\nMay 25, 2023\n\n\nPower, Part II: What Do Confidence Intervals from Well-Powered Studies Look Like?\n\n\n10 min\n\n\n\n\nMay 21, 2023\n\n\nPower, Part I: Power Is for You, Not for Reviewer Two\n\n\n8 min\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/2025-03-21-customize-your-terminal-prompt-on-macOS/index.html",
    "href": "blog/2025-03-21-customize-your-terminal-prompt-on-macOS/index.html",
    "title": "Customize Your Terminal Prompt on macOS",
    "section": "",
    "text": "This is for macOS using the default zsh shell (used in macOS Catalina and later).\nThe command prompt on macOS doesn’t look nice by default. Here’s my preferred adjustment, where the prompt shows just the current folder followed by &gt;.\nLike this:\nmy-folder &gt;\nHere’s what I want the Terminal to look like on macOS (when I’m working on website/).\n\n\n\n\n\nAnd Here’s what I want it to look like in RStudio."
  },
  {
    "objectID": "blog/2025-03-21-customize-your-terminal-prompt-on-macOS/index.html#motivation",
    "href": "blog/2025-03-21-customize-your-terminal-prompt-on-macOS/index.html#motivation",
    "title": "Customize Your Terminal Prompt on macOS",
    "section": "",
    "text": "This is for macOS using the default zsh shell (used in macOS Catalina and later).\nThe command prompt on macOS doesn’t look nice by default. Here’s my preferred adjustment, where the prompt shows just the current folder followed by &gt;.\nLike this:\nmy-folder &gt;\nHere’s what I want the Terminal to look like on macOS (when I’m working on website/).\n\n\n\n\n\nAnd Here’s what I want it to look like in RStudio."
  },
  {
    "objectID": "blog/2025-03-21-customize-your-terminal-prompt-on-macOS/index.html#how-to-do-it",
    "href": "blog/2025-03-21-customize-your-terminal-prompt-on-macOS/index.html#how-to-do-it",
    "title": "Customize Your Terminal Prompt on macOS",
    "section": "How to Do It",
    "text": "How to Do It\nHere’s how to make the change:\n\nOpen Terminal and run open -a TextEdit ~/.zshrc. This opens ~/.zshrc in TextEdit.\nAdd export PS1=\"%1~ &gt; \" to the bottom of the file.\nSave and close TextEdit.\nRestart Terminal or run source ~/.zshrc to see the new prompt.\n\nIf you ever want to reset or further customize your prompt, you can edit ~/.zshrc again and change the PS1 line."
  },
  {
    "objectID": "blog/2024-06-10-pilot-power-example/index.html",
    "href": "blog/2024-06-10-pilot-power-example/index.html",
    "title": "Statistical Power from Pilot Data: An Example",
    "section": "",
    "text": "We can think of statistical power as determined by the ratio \\(\\frac{\\tau}{SE}\\), where \\(\\tau\\) is the treatment effect and SE is the standard error of the estimate. To reason about statistical power, one needs to make assumptions or predictions about the treatment effect and the standard error.\nAnd as data-oriented researchers, we often want to use data to inform these predictions and assumptions. We might want to use pilot data.1\nUsually:\nWith a predicted standard error in hand, we can predict the minimum detectable effect, the statistical power, or the required sample size in the planned study.\nIn this post, I give an example of how this can work."
  },
  {
    "objectID": "blog/2024-06-10-pilot-power-example/index.html#predicting-the-se-from-pilot-data",
    "href": "blog/2024-06-10-pilot-power-example/index.html#predicting-the-se-from-pilot-data",
    "title": "Statistical Power from Pilot Data: An Example",
    "section": "Predicting the SE from pilot data",
    "text": "Predicting the SE from pilot data\nHere’s how I suggest we use pilot data to predict the standard error in the planned study:\n\n\n\n\n\n\nPredicting the SE in the planned study using pilot data\n\n\n\nConservatively, the standard error will be about \\(\\sqrt{\\frac{n^{pilot}}{n^{planned}}} \\cdot \\left\\lbrack \\left( \\sqrt{\\frac{1}{n^{pilot}}} + 1 \\right) \\cdot {\\widehat{SE}}_{\\widehat{\\tau}}^{pilot} \\right\\rbrack\\), where \\(n^{pilot}\\) is the number of respondents per condition in the pilot data, \\(SE_{\\widehat{\\tau}}^{pilot}\\) is the estimated standard error using the pilot data, and \\(n^{planned}\\) is the number of respondents per condition in the planned study.\n\n\nThe factor \\(\\left( \\sqrt{\\frac{1}{n^{pilot}}} + 1 \\right)\\) nudges the standard error from the pilot study in a conservative direction, since it might be an under-estimate of the actual standard error.2 For the details, see this early paper, but this conservative standard error estimate is approximately the upper bound of a 95% confidence interval for the standard error using the pilot data.\n2 More generally, we can use a bootstrap to conservatively estimate the standard error, without relying on this analytical approximation."
  },
  {
    "objectID": "blog/2024-06-10-pilot-power-example/index.html#example",
    "href": "blog/2024-06-10-pilot-power-example/index.html#example",
    "title": "Statistical Power from Pilot Data: An Example",
    "section": "Example",
    "text": "Example\n\nThe Robbins et al. study\nAs an example, let’s use half of the experiment conducted by Robbins et al. (2024).\nRobbins et al. use a 2x2 factorial vignette design, randomly assigning each respondent to read one of four vignettes. The vignette describes a hypothetical covert operation ordered by the president that ends in either success or failure. Then, the vignette describes a whistleblower coming forward and describes the president’s opposition in Congress as either amplifying or ignoring the whistleblower.\n\n\n\n    \n\n\n\n\n\n\n\nPresident's Opposition in Congress\nOutcome of Operation\n\n\n\n\n\nSuccess\nFailure\n\n\nAmplifies Whistleblower\nVignette 1: Success & Amplify\nVignette 2: Failure & Amplify\n\n\nIgnores Whistleblower\nVignette 3: Success & Ignore\nVignette 4: Failure & Ignore\n\n\n\n\n\n\n\nAfter the vignette, the respondent is asked whether they approve of the opposition in Congress’ actions on a seven-point Likert scale from strongly approve to strongly disapprove.\nFor a simple example, let’s focus on the effect of amplifying the whistleblower when the operation succeeds. That is, let’s compare responses after Vignette 1 and Vignette 3. How much does amplifying a whistleblower increase approval when the opperation succeeds? We expect a small effect here, so we should pay careful attention to power."
  },
  {
    "objectID": "blog/2024-06-10-pilot-power-example/index.html#the-task",
    "href": "blog/2024-06-10-pilot-power-example/index.html#the-task",
    "title": "Statistical Power from Pilot Data: An Example",
    "section": "The task",
    "text": "The task\nWe hoped to detect an effect as small as 0.35 points on the seven-point scale and had tentatively planned on 250 respondents per condition. To test the survey instrument and data provider, we conducted a small pilot with about 75 respondents per condition. Let’s use those pilot data to check whether 250 respondents seem sufficient."
  },
  {
    "objectID": "blog/2024-06-10-pilot-power-example/index.html#the-data",
    "href": "blog/2024-06-10-pilot-power-example/index.html#the-data",
    "title": "Statistical Power from Pilot Data: An Example",
    "section": "The data",
    "text": "The data\nIn the {crdata} package on GitHub, you can find the the pilot data we collected leading up to the main study.\n\n# download the {crdata} package from github\nremotes::install_github(\"carlislerainey/crdata\")\n\nNow let’s load the pilot data. To focus on observations where the operation succeeds, we’re going to keep only the observations where the vignette describes a successful observation.\n\n# load pilot data and keep only success condition\nrobbins2_pilot &lt;- crdata::robbins_pilot |&gt;\n  subset(failure == \"Success\") |&gt;\n  glimpse()\n\nRows: 147\nColumns: 5\n$ cong_overall &lt;dbl&gt; 3, 1, -2, 0, -2, -1, 0, -1, 0, 0, 0, 2, -1, 3, -3, 0, 0, …\n$ failure      &lt;fct&gt; Success, Success, Success, Success, Success, Success, Suc…\n$ amplify      &lt;fct&gt; Ignore, Ignore, Ignore, Amplify, Ignore, Ignore, Ignore, …\n$ pid7         &lt;fct&gt; Strong Democrat, Not very strong Republican, Strong Democ…\n$ pid_strength &lt;dbl&gt; 3, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 1, 0, 3, 3, 3, 3, 1, 1, …\n\n\ncong_overall is the respondent’s approval of Congress’ actions on a seven-point scale and amplify indicates whether Congress amplified the whistleblower (i.e., criticized the president).\n\nAnalyzing the pilot data\nNow let’s analyze the pilot data as we plan to analyze the main data set that we plan to collect later. We’re interested in the average response in the Amplify and Ignore conditions, so let’s use a t-test.\n\n# t test\nfit_pilot &lt;- t.test(cong_overall ~ amplify, data = robbins2_pilot)\n\n\n\n\n\n\n\nIgnore the Estimated Treatment Effect\n\n\n\nIt can be really tempting to look at the estimated treatment effect. In this pilot study, it’s actually statistically significant. I intentionally don’t show the estimated treatment effect (or quantities requiring it, like p-values). If we looked at these, we might make one of the following mistakes:\n\n“The pilot got significant results, therefore even the pilot is sufficiently powered.”\n“The estimate from the pilot is significant, therefore we can use the estimated treatment effect in the power analysis.”\n\nBoth of these claims are misleading. The estimated treatment effect is very noisy, so ignore the estimated treatment effect.\n\n\n\n\nPredicting the SE in the main study\nTo predict the standard error in the main study, we need two pieces of information from this pilot:\n\nthe sample size per condition and\nthe estimated standard error.\n\nWe can get the number of observations per condition using table().\n\n# create a table showing the observations per condition\ntable(robbins2_pilot$amplify)\n\n\n Ignore Amplify \n     70      77 \n\n# sample size per condition\nn_pilot &lt;- mean(table(robbins2_pilot$amplify))\nn_pilot\n\n[1] 73.5\n\n\nAnd then we need the estimated standard error, which is computed by t.test().\n\n# get estimated standard error from pilot\nse_hat_pilot &lt;- fit_pilot$stderr\nse_hat_pilot\n\n[1] 0.2761011\n\n\nNow we can predict the standard error in the planned study.\nFor the main study, we planned on about 250 respondents per condition.\n\nn_planned &lt;- 250\n\nThe we can conservatively predict the standard error in the full study as \\(\\sqrt{\\frac{n^{pilot}}{n^{planned}}} \\cdot \\left\\lbrack \\left( \\sqrt{\\frac{1}{n^{pilot}}} + 1 \\right) \\cdot {\\widehat{SE}}_{\\widehat{\\tau}}^{pilot} \\right\\rbrack\\).\n\npred_se_cons &lt;- sqrt(n_pilot/n_planned)*((sqrt(1/n_pilot) + 1)*se_hat_pilot)\npred_se_cons\n\n[1] 0.1671691\n\n\nBut is this standard error small enough?\n\n\nEvaluating the predicted SE in the main study\nWe can convert the standard error to the minimum detectable effect with 80% power using \\(2.5 \\times SE\\).3\n3 See Bloom (1995) for an excellent discussion of this rule. I also write about it here.\n# compute conservative minimum detectable effect\n2.5*pred_se_cons\n\n[1] 0.4179227\n\n\nWe hoped to detect an effect as small as 0.35 points on the seven-point scale, so we’re going to need more than 250 respondents per condition!\nWe can also compute the power to detect an effect of 0.35 points on the seven-point scale.\n\n# compute power as a percent\n1 - pnorm(1.64 - 0.35/pred_se_cons)\n\n[1] 0.6749736\n\n\nNote that these are conservative estimates of the minimum detectable effect and statistical power.\nHere’s what things look like if we remove the conservative nudge \\(\\left( \\sqrt{\\frac{1}{n^{pilot}}} + 1 \\right)\\) and predict the standard error as \\(\\sqrt{\\frac{n^{pilot}}{n^{planned}}} \\cdot {\\widehat{SE}}_{\\widehat{\\tau}}^{pilot}\\).\n\n# without the conservative nudge\npred_se &lt;- sqrt(n_pilot/n_planned)*se_hat_pilot\n2.5*pred_se  # best guess of minimum detectable effect\n\n[1] 0.3742673\n\n1 - pnorm(1.64 - 0.35/pred_se)  # best guess of power\n\n[1] 0.7573806\n\n\nAs you can see, the minimum detectable effect and power are a little too low. We need more respondents!\n\n\nAdjusting the Sample Size\nOur plan of 250 respondents per condition seems too low. If we want, we can predict the sample size we need to get to 80% power using the following rule:\n\n\n\n\n\n\nPredicting the required sample size in the planned study using pilot data\n\n\n\nFor 80% power to detect the treatment effect \\(\\widetilde{\\tau}\\), we will (conservatively) need about \\(n^{pilot} \\cdot \\left\\lbrack \\frac{2.5}{\\widetilde{\\tau}} \\cdot \\left( \\sqrt{\\frac{1}{n^{pilot}}} + 1 \\right) \\cdot {\\widehat{SE}}_{\\widehat{\\tau}}^{pilot} \\right\\rbrack^{2}\\) respondents per condition, where \\(n^{pilot}\\) is the number of respondents per condition in the pilot data and \\(SE_{\\widehat{\\tau}}^{pilot}\\) is the estimated standard error using the pilot data.\n\n\n\nn_pilot*((2.5/0.35)*((sqrt(1/n_pilot) + 1)*se_hat_pilot))^2\n\n[1] 356.4477\n\n\nThus to get 80% power, the pilot data suggest that we (conservatively) need about 360 respondents per condition. We used 367 in the full study. Here are the conservative predictions for 367 respondents per condition.\n\nn_planned &lt;- 367\npred_se_cons &lt;- sqrt(n_pilot/n_planned)*((sqrt(1/n_pilot) + 1)*se_hat_pilot)\npred_se_cons\n\n[1] 0.1379726\n\n2.5*pred_se_cons  # conservative minimum detectable effect\n\n[1] 0.3449315\n\n1 - pnorm(1.64 - 0.35/pred_se_cons)  # conservative power\n\n[1] 0.8150699"
  },
  {
    "objectID": "blog/2024-06-10-pilot-power-example/index.html#how-did-we-do",
    "href": "blog/2024-06-10-pilot-power-example/index.html#how-did-we-do",
    "title": "Statistical Power from Pilot Data: An Example",
    "section": "How did we do?",
    "text": "How did we do?\nWe ran the full study.4\n4 See Robbins et al. (2024) for the full results.\nrobbins2_main &lt;- crdata::robbins_main |&gt;\n  subset(failure == \"Success\") |&gt;\n  glimpse()\n\nRows: 735\nColumns: 5\n$ cong_overall &lt;dbl&gt; 2, -2, -1, -3, 0, -1, -2, -1, 1, 1, 0, -3, -2, 2, 2, -3, …\n$ failure      &lt;fct&gt; Success, Success, Success, Success, Success, Success, Suc…\n$ amplify      &lt;fct&gt; Ignore, Amplify, Amplify, Amplify, Ignore, Ignore, Amplif…\n$ pid7         &lt;fct&gt; Not very strong Republican, Not very strong Republican, S…\n$ pid_strength &lt;dbl&gt; 2, 2, 3, 3, 3, 2, 0, 2, 2, 1, 2, 0, 3, 1, 2, 3, 2, 3, 2, …\n\n\n\nfit_main &lt;- t.test(cong_overall ~ amplify, data = robbins2_main)\nfit_main$stderr\n\n[1] 0.1322618\n\n1 - pnorm(1.64 - 0.35/fit_main$stderr)\n\n[1] 0.8428563\n\n\nAs you can see, the pilot data gave us a good, slightly conservative prediction. We conservatively predicted a standard error of 0.138 in the planned study and we estimated a standard error of 0.132 after running the study. We conservatively predicted our power would be about 82% to detected an effect of 0.35 on the seven-point scale, but after running the study, it seems like we had about 84% power."
  },
  {
    "objectID": "blog/2024-06-10-pilot-power-example/index.html#a-bootstrap-alternative",
    "href": "blog/2024-06-10-pilot-power-example/index.html#a-bootstrap-alternative",
    "title": "Statistical Power from Pilot Data: An Example",
    "section": "A bootstrap alternative",
    "text": "A bootstrap alternative\nWe can also use the bootstrap as an alternative. There are a few ways one might approach it.\nHere’s one:\n\nTreat the pilot data as a population. Create a data set with the planned sample size by sampling with replacement from the pilot data.\nPerform the planned analysis on each resampled data set.\nStore the estimated standard error from each analysis.\n\nRepeat the process above many times. For each standard error estimate, compute the implied statistical power. This gives a distribution of power estimates. Find a value near the bottom of this distribution. The factor we used above—The factor \\(\\left( \\sqrt{\\frac{1}{n^{pilot}}} + 1 \\right)\\)—nudges the standard error to about the 2.5th percentile, so we can use that here, too.\n\n# number of bootstrap iterations\nn_bs &lt;- 10000\n\nbs_se &lt;- numeric(n_bs)  # a container\nfor (i in 1:n_bs){\n  # resample 367 observations from each condition\n  bs_data &lt;- robbins2_main %&gt;%\n    group_by(amplify) %&gt;%\n    sample_n(size = 367, replace = TRUE)\n  # run planned analysis\n  bs_fit &lt;- t.test(cong_overall ~ amplify, data = bs_data)\n  # grab se\n  bs_se[i] &lt;- bs_fit$stderr\n}\n\n# compute 2.5th percentile of power to obtain conservative estimate\npwr &lt;- 1 - pnorm(1.64 - 0.35/bs_se)\nquantile(pwr, probs = 0.025)\n\n     2.5% \n0.8206062 \n\n\nUsing the analytical approximation, we got 0.815 as a conservative estimate of power. The bootstrap gave us 0.820 as a conservative estimate. The actual power in the full study turned out to be about 0.843. (Remember, all of these power calculations are power to detected an effect of 0.35 points on the seven-point scale.)"
  },
  {
    "objectID": "blog/2024-06-10-pilot-power-example/index.html#the-paper",
    "href": "blog/2024-06-10-pilot-power-example/index.html#the-paper",
    "title": "Statistical Power from Pilot Data: An Example",
    "section": "The paper",
    "text": "The paper\nI have an early draft of a paper on these (and other) ideas. Please test them out in your own work and let me know if you have questions, comments, and suggestions. I’m interested in making the paper as clear and useful as I can."
  },
  {
    "objectID": "blog/2023-08-30-firth-references/index.html",
    "href": "blog/2023-08-30-firth-references/index.html",
    "title": "Firth’s Logit: Some References",
    "section": "",
    "text": "In Rainey and McCaskey (2021), Kelly McCaskey and I offer a accessible and practical (re)introduction to Firth’s penalized maximum likelihood estimator that (1) corrects the small sample bias and (2) reduces the excessive variance of the usual maximum likelihood estimator.\nBelow, I bookmark other references that might be helpful.\n\n\n\n\n\n\nI’m sure there are embarrassing omissions. If you see an omission, please let me know (self-promotion is encouraged, especially not-yet-published papers).\n\n\n\nThis Stack Exchange answer gives a brief, but careful explanation of Firth’s logit. If you’re looking for a quick explanation, start here.\n\nThe Two Main Papers\n\nFirth (1993) originally introduced the idea. Kelly and I draw mostly on this paper—it’s a wonderful paper.\nKosmidis and Firth (2021) follow-up with additional theoretical results that are relevant for the estimator as used in practice since 1993. This happened to come out while our paper was working its way through the publication process. Most importantly, they discuss the shrinkage property of the estimator, which is what Kelly and I highlight as under-appreciated (and really important!).\n\nFrom my perpective, these are the two main papers to refer to if you’re concerned about small sample bias in logistic regression models.\n\n\nExtensions\nBeyond these two main papers, there have been a few extensions. Zietkiewicz and Kosmidis (2023) talk about Firth’s logit in very large data sets. Cook, Hays, and Franzese (2018) make a good argument for using Firth’s estimator in panel data sets with binary outcomes and fixed effects. Sterzinger and Kosmidis (2023) apply these ideas to mixed models (or random effects models). Šinkovec et al. (2021) compare Firth’s approach to ridge regression, and suggest that Firth’s is superior in small or sparse data sets. Puhr et al. (2017) study Firth’s logit in the context of rare events and propose FLIC and FLAC as alternatives.\n\n\nApplications\n\nRöver et al. (2022) offer an application of Firth’s logit to clinical trials.\nTurner and Firth (2012) offer an application to Bradley-Terry models with the {BradleyTerry2} R package.\n\n\n\nSeparation and Finiteness\nI learned about Firth’s estimator from Zorn (2005), who follows Heinze and Schemper (2002) in suggesting it as a solution to separation. According to David Firth in this blog post, this is the application that stimulated interest in the approach after it went relatively unnoticed for a few years. (Great post, I highly recommend reading it!) This application piqued my interest in Firth’s estimator. Briefly, I think Firth’s default penalty might not be substantively reasonable in a given application (Rainey 2016) (see also Beiser-McGrath (2020)) and the usual likelihood ratio and score tests work well without the penalty (Rainey 2023).\n\n\n\n\n\n\nFor more on Firth’s logit, see Ioannis Kosmidis’ research page and Georg Heinz Google Scholar page.\n\n\n\n\n\n\n\n\nReferences\n\nBeiser-McGrath, Liam F. 2020. “Separation and Rare Events.” Political Science Research and Methods 10 (2): 428–37. https://doi.org/10.1017/psrm.2020.46.\n\n\nCook, Scott J., Jude C. Hays, and Robert J. Franzese. 2018. “Fixed Effects in Rare Events Data: A Penalized Maximum Likelihood Solution.” Political Science Research and Methods 8 (1): 92–105. https://doi.org/10.1017/psrm.2018.40.\n\n\nFirth, David. 1993. “Bias Reduction of Maximum Likelihood Estimates.” Biometrika 80 (1): 27–38. https://doi.org/10.1093/biomet/80.1.27.\n\n\nHeinze, Georg, and Michael Schemper. 2002. “A Solution to the Problem of Separation in Logistic Regression.” Statistics in Medicine 21 (16): 2409–19. https://doi.org/10.1002/sim.1047.\n\n\nKosmidis, Ioannis, and David Firth. 2021. “Jeffreys-Prior Penalty, Finiteness and Shrinkage in Binomial-Response Generalized Linear Models.” Biometrika 108 (1): 71–82. https://doi.org/10.1093/biomet/asaa052.\n\n\nPuhr, Rainer, Georg Heinze, Mariana Nold, Lara Lusa, and Angelika Geroldinger. 2017. “Firth’s Logistic Regression with Rare Events: Accurate Effect Estimates and Predictions?” Statistics in Medicine. https://doi.org/10.1002/sim.7273.\n\n\nRainey, Carlisle. 2016. “Dealing with Separation in Logistic Regression Models.” Political Analysis 24 (3): 339–55. https://doi.org/10.1093/pan/mpw014.\n\n\n———. 2023. “Hypothesis Tests Under Separation.” http://dx.doi.org/10.31235/osf.io/bmvnu.\n\n\nRainey, Carlisle, and Kelly McCaskey. 2021. “Estimating Logit Models with Small Samples.” Political Science Research and Methods 9 (3): 549–64. https://doi.org/10.1017/psrm.2021.9.\n\n\nRöver, Christian, Moreno Ursino, Tim Friede, and Sarah Zohar. 2022. “A Straightforward Meta-Analysis Approach for Oncology Phase I Dose-Finding Studies.” Statistics in Medicine 41 (20): 3915–40. https://doi.org/10.1002/sim.9484.\n\n\nŠinkovec, Hana, Georg Heinze, Rok Blagus, and Angelika Geroldinger. 2021. “To Tune or Not to Tune, a Case Study of Ridge Logistic Regression in Small or Sparse Datasets.” BMC Medical Research Methodology 21 (1). https://doi.org/10.1186/s12874-021-01374-y.\n\n\nSterzinger, Philipp, and Ioannis Kosmidis. 2023. “Maximum Softly-Penalized Likelihood for Mixed Effects Logistic Regression.” Statistics and Computing 33 (2). https://doi.org/10.1007/s11222-023-10217-3.\n\n\nTurner, Heather, and David Firth. 2012. “Bradley-Terry Models inR: TheBradleyTerry2Package.” Journal of Statistical Software 48 (9). https://doi.org/10.18637/jss.v048.i09.\n\n\nZietkiewicz, Patrick, and Ioannis Kosmidis. 2023. “Bounded-Memory Adjusted Scores Estimation in Generalized Linear Models with Large Data Sets.” https://doi.org/10.48550/ARXIV.2307.07342.\n\n\nZorn, Christopher. 2005. “A Solution to Separation in Binary Response Models.” Political Analysis 13 (2): 157–70. https://doi.org/10.1093/pan/mpi009."
  },
  {
    "objectID": "blog/2023-08-15-daily-writing/index.html",
    "href": "blog/2023-08-15-daily-writing/index.html",
    "title": "Daily Writing",
    "section": "",
    "text": "I try to write everyday.1\nBy “writing,” I mean “pushing the paper closest to publication just a little bit closer.” I want to think about the next step on the journey to the published paper and do it. According to this loose definition of writing, it might involve data collection, data analysis, creating slides, or even writing and polishing text. It might involve organization, planning, or learning new skills. It excludes any tasks that aren’t necessary to complete the project.\nBy “everyday,” I mean at least every weekday, probably at the same time every day and probably first thing in the morning. For better or worse, academics are evaluated by their research productivity."
  },
  {
    "objectID": "blog/2023-08-15-daily-writing/index.html#urgency-and-importance",
    "href": "blog/2023-08-15-daily-writing/index.html#urgency-and-importance",
    "title": "Daily Writing",
    "section": "Urgency and Importance",
    "text": "Urgency and Importance\nPresident Eisenhower famously characterized his duties: “I have two kinds of problems, the urgent and the important. The urgent are not important, and the important are never urgent.”\nFollowing Eisenhower’s Box, we might assign degrees of urgency and importance to tasks in academic tasks. In graduate school, I had teaching responsibilities, RA duties, readings for seminars, homework for methods classes, preliminary exams, and administrative tasks. All of these tasks are important. They must be completed. They must be completed well. Yet I was evaluated largely on my papers. As a faculty member, little has changed.\nWriting is important, but writing never quite becomes urgent. It’s easy to put off writing to prepare a lecture (or write a blog post)."
  },
  {
    "objectID": "blog/2023-08-15-daily-writing/index.html#the-evidence",
    "href": "blog/2023-08-15-daily-writing/index.html#the-evidence",
    "title": "Daily Writing",
    "section": "The Evidence",
    "text": "The Evidence\nRobert Boice studied academic productivity carefully. A couple of his studies provide some evidence for my strategy to write every day.\nFirst, he assessed how early-career academics spend their time. The figure below shows the results. Notice that these faculty spend more time in committee meetings (2 hrs.) than writing (1.5 hours).\n\n\n\n\n\n\n\n\n\nSecond, Boice conducted an experiment to assess the effect of writing strategies.\nBoice randomly divided 27 academics into three groups:2\n2 This is a small sample, but it supports my claim so it’s okay.\nThe control group agreed to defer all but the most urgent writing for ten weeks.\nThe spontaneous group agreed write when they felt like it.\nThe contingency group agreed to donate to an anti-charity if they failed to write every day.\n\nThe figure below shows that regular writing routine increase production of both pages and ideas. Notice that the spontaneous writers barely produced more ideas and pages than the group trying to avoid writing.\n\n\n\n\n\n\n\n\n\nI find these results compelling, but note that Helen Sword urges some caution."
  },
  {
    "objectID": "blog/2023-08-15-daily-writing/index.html#how-i-do-it",
    "href": "blog/2023-08-15-daily-writing/index.html#how-i-do-it",
    "title": "Daily Writing",
    "section": "How I Do It",
    "text": "How I Do It\nEveryone is different, and my own approach has evolved over time. Here are the key ingredients (for me):\n\nWrite for two hours at a regular time. Consistency is key.3\nAvoid writing outside this window. Set your window so that your window is “enough.”\nTake breaks. I take long breaks from writing. But these are intentional and planned.4\nFamily permitting, I think it’s helpful to spend a little while pushing the projects forward on the weekends, just to keep the momentum up.5\n\n3 Two hours works really well for me. My productivity degrades quickly after two hours, so it’s best to move on to less taxing tasks. But it takes me a while to get warmed up, so I need to keep moving while I’ve got momentum.4 An unfortunate outcome is not writing and being stressed about not writing.5 Just 15 minutes is great. This slot is perfect for proof-reading.I admit that I deviate from the strategy above (and not always intentionally). But I’ve been at this long enough to know that a regular routine works really well for me."
  },
  {
    "objectID": "blog/2023-08-15-daily-writing/index.html#what-if-youre-not-ready-to-write-yet",
    "href": "blog/2023-08-15-daily-writing/index.html#what-if-youre-not-ready-to-write-yet",
    "title": "Daily Writing",
    "section": "What if you’re not ready to write yet?",
    "text": "What if you’re not ready to write yet?\nIt’s my view that PhD students should write every day, from the first day of their first semester (remember that I have a broad definition of “write”). Most students need some time before they’re ready to jump into the technical details a solo project, but there are always things to do.\nIf you can’t identify a specific task to work on, here are some resources to help you brainstorm.\n\nPlan and organize. Start by reading How to Write a Lot. Perhaps read Getting Things Done. Perhaps read Air & Light & Time & Space or Writing for Social Scientists.\nRead “Publication, Publication” and the updates.\nBefore you can jump into a project, you need to know the literature. Spend some writing time exploring literatures that you might want to contribute to. What interests you most? The Annual Review of Political Science is a valuable resource.\nOnce you have a specific topic of interest, you need to learn that literature. You can spend dozens of “writing” sessions reading and taking notes. I strongly encourage you to read and take notes systematically, as Raul Pacheco-Vega suggests using a spreadsheet, Elaine Campbell suggests a similar method, and Katherine Firth suggests a using Cornell notes.\nTanya Golash-Boza lists ten ways to write everyday if you’ve got a paper in-progress."
  },
  {
    "objectID": "blog/2023-06-12-power-3-rule-of-364/index.html",
    "href": "blog/2023-06-12-power-3-rule-of-364/index.html",
    "title": "Power, Part III: The Rule of 3.64 for Statistical Power",
    "section": "",
    "text": "A Paper\n\n\n\nThis post turned out to be somewhat popular, so I’ve written up a more formal, careful description of the idea in a full-length paper. You can find the preprint “Power Rules” here."
  },
  {
    "objectID": "blog/2023-06-12-power-3-rule-of-364/index.html#background",
    "href": "blog/2023-06-12-power-3-rule-of-364/index.html#background",
    "title": "Power, Part III: The Rule of 3.64 for Statistical Power",
    "section": "Background",
    "text": "Background\nI’ve wrapped up the argument that you should pursue statistical power in your experiments. In sum, you should do it for you (not a future Reviewer 2) and you shouldn’t see confidence intervals nestle consistently against zero.\n\n“Power Is For You, Not For Reviewer 2”\n“What Do Confidence Intervals From Well-Powered Studies Look Like?”\n\nIn this post, I’d like to develop the intuition for power calculations, two helpful guidelines, and one implication.\nMain takeaway: You need the ratio of the true effect and the standard error to be more than 3.64."
  },
  {
    "objectID": "blog/2023-06-12-power-3-rule-of-364/index.html#starting-point-the-sampling-distribution",
    "href": "blog/2023-06-12-power-3-rule-of-364/index.html#starting-point-the-sampling-distribution",
    "title": "Power, Part III: The Rule of 3.64 for Statistical Power",
    "section": "Starting Point: The Sampling Distribution",
    "text": "Starting Point: The Sampling Distribution\nWhen you run one experiment, you realize one of many possible patterns of randomization. This particular realization produces a single estimate of the treatment effect from a distribution of possible estimates. The distribution of possibilities is called a “sampling distribution.”\nThus, we can think of the estimate as a random variable and its distribution as the sampling distribution. The sampling distribution is key to everything I do in this post, so let’s spend some time with it.\nLet’s imagine that we did the exact same study 50 times. Let’s say that we computed a difference-in-means in dollars ($) donated.1 I refer to this difference-in-means as the estimated treatment effect. It is the estimate of the average treatment effect (in $). This estimate will vary across the many possible patterns of randomization because each pattern puts different respondents in the treatment and control group.\n1 I just want an easy-to-use unit here, and dollars meets that criteria. Other units work fine, too.We can visualize this with ggnaminate. We can imagine each iteration of the study as producing a particular estimate. We continue to repeat the study and collect the estimates. Eventually, we can produce a histogram from this collection of estimates. This histogram represents the sampling distribution and is fundamental to the calculations that follow. The figure belows shows how we might collect the points into a histogram.\n\n\nCode\nlibrary(tidyverse)\nlibrary(gganimate)\nlibrary(magick)\n\n# gif pars\nduration &lt;- 24 # must be even\nfps &lt;- 25\nnframes &lt;- duration*fps\nscale &lt;- 2.5\nwidth &lt;- 8\nheight &lt;- 6\nres &lt;- 125\n\n# study parameters\ntrue_effect &lt;- 1\nse &lt;- 0.4\n\n# number of times to repeat the study\nn_studies &lt;- 50 # nframes\n\n# create a data frame of confidence intervals\nests &lt;- tibble(study_id = 1:n_studies, \n               est = c(rnorm(n_studies, true_effect, se))) %&gt;%\n  mutate(reject_null = ifelse(est - 1.64*se &gt; 0, \"Yes\", \"No\"))\n\n# add two things to the data frame of confidence intervals\n# 1. an initial row with study_id = 1 and est = NA so that \n#    the plot starts empty (gganimate would start with the \n#    first observation in place otherwise).\n# 2. a group variable that defines the row. This is the same\n#    as the study_id, except the dummy row from (1) and the \n#    actual first row have different groups.\nanimate_data &lt;- bind_rows(\n  tibble(study_id = 1, est = NA),  # study_id = 1, est = NA\n  ests                             # actual cis\n) %&gt;%\n  mutate(group = 1:n()) \n\nsplit_animate_data &lt;- animate_data %&gt;%        # group (row index)\n  split(.$group) %&gt;%\n  accumulate(~ bind_rows(.x, .y)) %&gt;% \n  bind_rows(.id = \"frame\") %&gt;% \n  mutate(frame = as.integer(frame))\n\n\nse_lines &lt;- tribble(\n  ~se_, ~label, ~chance, ~ch_loc_,  # trailing _ means not rescaeld to study se\n  0, \"True Effect\", NA, NA,\n  1, \"+1 SE\", scales::percent(pnorm(1) - pnorm(0), accuracy = 1), 0.5,\n  2, \"+2 SE\", scales::percent(pnorm(2) - pnorm(1), accuracy = 1), 1.5,\n  3, \"+3 SE\", scales::percent(pnorm(3) - pnorm(2), accuracy = 1), 2.5,\n  -1, \"-1 SE\", scales::percent(pnorm(0) - pnorm(-1), accuracy = 1), -0.5,\n  -2, \"-2 SE\", scales::percent(pnorm(-1) - pnorm(-2), accuracy = 1), -1.5,\n  -3, \"-3 SE\", scales::percent(pnorm(-2) - pnorm(-3), accuracy = 1), -2.5,\n) %&gt;%\n  mutate(ch_loc = ch_loc_*se + true_effect,\n         se = se_*se + true_effect)\n\n# start with a ggplot\ngg1 &lt;- ggplot(animate_data, aes(x = est, \n                               y = study_id,\n                               group = group)) + \n  geom_vline(data = se_lines, aes(xintercept = se, \n                                  color = -dnorm(se_)), linetype = \"dashed\") + \n  geom_label(data = se_lines, aes(x = se, y = n_studies + 2, label = label, group = NULL, color = -dnorm(se_))) + \n  geom_text(data = se_lines, aes(x = ch_loc, y = 4, label = chance, group = NULL)) + \n  geom_point(aes(color = -dnorm((est- true_effect)/se)),\n             size = 3) + \n  geom_rug(sides = \"b\", \n           aes(x = est, \n               color = -dnorm((est- true_effect)/se)), \n           alpha = 0.5, \n           length = unit(0.025, \"npc\")) + \n  theme_bw() + \n  theme(panel.grid.minor.y = element_blank()) + \n  labs(x = \"Estimate of Effect\",\n       y = \"Study Number\") +\n  theme(legend.position = \"none\")\n\n# add dyamics to the plot\ngg1_anim &lt;- gg1 +  \n  transition_states(states = group) + \n  # how points enter\n  enter_drift(y_mod = 10) +\n  enter_grow() +\n  enter_fade() + \n  # how points exit/remain\n  exit_fade(alpha = 0.5) +\n  exit_shrink(size = 1) + \n  shadow_mark(alpha = 0.5) \n\ngg1_gif&lt;- animate(gg1_anim, nframes = nframes, duration = duration, width = width, height = height, units = \"in\", res = res)\nanim_save(\"gg1.gif\")\ngg1_mgif &lt;- image_read(\"gg1.gif\")\n\n\n## plot 2: histogram\n# start with a ggplot\ngg2 &lt;- ggplot(split_animate_data, aes(x = est, group = frame)) + \n  geom_histogram(binwidth = se, boundary = true_effect, fill = \"grey\") + \n  geom_vline(data = se_lines, aes(xintercept = se, \n                                  color = -dnorm(se_)), linetype = \"dashed\") + \n  geom_label(data = se_lines, aes(x = se, y = Inf, label = label, group = NULL, color = -dnorm(se_)), vjust = 1.5) + \n  geom_label(data = se_lines, aes(x = ch_loc, y = 0, label = chance, group = NULL), vjust = -1) + \n  #geom_density(linewidth = 2) + \n  geom_rug(sides = \"b\", \n           aes(x = est, \n               color = -dnorm((est- true_effect)/se)), \n           alpha = 0.5, \n           length = unit(0.025, \"npc\")) + \n  theme_bw() + \n  labs(x = \"Estimate of Effect\",\n       y = \"Count\") + \n  theme(legend.position = \"none\")\n\n# add dyamics to the plot\ngg2_anim &lt;- gg2 +  \n  transition_states(states = frame)\n\ngg2_gif&lt;- animate(gg2_anim, nframes = nframes, duration = duration, width = width, height = height, units = \"in\", res = res)\nanim_save(\"gg2.gif\")\ngg2_mgif &lt;- image_read(\"gg2.gif\")\n\n\n\nnew_gif &lt;- image_append(c(gg1_mgif[1], gg2_mgif[1]), stack = FALSE)\nfor(i in 2:nframes){\n  combined_gif &lt;- image_append(c(gg1_mgif[i], gg2_mgif[i]), stack = FALSE)\n  new_gif &lt;- c(new_gif, combined_gif)\n}\nnew_gif\n\n\n\n\n\n\n\n\n\nBefore the study, we can predict two features of this sampling distribution.\n\nFirst, it will usually have a bell-shaped, normal distribution.\nSecond, we can predict the standard deviation of this distribution with good accuracy before conducting a single study and excellent accuracy after just one study. We call the standard deviation of the sampling distribution the standard error or SE.\n\nFor our purposes, then, we can describe the sampling distribution as normally distributed with an assumed mean and SD. The mean is the assumed true effect and the SD is the well-predicted SE.\nThis is the key claim: In order to build power into your experiment, you must build certain properties into the sampling distribution.\nThe design of your experiment will not affect the normality of the sampling distribution, but it will change the true effect and the SE. Changing the true effect and the SE will change the power."
  },
  {
    "objectID": "blog/2023-06-12-power-3-rule-of-364/index.html#thinking-about-true-effect-and-standard-error-as-targets",
    "href": "blog/2023-06-12-power-3-rule-of-364/index.html#thinking-about-true-effect-and-standard-error-as-targets",
    "title": "Power, Part III: The Rule of 3.64 for Statistical Power",
    "section": "Thinking about True Effect and Standard Error as Targets",
    "text": "Thinking about True Effect and Standard Error as Targets\nI’m leaving aside how to predict the standard error of the experiment or choose the true effect. This post is about the target standard error and true effect.\nPredicting the standard error is a mechanical process mixed with a little guesswork.2 Choose a true effect is a mostly substantive decision.3\n2 I’ll suggest two methods I like. First, use \\(\\text{predicted SE} = \\frac{\\text{SD of same outcome in different data set}}{0.5 \\sqrt{\\text{sample size}}}\\). I like to confirm this estimate with a small pilot. I sample observations from this pilot data set, run the full analysis, and confirm that my prediction is close, and make any needed adjustments.3 Cyrus Samii likes to use the MME, I like a conservatively choosen guess of what the effect actually is.But instead of talking about a target power, I like to talk about a target standard error (given a true effect) or a target true effect (given a standard error)—power is just not a very intuitive quantity.\nTo understand how the true effect and the SE relate to power, we need to introduce the confidence interval."
  },
  {
    "objectID": "blog/2023-06-12-power-3-rule-of-364/index.html#testing-with-confidence-intervals",
    "href": "blog/2023-06-12-power-3-rule-of-364/index.html#testing-with-confidence-intervals",
    "title": "Power, Part III: The Rule of 3.64 for Statistical Power",
    "section": "Testing with Confidence Intervals",
    "text": "Testing with Confidence Intervals\nI like to use 90% confidence intervals to test hypotheses (see Rainey 2014 and Rainey 2015). In short, 90% confidence intervals correspond to one-tailed tests with size 0.05 and equivalence tests with size 0.05.] The formula for a 90% CI is \\(\\text{estimate} \\pm 1.64\\text{SE}\\). That is, we put “arms” around our estimate—one to the left and another to the right. Each arm is 1.64 standard errors long.\nI’ll assume we have a one-sided research hypothesis that suggests a positive effect. If the lower bound (\\(\\text{estimate} - 1.64\\text{SE}\\)) is less than zero, then we fail to reject the null hypothesis. If this lower bound is greater than zero, then we reject the null hypothesis.4\n4 This is equivalent to a z-test in a standard hypothesis testing framework using a p-value of less than 0.05 as the threshold for rejecting the null hypothesis.This focus on testing rather than estimation changes the nature of the sampling distribution. Rather than an estimate along a continuous range, we get a binary outcome: either (1) reject the null hypothesis or (2) fail to reject the null hypothesis.\nBut the sampling distribution of estimates and the associated outcomes of tests are closely related. In particular, the logic of the test implies that *if the estimate falls less than 1.64 SEs above zero, we cannot reject the null.\nWe can reconstruct the figure above using this logic. Rather than plot the points continuously along the x-axis, we can color the points (and now error bars) according to whether the lower bound falls above zero or not. And we can use a bar plot showing the number of rejections and non-rejections.\n\n\nCode\nlibrary(tidyverse)\nlibrary(gganimate)\nlibrary(magick)\n\n# gif pars\nduration &lt;- 24 # must be even\nfps &lt;- 25\nnframes &lt;- duration*fps\nscale &lt;- 2.5\nwidth &lt;- 8\nheight &lt;- 6\nres &lt;- 125\n\n# study parameters\ntrue_effect &lt;- 1\nse &lt;- 0.4\n\n# number of times to repeat the study\nn_studies &lt;- 50 # nframes\n\n# create a data frame of confidence intervals\nests &lt;- tibble(study_id = 1:n_studies, \n               est = c(rnorm(n_studies, true_effect, se))) %&gt;%\n  mutate(reject_null = ifelse(est - 1.64*se &gt; 0, \"Yes\", \"No\"),\n         lwr = est - 1.64*se,\n         upr = est + 1.64*se)\n\n# add two things to the data frame of confidence intervals\n# 1. an initial row with study_id = 1 and est = NA so that \n#    the plot starts empty (gganimate would start with the \n#    first observation in place otherwise).\n# 2. a group variable that defines the row. This is the same\n#    as the study_id, except the dummy row from (1) and the \n#    actual first row have different groups.\nanimate_data &lt;- bind_rows(\n  tibble(study_id = 1, est = NA),  # study_id = 1, est = NA\n  ests                             # actual cis\n) %&gt;%\n  mutate(group = 1:n()) \n\nsplit_animate_data &lt;- animate_data %&gt;%        # group (row index)\n  split(.$group) %&gt;%\n  accumulate(~ bind_rows(.x, .y)) %&gt;% \n  bind_rows(.id = \"frame\") %&gt;% \n  mutate(frame = as.integer(frame))\n\n\nse_lines &lt;- tribble(\n  ~se_, ~label, ~chance, ~ch_loc_,  # trailing _ means not rescaeld to study se\n  0, \"True Effect\", NA, NA,\n  1, \"+1 SE\", scales::percent(pnorm(1) - pnorm(0), accuracy = 1), 0.5,\n  2, \"+2 SE\", scales::percent(pnorm(2) - pnorm(1), accuracy = 1), 1.5,\n  3, \"+3 SE\", scales::percent(pnorm(3) - pnorm(2), accuracy = 1), 2.5,\n  -1, \"-1 SE\", scales::percent(pnorm(0) - pnorm(-1), accuracy = 1), -0.5,\n  -2, \"-2 SE\", scales::percent(pnorm(-1) - pnorm(-2), accuracy = 1), -1.5,\n  -3, \"-3 SE\", scales::percent(pnorm(-2) - pnorm(-3), accuracy = 1), -2.5,\n) %&gt;%\n  mutate(ch_loc = ch_loc_*se + true_effect,\n         se = se_*se + true_effect)\n\n# start with a ggplot\ngg1 &lt;- ggplot(animate_data, aes(x = est, \n                                y = study_id,\n                                group = group)) + \n  geom_vline(xintercept = 1.64*se) + \n  annotate(\"label\", x = 1.64*se, y = 5, label = \"1.64 SEs\") + \n  geom_errorbarh(height = 0, aes(xmin = lwr, xmax = upr, color = reject_null)) + \n  geom_point(aes(color = reject_null),\n             size = 3) + \n  geom_rug(sides = \"b\", \n           aes(x = est, \n               color = reject_null), \n           alpha = 0.5, \n           length = unit(0.025, \"npc\")) + \n  theme_bw() + \n  theme(panel.grid.minor.y = element_blank()) + \n  labs(x = \"Estimate of Effect\",\n       y = \"Study Number\") +\n  theme(legend.position = \"none\") + \n  scale_color_manual(values = c(\"Yes\" = \"#1b9e77\", \"No\" = \"#d95f02\"))\n\n# add dyamics to the plot\ngg1_anim &lt;- gg1 +  \n  transition_states(states = group) + \n  # how points enter\n  enter_drift(y_mod = 10) +\n  enter_grow() +\n  enter_fade() + \n  # how points exit/remain\n  exit_fade(alpha = 0.5) +\n  exit_shrink(size = 1) + \n  shadow_mark(alpha = 0.5) \n\ngg1_gif&lt;- animate(gg1_anim, nframes = nframes, duration = duration, width = width, height = height, units = \"in\", res = res)\nanim_save(\"gg1.gif\")\ngg1_mgif &lt;- image_read(\"gg1.gif\")\n\n\n## plot 2: histogram\n# start with a ggplot\ngg2 &lt;- ggplot(split_animate_data, aes(x = reject_null, fill = reject_null),  na.rm = TRUE) + \n  geom_bar(na.rm = TRUE) + \n  theme_bw() + \n  labs(x = \"Reject Null\",\n       y = \"Count\") + \n  theme(legend.position = \"none\") + \n  scale_x_discrete(na.translate = FALSE) + \n  scale_fill_manual(values = c(\"Yes\" = \"#1b9e77\", \"No\" = \"#d95f02\"))\n\n# add dyamics to the plot\ngg2_anim &lt;- gg2 +  \n  transition_states(states = frame)\n\ngg2_gif&lt;- animate(gg2_anim, nframes = nframes, duration = duration, width = width, height = height, units = \"in\", res = res)\nanim_save(\"gg2.gif\")\ngg2_mgif &lt;- image_read(\"gg2.gif\")\n\n\n\nnew_gif &lt;- image_append(c(gg1_mgif[1], gg2_mgif[1]), stack = FALSE)\nfor(i in 2:nframes){\n  combined_gif &lt;- image_append(c(gg1_mgif[i], gg2_mgif[i]), stack = FALSE)\n  new_gif &lt;- c(new_gif, combined_gif)\n}\nnew_gif"
  },
  {
    "objectID": "blog/2023-06-12-power-3-rule-of-364/index.html#the-key-ratio",
    "href": "blog/2023-06-12-power-3-rule-of-364/index.html#the-key-ratio",
    "title": "Power, Part III: The Rule of 3.64 for Statistical Power",
    "section": "The Key Ratio",
    "text": "The Key Ratio\nThe key to building statistical power into your experiment is to get “almost all” of the sampling distribution above 1.64 standard errors above zero. The portion of the sampling distribution that falls below 1.64 standard errors above zero does not allow the researcher to reject the null.\n\n\nCode\nlibrary(tidyverse)\n\n# study parameters\ntrue_effect &lt;- 1\nse &lt;- 0.4\n\n\nse_lines &lt;- tribble(\n  ~se_, ~label, ~chance, ~ch_loc_,  # trailing _ means not rescaeld to study se\n  0, \"True Effect\", NA, NA,\n  1, \"+1 SE\", scales::percent(pnorm(1) - pnorm(0), accuracy = 1), 0.5,\n  2, \"+2 SE\", scales::percent(pnorm(2) - pnorm(1), accuracy = 1), 1.5,\n  3, \"+3 SE\", scales::percent(pnorm(3) - pnorm(2), accuracy = 1), 2.5,\n  -1, \"-1 SE\", scales::percent(pnorm(0) - pnorm(-1), accuracy = 1), -0.5,\n  -2, \"-2 SE\", scales::percent(pnorm(-1) - pnorm(-2), accuracy = 1), -1.5,\n  -3, \"-3 SE\", scales::percent(pnorm(-2) - pnorm(-3), accuracy = 1), -2.5,\n) %&gt;%\n  mutate(ch_loc = ch_loc_*se + true_effect,\n         se = se_*se + true_effect) \n\nx &lt;- rnorm(500000, mean = true_effect, sd = se)\ndf &lt;- data.frame(x)\n\nbg_alpha &lt;- 0.3\nggplot() + \n  geom_histogram(data = df, \n                 aes(x = x, y = after_stat(density)), binwidth = se, boundary = true_effect, fill = \"grey\", alpha = bg_alpha) + \n  geom_vline(data = se_lines, aes(xintercept = se, \n                                  color = -dnorm(se_)), linetype = \"dashed\", alpha = bg_alpha) + \n  geom_label(data = se_lines, aes(x = se, y = Inf, label = label, group = NULL), vjust = 1.5, color = alpha('black', bg_alpha)) + \n  geom_label(data = se_lines, aes(x = ch_loc, y = 0, label = chance, group = NULL), vjust = -1, color = alpha('black', bg_alpha)) + \n  geom_vline(xintercept = 0) + \n  geom_function(fun = dnorm, args = list(mean = true_effect, sd = se), size = 1) + \n  theme_bw() + \n  labs(x = \"Estimate of Effect\",\n       y = \"Density\") + \n  geom_area(data = tibble(x = seq(1.64*se, 3*se + true_effect, by = 0.1)), aes(x = x), \n            stat = \"function\", fun = dnorm, args = list(mean = true_effect, sd = se),\n            fill = \"#d95f02\", alpha = 0.1, xlim = c(1.64*se, 3**se + true_effect)) + \n  annotate(\"label\", x = 1.3, y = .1, label = \"fraction rejected\", color = \"#d95f02\", size = 6) +\n  annotate(\"segment\", x = 1.64*se, xend = 1.64*se, y = 0, yend = dnorm(1.64*se, mean = true_effect, sd = se), color = \"#1b9e77\", size = 1) + \n  annotate(\"label\", x = 1.64*se, y = dnorm(1.64*se, mean = true_effect, sd = se)/2, label = \"1.64 SEs above zero\", color = \"#1b9e77\") + \n  annotate(\"segment\", x = 0, xend = 1.64*se, \n           y = dnorm(1.64*se, mean = true_effect, sd = se), \n           yend = dnorm(1.64*se, mean = true_effect, sd = se), \n           color = \"#7570b3\", size = 1) + \n  annotate(\"label\", x = 0.5*1.64*se, y = dnorm(1.64*se, mean = true_effect, sd = se), label = \"width of 90% CI\", color = \"#7570b3\") + \n  theme(legend.position = \"none\") + \n  xlim(-1, 3)\n\n\n\n\n\n\n\n\n\nYou’ll remember that “almost all” of the normal distribution falls within two standard errors of its mean. So as a starting point, let’s use this rule: get the sampling distribution two standard errors above 1.64 standard errors above zero. We can just add 1.64 and 2 together to get a sampling distribution 3.64 standard errors above zero. That is, we need \\(\\frac{\\text{true effect}}{\\text{standard error}} &gt; 3.64\\). If the true effect is larger than 3.64 standard errors, then the confidence interval will “rarely” overlap zero (about 2% of the time).\n\n\nCode\nlibrary(tidyverse)\n\n# study parameters\ntrue_effect &lt;- 1\nse &lt;- 1/3.84\n\nx &lt;- rnorm(500000, mean = true_effect, sd = se)\ndf &lt;- data.frame(x)\n\nggplot() + \n  geom_histogram(data = df, \n                 aes(x = x, y = after_stat(density)), binwidth = se, boundary = true_effect, fill = \"grey\", alpha = bg_alpha) + \n  geom_vline(data = se_lines, aes(xintercept = se, \n                                  color = -dnorm(se_)), linetype = \"dashed\", alpha = bg_alpha) + \n  geom_label(data = se_lines, aes(x = se, y = Inf, label = label, group = NULL), vjust = 1.5, color = alpha('black', bg_alpha)) + \n  geom_label(data = se_lines, aes(x = ch_loc, y = 0, label = chance, group = NULL), vjust = -1, color = alpha('black', bg_alpha)) + \n  geom_vline(xintercept = 0) + \n  geom_function(fun = dnorm, args = list(mean = true_effect, sd = se), size = 1) + \n  theme_bw() + \n  labs(x = \"Estimate of Effect\",\n       y = \"Density\") + \n  geom_area(data = tibble(x = seq(1.64*se, 3*se + true_effect, by = 0.1)), aes(x = x), \n            stat = \"function\", fun = dnorm, args = list(mean = true_effect, sd = se),\n            fill = \"#d95f02\", alpha = 0.1, xlim = c(1.64*se, 3**se + true_effect)) + \n  #annotate(\"label\", x = 1.3, y = .1, label = \"fraction rejected\", color = \"#d95f02\", size = 6) +\n  annotate(\"segment\", x = true_effect, xend = true_effect, y = 0, yend = dnorm(true_effect, mean = true_effect, sd = se), color = \"#d95f02\", size = 1) + \n  annotate(\"label\", x = true_effect, y = dnorm(true_effect, mean = true_effect, sd = se)/2, label = \"true effect\", color = \"#d95f02\") +   \n  annotate(\"segment\", x = 1.64*se, xend = 1.64*se, y = 0, yend = .75, color = \"#1b9e77\", size = 1) + \n  annotate(\"label\", x = 1.64*se, y = .75, label = \"1.64 SEs above zero\", color = \"#1b9e77\") + \n  annotate(\"segment\", x = 0, xend = 1.64*se, \n           y = .125, \n           yend = .125, \n           color = \"#7570b3\", size = 1,\n           lineend = \"round\", linejoin = \"round\", arrow = arrow(length = unit(0.1, \"inches\"), ends = \"both\")) + \n  annotate(\"label\", x = 0.5*1.64*se, y = .125, label = \"1.64 SEs\", color = \"#7570b3\", size = 4) + \n  annotate(\"segment\", x = true_effect, xend = 1.64*se, \n           y = .125, \n           yend = .125, \n           color = \"black\", size = 1,\n           lineend = \"round\", linejoin = \"round\", arrow = arrow(length = unit(0.1, \"inches\"), ends = \"both\")) + \n  annotate(\"label\", x = 1.64*se + (true_effect - 1.64*se)/2, y = .125\n  \n  , label = \"ideally 2 SEs\", color = \"black\", size = 4) + \n  annotate(\"segment\", x = true_effect, xend = 0, \n           y = 1, yend = 1, \n           color = \"black\", size = 1,\n           lineend = \"round\", linejoin = \"round\", arrow = arrow(length = unit(0.1, \"inches\"), ends = \"both\")) + \n  annotate(\"label\", x = true_effect/2, y = 1, label = \"ideally 3.64 SEs\", color = \"black\", size = 4) + \n  theme(legend.position = \"none\") + \n  xlim(-0.1, 2.0)"
  },
  {
    "objectID": "blog/2023-06-12-power-3-rule-of-364/index.html#the-resulting-guidelines",
    "href": "blog/2023-06-12-power-3-rule-of-364/index.html#the-resulting-guidelines",
    "title": "Power, Part III: The Rule of 3.64 for Statistical Power",
    "section": "The Resulting Guidelines",
    "text": "The Resulting Guidelines\nThere are a few ways to write the ratio:\n\n\\(\\frac{\\text{true effect}}{\\text{standard error}} &gt; 3.64\\)\n\\(\\text{true effect} &gt; 3.64 \\times \\text{standard error}\\)\n\\(\\text{standard error} &lt; \\frac{1}{3.64} \\times \\text{true effect} \\approx 0.27 \\times \\text{true effect}\\)\n\nThese are all the same goals. I prefer the first, but they are all equivalent targets for power.\nI like this ratio because it points to strategies to increase power. You can do two things:\n\nIncrease the true effect.\nDecrease the standard error.\n\nThis ratio gets you thinking not what your power is, but how to increase it. Again, power isn’t a task for the experimenter, it’s the task.5 Kane (2023)6 suggests several ways researchers can increase the true effect or decrease the standard error.\n5 I’m leaving aside the question of how to predict the standard error or choose a true effect for a paricular design.6 Kane, John V. 2023. “More Than Meets the ITT: A Guide for Investigating Null Results.” APSA Preprints. doi: 10.33774/apsa-2023-h4p0q-v2.I’ll highlight a few examples here. To increase the treatment effect, you can:\n\nIncrease attentiveness.\nEnsure that respondents are not pre-treated.\nWrite strong treatments or “hit them between the eyes” (Kuklinski et al. 2000).\n\nTo decrease the standard error, you can:\n\nIncrease the sample size.\nUse multiple measurements for the outcome.\nUse a pre-post design (Clifford, Sheagley, and Piston 2022)."
  },
  {
    "objectID": "blog/2023-06-12-power-3-rule-of-364/index.html#implication",
    "href": "blog/2023-06-12-power-3-rule-of-364/index.html#implication",
    "title": "Power, Part III: The Rule of 3.64 for Statistical Power",
    "section": "Implication",
    "text": "Implication\nIf you get the ratio to 3.64, then you’ll have 98%. This is much higher than the cutoff of 80% I hear suggested most often. If you want 80% power, rather than 98% power, you can change the 3.64 to 2.48.7 But notice that 3.64 and 2.48 are not very different. This has an important implication.\n7 Find this by adding 1.64 to -qnorm(0.2).A researcher can lower the risk of a failed study from about 1 in 5 (80% power) to about 1 in 50 (98% power) by increasing the ratio from 2.48 to 3.64. This requires increasing the treatment effect by about 50% or shrinking the standard error by about 33%. Stated differently, if you are careless and let your treatment effect fall by 33% or let your standard error increase by 50%, then your risk of a failed study increases 10-fold! Small differences can matter a lot.\nThe implication is this: when you are on the cusp of a well-powered study (about 80% power), then small increases in the ratio have a disproportionate impact on your risk of a failed study."
  },
  {
    "objectID": "blog/2023-06-12-power-3-rule-of-364/index.html#computing-power-from-the-ratio",
    "href": "blog/2023-06-12-power-3-rule-of-364/index.html#computing-power-from-the-ratio",
    "title": "Power, Part III: The Rule of 3.64 for Statistical Power",
    "section": "Computing Power from the Ratio",
    "text": "Computing Power from the Ratio\nFollowing the logic of the picture above, we need to compute the fraction of the sampling distributin that lies above 1.64 SEs. The pnorm() function returns normal probabilities, but below specfic thresholds. By supplying the argument lower.tail = FALSE, we can get the probabilities of falling above a specific threshold.\n\ntrue_effect &lt;- 1.00\nse &lt;- 0.4\n\n# compute power\npnorm(1.64*se,   # want fraction above 1.64 SE\n      mean = true_effect,  # mean of sampling distribution\n      sd = se,  # sd of sampling distribution\n      lower.tail = FALSE)  # fraction above, not below\n\n[1] 0.8051055"
  },
  {
    "objectID": "blog/2023-06-12-power-3-rule-of-364/index.html#summary",
    "href": "blog/2023-06-12-power-3-rule-of-364/index.html#summary",
    "title": "Power, Part III: The Rule of 3.64 for Statistical Power",
    "section": "Summary",
    "text": "Summary\nStatistical power is an abstract quantity. It’s easy to understand what it means, but harder to think about how to manipulate it. I explain why the target \\(\\frac{\\text{true effect}}{\\text{standard error}} &gt; 2.48\\) is equivalent to a target power of 80%. But you want to “almost always” reject the null, so you should shoot for \\(\\frac{\\text{true effect}}{\\text{standard error}} &gt; 3.64\\). Hopefully these guidelines help you build a bit of actionable intuition about statistical power."
  },
  {
    "objectID": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html",
    "href": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html",
    "title": "Power, Part II: What Do Confidence Intervals from Well-Powered Studies Look Like?",
    "section": "",
    "text": "A Paper\n\n\n\nThis post turned out to be somewhat popular, so I’ve written up a more formal, careful description of the idea in a full-length paper. You can find the preprint “Power Rules” here."
  },
  {
    "objectID": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#background",
    "href": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#background",
    "title": "Power, Part II: What Do Confidence Intervals from Well-Powered Studies Look Like?",
    "section": "Background",
    "text": "Background\nIn this post, I address confidence intervals that are nestled right up against zero.1 These intervals indicate that an estimate is “barely” significant. I want to be clear: “barely significant” is still significant, so you should still reject the null hypothesis.2\n1 This is the second post in a series. In my previous post, I mentioned two new papers that have me thinking about power: Arel-Bundock et al.’s “Quantitative Political Science Research Is Greatly Underpowered” and Kane’s “More Than Meets the ITT: A Guide for Investigating Null Results”. Go check out that post and those papers if you haven’t.2 I’m focusing on confidence intervals here because inference from confidence intervals is a bit more intuitive (see Rainey 2014 and Rainey 2015. In the cases I discuss, whether one checks whether the p-value is less than 0.05 or checks that confidence interval contains zero are equivalent.But I want to address a feeling that can come along with a confidence interval nestled right up against zero. A feeling of victory. It seems like a perfectly designed study. You rejected the null and collected just enough data to do it.\nBut instead, it should feel like a near-miss. Like an accident narrowly avoided. A confidence interval nestled right up against zero indicates that one of two things has happened: either you were (1) unlucky or (2) under-powered.\nBecause “unlucky” is always a possibility, we can’t learn much from a particular confidence interval, but we can learn a lot from a literature. A literature with well-powered studies produces confidence intervals that often fall far from zero. A well-powered literature does not produce confidence intervals that consistently nestle up against zero. Under-powered studies, though, do tend to produce confidence intervals that nestle right up against zero."
  },
  {
    "objectID": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#a-simulation",
    "href": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#a-simulation",
    "title": "Power, Part II: What Do Confidence Intervals from Well-Powered Studies Look Like?",
    "section": "A Simulation",
    "text": "A Simulation\nI’m going to explore the behavior of confidence intervals with a little simulation. In this simulation, I’m going to assert a standard error rather than create the standard error endogenously through sample size, etc. I use a true effect size of 0.5 and standard errors of 0.5, 0.3, 0.2, and 0.15 to create studies with 25%, 50%, 80%, and 95% power, respectively.3 I think of 80% as “minimally-powered” and 95% as “well-powered.”\n3 I’m ignoring how to choose the true effect, estimate the standard error, and compute power. For now, I’m placing all this behind the curtain. See Daniël Lakens’ book [Improving Your Statistical Inferences] for discussion (h/t Bermond Scoggins).I’m using a one-sided test (hypothesizing a positive effect), so I’ll use 90% confidence intervals with arms that are 1.64 standard errors wide. Let’s simulate some estimates from each of our four studies and compute their confidence intervals. I simulate 5,000 confidence intervals to explore below.\n\n\nCode\n# load packages\nlibrary(tidyverse)\n\n# create a parameter for the true effect\ntrue_effect &lt;- 0.5 # just assumed by me\n\n# create a data frame of standard errors (with approximate power)\nse_df &lt;- tribble(\n  ~se,    ~pwr,\n  0.5,    \"about 25% power\",\n  0.3,    \"about 50% power\",\n  0.2,    \"about 80% power\",\n  0.15,   \"about 95% power\"\n)\n\n# create function to simulate estimates for each standard error\nsimulate_estimates &lt;- function(se, pwr) {\n  tibble(\n    est = rnorm(n_cis, mean = true_effect, sd = se),\n    se = se,\n    pwr = pwr\n  )\n}\n\n# simulate the estimates, compute the confidence intervals, and wrangle\nn_cis &lt;- 5000  # the number of cis to create\nci_df &lt;- se_df %&gt;% \n  # simulate estimates\n  pmap_dfr(simulate_estimates) %&gt;%\n  # compute confidence intervals\n  mutate(lwr = est - 1.64*se, \n         upr = est + 1.64*se) %&gt;%\n  # summarize the location of the confidence interval\n  mutate(result = case_when(lwr &lt; 0 ~ \"Not significant\",\n                            lwr &lt; se ~ \"Nestled against zero\",\n                            lwr &gt;= se~ \"Not nestled against zero\"))\n\n\nNow let’s quickly confirm my power calculations by computing the proportion of confidence intervals to the right of zero. These are about right. In a later post, I’ll describe how I think about computing these quantities.\n\n\nCode\n# confirm power calculations\nci_df %&gt;%\n  group_by(se, pwr) %&gt;%\n  summarize(sim_pwr = 1 - mean(result == \"Not significant\"),\n            sim_pwr = scales::percent(sim_pwr, accuracy = 1)) %&gt;%\n    select(SE = se, \n         Power = pwr,\n         `Percent Significant` = sim_pwr) %&gt;%\n  kableExtra::kable(format = \"markdown\")\n\n\n\n\n\nSE\nPower\nPercent Significant\n\n\n\n\n0.15\nabout 95% power\n96%\n\n\n0.20\nabout 80% power\n80%\n\n\n0.30\nabout 50% power\n52%\n\n\n0.50\nabout 25% power\n26%"
  },
  {
    "objectID": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#what-do-confidence-intervals-from-well-powered-studies-look-like",
    "href": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#what-do-confidence-intervals-from-well-powered-studies-look-like",
    "title": "Power, Part II: What Do Confidence Intervals from Well-Powered Studies Look Like?",
    "section": "What Do Confidence Intervals from Well-Powered Studies Look Like?",
    "text": "What Do Confidence Intervals from Well-Powered Studies Look Like?\nNow let’s see what these confidence intervals look like. 5,000 is too many to plot, so I sample 25. But I applied the statistical significance filter first. This mimics the publication process and makes the plots a little easier to compare. My argument doesn’t depend on this filter, though.\nI plotted these 100 intervals below4\n4 4 studies x 25 simulated intervals per study = 100 intervals.There are three important vertical lines in these plots.\n\nThe solid line indicates zero. All confidence intervals are above zero because I applied the significance filter.\nThe dotted line indicates one standard error above zero. This varies across panels because the standard error varies across panels.\nThe dashed line indicates the true effect of 0.5. Because I applied the significance filter, the lower-powered studies are consistently over-estimating the true effect.\n\nThe intervals are green when the lower bound of the 90% confidence interval falls within one standard error of zero—that’s my definition of “nestled up against zero.” The intervals are orange when the lower bound falls further than one standard error above zero.\nNotice how low-powered studies tend to nestle their confidence intervals right up against zero. Almost all of the confidence intervals from the study with 25% power are nestled right up against zero. Very few of the confidence intervals from the study with 95% power are nestled up against zero.\nAgain, you should apply this standard to a literature. You should not apply this standard to a particular study because even well-powered studies sometimes produce confidence intervals that nestle up against zero. But when you start to see confidence intervals consistently falling close to zero, you should start to assume that the literature uses under-powered studies and that the estimates in that literature are inflated due to Type M errors (Gelman and Stern 2014).\n\n\nCode\ngg_df &lt;- ci_df %&gt;%\n  filter(lwr &gt; 0) %&gt;% # apply significance filter \n  # sample 25 intervals (from those that are significant)\n  group_by(se, pwr) %&gt;%\n  sample_n(25) %&gt;%\n  # create id (ordered by estimate value)\n  group_by(se, pwr) %&gt;%\n  arrange(est) %&gt;%\n  mutate(ci_id = 1:n())\n  \nggplot(gg_df, aes(x = est, xmin = lwr, xmax = upr, y = ci_id,\n                    color = result)) + \n  facet_wrap(vars(pwr), ncol = 1, scales = \"free_x\") + \n  geom_vline(data = se_df, aes(xintercept = se), linetype = \"dotted\") +\n  geom_vline(xintercept = 0) + \n  geom_vline(xintercept = true_effect, linetype = \"dashed\") + \n  geom_errorbarh(height = 0) + \n  geom_point() + \n  scale_color_brewer(type = \"qual\", palette = 2) + \n  theme_bw() + \n  labs(x = \"Estimate and 90% CI\",\n       y = NULL,\n       color = \"Result\")"
  },
  {
    "objectID": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#showing-this-another-way-density-of-the-lower-bounds",
    "href": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#showing-this-another-way-density-of-the-lower-bounds",
    "title": "Power, Part II: What Do Confidence Intervals from Well-Powered Studies Look Like?",
    "section": "Showing This Another Way: Density of the Lower Bounds",
    "text": "Showing This Another Way: Density of the Lower Bounds\nWe can also plot the density of the lower bounds of these 5,000 intervals. This approach shows the “nestling” most clearly. The plots below show that the lower bounds of confidence intervals tend to nestle close to zero when the power is low, and lie further from zero when the power is high.\n\n\nCode\ngg_df &lt;- ci_df %&gt;%\n  filter(lwr &gt; 0) # apply significance filter \nggplot(gg_df, aes(x = lwr)) + \n  facet_wrap(vars(pwr), scales = \"free_x\") + \n  geom_density(fill = \"grey50\") + \n  geom_vline(data = se_df, aes(xintercept = se), linetype = \"dotted\") + \n  theme_bw() + \n  labs(x = \"Location of Lower Bound of 90% CI\",\n       y = \"Density\",\n       color = \"Power\")"
  },
  {
    "objectID": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#showing-this-another-way-frequency-of-nestling",
    "href": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#showing-this-another-way-frequency-of-nestling",
    "title": "Power, Part II: What Do Confidence Intervals from Well-Powered Studies Look Like?",
    "section": "Showing This Another Way: Frequency of Nestling",
    "text": "Showing This Another Way: Frequency of Nestling\nLastly, I compute the percent of confidence intervals that are nestled right up against zero. For a well-powered study with 95% power, only about 1 in 5 confidence intervals nestle up against zero. For a poorly-powered study with 25% power, about 4 in 5 of confidence intervals nestle up against zero (among those that are above zero). The table below shows the remaining frequencies.\n\n\nCode\nci_df %&gt;%\n  group_by(se, pwr, result) %&gt;%\n  summarize(frac = n()/n_cis, .groups = \"drop\") %&gt;%\n  pivot_wider(names_from = result, values_from = frac) %&gt;%\n  mutate(`Nestled, given significant` = `Nestled against zero`/(1 - `Not significant`),\n         `Not nestled, given significant` = `Not nestled against zero`/(1 - `Not significant`)) %&gt;%\n  select(SE = se, \n         Power = pwr,\n         `Not significant`,\n         `Nestled against zero`,\n         `Not nestled against zero`,\n         `Nestled, given significant`,\n         `Not nestled, given significant`) %&gt;%\n  mutate(across(`Not significant`:`Not nestled, given significant`, ~ scales::percent(., accuracy = 1))) %&gt;%\n  kableExtra::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSE\nPower\nNot significant\nNestled against zero\nNot nestled against zero\nNestled, given significant\nNot nestled, given significant\n\n\n\n\n0.15\nabout 95% power\n4%\n20%\n76%\n20%\n80%\n\n\n0.20\nabout 80% power\n20%\n36%\n44%\n45%\n55%\n\n\n0.30\nabout 50% power\n48%\n36%\n17%\n68%\n32%\n\n\n0.50\nabout 25% power\n74%\n21%\n5%\n80%\n20%"
  },
  {
    "objectID": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#summary",
    "href": "blog/2023-05-25-power-2-what-do-confidence-intervals-look-like/index.html#summary",
    "title": "Power, Part II: What Do Confidence Intervals from Well-Powered Studies Look Like?",
    "section": "Summary",
    "text": "Summary\nIn this post, I address confidence intervals that are nestled right up against zero. These intervals can suggest a perfectly powered study—not too much, not too little. But instead, a confidence interval nestled right up against zero indicates that one of two things has happened: either you were (1) unlucky or (2) under-powered.\nBecause “unlucky” is always a possibility, we can’t learn much from a particular confidence interval, but we can learn a lot from a literature. A literature with well-powered studies produces confidence intervals that often fall far from zero. A well-powered literature does not produce confidence intervals that consistently nestle up against zero. Under-powered studies, though, do tend to produce confidence intervals that nestle right up against zero.\n\n\n\n\n\n\nKey Takeaway\n\n\n\nUnder-powered studies tend to produce confidence intervals that are nestled right up against zero. Well-powered studies tend to produce confidence intervals that fall further away. A literature that produces confidence intervals that consistently nestle right up against zero is likely a collection of under-powered studies."
  },
  {
    "objectID": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html",
    "href": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html",
    "title": "Power, Part I: Power Is for You, Not for Reviewer Two",
    "section": "",
    "text": "A Paper\n\n\n\nThis post turned out to be somewhat popular, so I’ve written up a more formal, careful description of the idea in a full-length paper. You can find the preprint “Power Rules” here."
  },
  {
    "objectID": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#background",
    "href": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#background",
    "title": "Power, Part I: Power Is for You, Not for Reviewer Two",
    "section": "Background",
    "text": "Background\nThere’s been some really good work lately on statistical power. I’ll point you to two really great papers.\n\nArel-Bundock, Vincent, Ryan C. Briggs, Hristos Doucouliagos, Marco Mendoza Aviña, and T.D. Stanley. 2022. “Quantitative Political Science Research Is Greatly Underpowered.” OSF Preprints. July 5. doi: 10.31219/osf.io/7vy2f.\nKane, John V. 2023. “More Than Meets the ITT: A Guide for Investigating Null Results .” APSA Preprints. doi: 10.33774/apsa-2023-h4p0q-v2.\n\nI’ve been long interested in statistical power (see Rainey 20141 and Rainey 20152), and these new papers have me thinking even more about the importance of power.\n1 Rainey, Carlisle. 2014. “Arguing for a Negligible Effect.” American Journal of Political Science 58(4): 1083-1091.2 McCaskey, Kelly and Carlisle Rainey. 2015. “Substantive Importance and the Veil of Statistical Significance.” Statistics, Politics, and Policy 6(1-2): 77-96.In this post, I argue that statistical power isn’t something ancillary. Power is primary. I also argue that power isn’t something you–the researcher–build to satisfy an especially cranky Reviewer 2, it’s something you do for yourself, to make sure that your study succeeds."
  },
  {
    "objectID": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#the-hypothesis-testing-framework",
    "href": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#the-hypothesis-testing-framework",
    "title": "Power, Part I: Power Is for You, Not for Reviewer Two",
    "section": "The Hypothesis Testing Framework",
    "text": "The Hypothesis Testing Framework\nIn the hypothesis testing framework, you consider two hypotheses: the null hypothesis and the alternative hypothesis.\nThe hypothesis test is all about arguing against the null hypothesis \\(H_0\\) (leaving the alternative \\(H_A\\) as the only remaining possibility). You will (try to) show that your data would be “unusual” if the null hypothesis were correct.3\n3 When hypothesizing about the average treatment effect (ATE), this can take a variety of forms. The form doesn’t really matter.If the data would NOT be unusual under the null hypothesis, then you do not reject the null hypothesis."
  },
  {
    "objectID": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#intepreting-a-failure-to-reject",
    "href": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#intepreting-a-failure-to-reject",
    "title": "Power, Part I: Power Is for You, Not for Reviewer Two",
    "section": "Intepreting a Failure to Reject",
    "text": "Intepreting a Failure to Reject\nA failure to reject means that the data “would not be unusual under the null hypothesis.” This does not imply that you should conclude the data are only consistent with the null. Indeed, there is a sharp asymmetry in hypothesis testing. I describe this in my 2014 AJPS:\n\nPolitical scientists commonly interpret a lack of statistical significance (i.e., a failure to reject the null) as evidence for a negligible effect (Gill 1999), but this approach acts as a broken compass… If the sample size is too small, the researcher often concludes that the effect is negligible even though the data are also consistent with large, meaningful effects. This occurs because the small sample leads to a large confidence interval, which is likely to contain both “no effect” and large effects.\n\nGill (1999)4 describes this more forcefully:\n4 Gill, Jeff. 1999. “The Insignificance of Null Hypothesis Significance Testing.” Political Research Quarterly 52(3): 647-674.\nWe teach graduate students to be very careful when describing the occurrence of not rejecting the null hypothesis. This is because failing to reject the null hypothesis does not rule out an infinite number of other competing research hypotheses. Null hypothesis significance testing is asymmetric: if the test statistic is sufficiently atypical given the null hypothesis then the null hypothesis is rejected, but if the test statistic is insufficiently atypical given the null hypothesis then the null hypothesis is not accepted. This is a double standard: H1 is held innocent until proven guilty and Ho is held guilty until proven innocent (Rozeboom 1960)…\n\n\nThere are two problems that develop as a result of asymmetry. The first is a misinterpretation of the asymmetry to assert that finding a non-statistically significant difference or effect is evidence that it is equal to zero or nearly zero. Regarding the impact of this acceptance error Schmidt (1996: 126) asserts that this: “belief held by many researchers is the most devastating of all to the research enterprise.” This acceptance of the null hypothesis is damaging because it inhibits the exploration of competing research hypotheses. The second problem pertains to the correct interpretation of failing to reject the null hypotheses. Failing to reject the null hypothesis essentially provides almost no information about the state of the world. It simply means that given the evidence at hand one cannot make an assertion about some relationship: all you can conclude is that you can’t conclude that the null was false (Cohen 1962).\n\nThere are many incorrect, but somewhat innocent interpretations of p-values. Interpreting a lack of statistical significance as evidence for the null is incorrect and wildly misleading in many cases.\n\n\n\n\n\n\nImportant Point\n\n\n\nA non-statistically significant difference is not evidence that an effect is equal to zero or nearly zero. Interpreting a non-statistically significant effect otherwise is “devastating.”"
  },
  {
    "objectID": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#the-implication-of-a-non-conclusion",
    "href": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#the-implication-of-a-non-conclusion",
    "title": "Power, Part I: Power Is for You, Not for Reviewer Two",
    "section": "The Implication of a Non-Conclusion",
    "text": "The Implication of a Non-Conclusion\nIf you cannot draw a conclusion then, what exactly has happened? Obtaining \\(p &gt; 0.05\\) will not be an “error” because you won’t make a strong claim that the research hypothesis is wrong. Instead, you will simply admit that you failed to uncover evidence against the null. Failing to uncover evidence isn’t an error.\nIndeed, Jones and Tukey (2000)5 write:\n5 Jones, Lyle V., and John W. Tukey. 2000. “A Sensible Formulation of the Significance Test.” Psychological Methods 5(4): 411-414.\nA conclusion is in error only when it is “a reversal,” when it asserts one direction while the (unknown) truth is the other direction. Asserting that the direction is not yet established may constitute a wasted opportunity, but it is not an error.\n\n\n\n\n\n\n\nImportant Point\n\n\n\nFailing to uncover evidence isn’t an “error,” it is a “wasted effort.”\n\n\nThis is worth emphasizing in a different way. Tests are not magical tools that tell you which hypothesis is correct. Instead, tests summarize the evidence against the null. There are two critical pieces to “evidence against the null”: (1) the amount of evidence and (2) whether the evidence is against the null. If you buy your own argument that the null is false (surely you do!), then (2) is taken care of. Only the amount of evidence remains, and you–the researcher–choose the amount of evidence to supply."
  },
  {
    "objectID": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#the-implication-for-power-calculations",
    "href": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#the-implication-for-power-calculations",
    "title": "Power, Part I: Power Is for You, Not for Reviewer Two",
    "section": "The Implication for Power Calculations",
    "text": "The Implication for Power Calculations\nThis perspective helps motivate power calculations. By their design, tests control the error rate in certain situations (when then null is correct). You do not need to worry about Type I errors. First, the test controls the error rate under the null. Second, you are pretty sure the null is wrong (see your theory section).\n\n\n\n\n\n\nImportant Point\n\n\n\nThe hypothesis test takes care of the the Type I error rate. If you choose a properly-sized test, you don’t need to worry about those errors any more.\n\n\nIf you aren’t worried about Type I errors, what are you worried about? They only thing left to worry about is wasting your time and money. Statistical power is the chance not of wasting your time and money.\nPower isn’t a secondary quantity that you compute for thoroughness or in anticipation of a comment from Reviewer 2. Power is something that you build for yourself.\nStatisticians talk a lot about Type I errors because that’s their contribution. It’s your job to bring the power.\nAnd importantly, power is under your control. Kane provides a rich summary of ways to increase the power of your experiment. At a minimum, you have brute force control through sample size.\nPower isn’t an ancillary concern, it’s the entire game from the very beginning of the planning stage. It should be at the forefront of the researchers mind from the very beginning. You should want the power as high as possible.6\n6 I hear that 80% is the standard, but I’m pretty uncomfortable spending dozens of hours and thousands of dollars running for a 1 in 5 chance of wasting my time. I want that chance as close to zero as I can get it. I want power close to 100%. 99% power and 80% power might both seem “high” or “acceptable,” but these are not the same. 80% power means 1 in 5 studies fail. 99% power means that 1 in 100 studies fail.You have to supply a test overwhelming evidence to consistently reject the null. Careful power calculations help you make sure you succeed in this war against the null.\nPower isn’t about Type S and M errors (Gelman and Carlin 2014)7. Power is about you protecting yourself from a failed study. And that seems like a protection worth pursuing carefully.8\n7 Gelman, Andrew, and John Carlin. “Beyond Power Calculations: Assessing Type S (Sign) and Type M (Magnitude) Errors.” Perspectives on Psychological Science 9(6): 641-651.8 Of course it’s also about Type S and M errors, but those are discipline-level concerns. I’m talking about your incentives as a researcher."
  },
  {
    "objectID": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#summary",
    "href": "blog/2023-05-22-power-1-for-you-not-reviewer-2/index.html#summary",
    "title": "Power, Part I: Power Is for You, Not for Reviewer Two",
    "section": "Summary",
    "text": "Summary\nHere are the takeaways:\n\nStatistical power is the chance of using your time and money productively (i.e., not wasting it).\nStatistical power is under your control (see Kane).\nYour power might be (much) lower than you think–you should check (see Arel-Bundock et al.).\nPower should be a primary concern throughout the design. The researcher should care deeply about power, perhaps more than anything else.\n\n\n\n\n\n\n\nImportant Point\n\n\n\nThe hypothesis test is no oracle. It will not consistently reject the null (even when the null is wrong) unless you supply overwhelming evidence. In experimental design, that’s not a task, that’s the task."
  },
  {
    "objectID": "blog/2023-06-01-teaching-confidence-intervals-with-gganimate/index.html",
    "href": "blog/2023-06-01-teaching-confidence-intervals-with-gganimate/index.html",
    "title": "Teaching Confidence Intervals and Hypothesis Testing with gganimate",
    "section": "",
    "text": "When I give students formula for confidence intervals, I find that students don’t have a sharp concept of how those confidence intervals work—even if I explain the components of the formula well.\nEven though they understand—seemingly very well—that the point estimate is noisy, they struggle to conceptualize that a confidence interval can often include values on the incorrect side of zero. Stated differently, they have a hard time understanding how a hypothesis test can fail to reject the null (when the null is incorrect). Their intuition suggests that a “test” should give you the correct answer.\nBecause their instincts are wrong, I want to undermine their trust in hypothesis tests. I want them to feel the riskiness of poorly-powered experiments that consistently nestle confidence intervals right up against zero. I want them ready and eager to work hard to avoid that risk—to make sure they have adequate statistical power.\nTo help undermine their confidence in confidence intervals, I like three exercises that mimic a test with 80% power. In each case, we are assuming that we have formulated a correct hypothesis and designed an excellent experiment with 80% power.\n\nFirst, I have students roll a six-sided die. If the die produces a , then their study fails—they wasted their opportunity. See this post for details on this perspective. This simulates the riskiness of an experiment with 80% power quite well. They get lots of failed experiments in a short period of time. They become well-aware of the possibility of a failed study and grow increasingly interested in reducing this risk.\nSecond, I use a computer to produce a plot of many (about 50 seems right) confidence intervals from the same repeated study. I explain that the interval we will get in the study we actually conduct is like a random draw from this collection. (See below for an example of this figure.)\nThird, I make the plot dynamic. I find that dynamics make the plot more memorable and convey the (appropriate) idea that hypothesis tests and confidence intervals are chaotic, noisy quantities.\n\nThese exercises make it clear that failed studies are real possibilities. Hopefully they clearly see and “feel”:\n\nThe hypothesis test is no oracle. It will not consistently reject the null (even when the null is wrong) unless you supply overwhelming evidence. In experimental design, that’s not a task, that’s the task.\n\nBelow, I walk through the plots I use in parts 2 and 3."
  },
  {
    "objectID": "blog/2023-06-01-teaching-confidence-intervals-with-gganimate/index.html#background",
    "href": "blog/2023-06-01-teaching-confidence-intervals-with-gganimate/index.html#background",
    "title": "Teaching Confidence Intervals and Hypothesis Testing with gganimate",
    "section": "",
    "text": "When I give students formula for confidence intervals, I find that students don’t have a sharp concept of how those confidence intervals work—even if I explain the components of the formula well.\nEven though they understand—seemingly very well—that the point estimate is noisy, they struggle to conceptualize that a confidence interval can often include values on the incorrect side of zero. Stated differently, they have a hard time understanding how a hypothesis test can fail to reject the null (when the null is incorrect). Their intuition suggests that a “test” should give you the correct answer.\nBecause their instincts are wrong, I want to undermine their trust in hypothesis tests. I want them to feel the riskiness of poorly-powered experiments that consistently nestle confidence intervals right up against zero. I want them ready and eager to work hard to avoid that risk—to make sure they have adequate statistical power.\nTo help undermine their confidence in confidence intervals, I like three exercises that mimic a test with 80% power. In each case, we are assuming that we have formulated a correct hypothesis and designed an excellent experiment with 80% power.\n\nFirst, I have students roll a six-sided die. If the die produces a , then their study fails—they wasted their opportunity. See this post for details on this perspective. This simulates the riskiness of an experiment with 80% power quite well. They get lots of failed experiments in a short period of time. They become well-aware of the possibility of a failed study and grow increasingly interested in reducing this risk.\nSecond, I use a computer to produce a plot of many (about 50 seems right) confidence intervals from the same repeated study. I explain that the interval we will get in the study we actually conduct is like a random draw from this collection. (See below for an example of this figure.)\nThird, I make the plot dynamic. I find that dynamics make the plot more memorable and convey the (appropriate) idea that hypothesis tests and confidence intervals are chaotic, noisy quantities.\n\nThese exercises make it clear that failed studies are real possibilities. Hopefully they clearly see and “feel”:\n\nThe hypothesis test is no oracle. It will not consistently reject the null (even when the null is wrong) unless you supply overwhelming evidence. In experimental design, that’s not a task, that’s the task.\n\nBelow, I walk through the plots I use in parts 2 and 3."
  },
  {
    "objectID": "blog/2023-06-01-teaching-confidence-intervals-with-gganimate/index.html#an-experiment-that-we-can-repeat",
    "href": "blog/2023-06-01-teaching-confidence-intervals-with-gganimate/index.html#an-experiment-that-we-can-repeat",
    "title": "Teaching Confidence Intervals and Hypothesis Testing with gganimate",
    "section": "An Experiment that We Can Repeat",
    "text": "An Experiment that We Can Repeat\nFirst, let’s choose to conduct an experiment with 80% power. To get this, we’ll suppose that the true effect is 1 and the standard error is 0.4. To obtain 80% power, you can use the guideline that the standard error should be about 40% of the true effect (or the true effect divided by 2.48). I’ll show where these guidelines come from in a future post. With the true effect and standard error in hand, we can compute the long-run properties of the experiment.\n\n\nCode\nlibrary(tidyverse)\n\n# study parameters\ntrue_effect &lt;- 1\nse &lt;- 0.40  # 1/2.48\n\n# identify effects of interest\neoi &lt;- tribble(\n  ~Effect, ~Description,\n  true_effect, \"True Effect (known in this exercise)\"\n) \n\n# compute quantities of interest regarding power\neoi %&gt;%\n  mutate(Power = 1 - pnorm(1.64*se, Effect, se),\n         Power = scales::percent(Power, accuracy = 1),\n         `Type S` = retrodesign::type_s(Effect, se)$type_s,\n         `Type S` = scales::number(`Type S`, accuracy = 0.01),\n         `Type M` = retrodesign::type_m(Effect, se)$type_m,\n         `Type M` = scales::number(`Type M`, accuracy = 0.01),\n         Effect = scales::number(Effect, accuracy = 0.01)) %&gt;% \n  pivot_longer(cols = Effect:`Type M`) %&gt;%\n  kableExtra::kable(format = \"markdown\", col.names = NULL)\n\n\n\n\n\nEffect\n1.00\n\n\nDescription\nTrue Effect (known in this exercise)\n\n\nPower\n81%\n\n\nType S\n0.00\n\n\nType M\n1.20"
  },
  {
    "objectID": "blog/2023-06-01-teaching-confidence-intervals-with-gganimate/index.html#a-static-plot",
    "href": "blog/2023-06-01-teaching-confidence-intervals-with-gganimate/index.html#a-static-plot",
    "title": "Teaching Confidence Intervals and Hypothesis Testing with gganimate",
    "section": "A Static Plot",
    "text": "A Static Plot\nBut these long-run properties remain a bit abstract and seem “distant” from the practical implications of our particular experiment. This is where the three exercises above come in handy.\nThe second exercise is a static plot. The plot shows 50 intervals; we can see that several include zero. These are wasted opporunities. We set out to reject a null hypothesis that the effect was less than or equal to zero, and we failed to do that.\n\n\nCode\n# number of studies to simulate\nn_studies &lt;- 50\n\n# a data frame of studies \nset.seed(123)\nests &lt;- tibble(study_id = 1:n_studies,\n               est = c(rnorm(n_studies, true_effect, se))) %&gt;%\n  mutate(reject_null = ifelse(est - 1.64*se &gt; 0, \"Yes\", \"No\")) \n\n# plot the confidence intervals for each study\ngg &lt;- ggplot(ests, aes(x = est, \n                         y = study_id, \n                         xmin = est - 1.64*se, \n                         xmax = est + 1.64*se, \n                         color = reject_null)) + \n  geom_vline(xintercept = true_effect, \n             linetype = \"dotted\") + \n  geom_vline(xintercept = 0) + \n  geom_point() + \n  geom_errorbarh(height = 0) + \n  geom_rug(sides = \"b\", \n           aes(x = est - 1.64*se, color = NULL), \n           alpha = 0.5, \n           length = unit(0.025, \"npc\")) + \n  scale_color_manual(values = c(\"No\" = \"#d95f02\", \"Yes\" = \"#1b9e77\")) +  # from https://colorbrewer2.org/#type=qualitative&scheme=Dark2&n=3\n  theme_bw() +\n    theme(panel.grid = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.title.y = element_blank()) + \n  labs(x = \"Estimate and 90% Confidence Interval\",\n       y = \"Study ID\", \n       color = \"Reject Null?\", \n       caption = \"Rug shows distribution of lower bound of confidence interval.\") \n\n# print plot\nprint(gg)"
  },
  {
    "objectID": "blog/2023-06-01-teaching-confidence-intervals-with-gganimate/index.html#dynamic-plot",
    "href": "blog/2023-06-01-teaching-confidence-intervals-with-gganimate/index.html#dynamic-plot",
    "title": "Teaching Confidence Intervals and Hypothesis Testing with gganimate",
    "section": "Dynamic Plot",
    "text": "Dynamic Plot\nThe static plot above is nice, but doesn’t convey an appropriate sense of randomness or “you don’t know what you’ll get this time.” To convey this feeling, I like to add dynamics. In particular, I like a confidence interval that’s moving and you’re not sure if it will cover zero or not. This conveys the sense that “something bad might happen” with each draw. Even though only about 10 of the 50 confidence intervals will cross zero, you feel the danger on all 50 simulations.\nIn designing this plot, two features are in tension:\n\nThe plot conveys the ideas.\nThe code is easy to update and understand.\n\nIn the past, I’ve prioritized making the plot look exactly like I want. But there are concrete downsides to using hacky solutions—using functions in ways not intended. The code is brittle and difficult. For examples that clearly convey the ideas, see Presidential Plinko and this “raindrop” plot. These are great ways to convey the randomness, but producing these plots requires some “tedious” coding.\nWith this code, I tried to illustrate the concept well while avoid hacky solutions to minor problems. This makes the code easier to understand, change, and update.\nFirst, let’s start by creating the data frame to plot. We need to make two small changes to the data frame above. These are both hacks, but worth it.\n\nAdd a dummy row to the data frame to trick gganimate into starting with an empty plot.\nAdd a grouping variable to indicate the states for the transitions. This is simply a row ID variable.\n\n\n\nCode\n# load packages\nlibrary(gganimate)\n\n# add two things to the data frame of confidence intervals\n# 1. an initial row with study_id = 1 and est = NA so that \n#    the plot starts empty (gganimate would start with the \n#    first observation in place otherwise).\n# 2. a group variable that defines the row. This is the same\n#    as the study_id, except the dummy row from (1) and the \n#    actual first row have different groups.\nanimate_data &lt;- bind_rows(\n  tibble(study_id = 1, est = NA),  # study_id = 1, est = NA\n  ests                             # combine dummy row with ests data frame from above\n  ) %&gt;%\n  mutate(group = 1:n())            # group (row index)\n\n\nNow let’s plot the confidence intervals much like above, except with expanded scales to give some more room for movement.\n\n\nCode\n# same ggplot, except three annotated changes\n1gg_exp &lt;- ggplot(animate_data, aes(x = est,\n                         y = study_id, \n                         xmin = est - 1.64*se, \n                         xmax = est + 1.64*se, \n                         color = reject_null, \n2                         group = group)) +\n  geom_vline(xintercept = true_effect, \n             linetype = \"dotted\") + \n  geom_vline(xintercept = 0) + \n  geom_point() + \n  geom_errorbarh(height = 0) + \n  geom_rug(sides = \"b\", \n           aes(x = est - 1.64*se, color = NULL), \n           alpha = 0.5, \n           length = unit(0.025, \"npc\")) + \n  scale_color_manual(values = c(\"No\" = \"#d95f02\", \"Yes\" = \"#1b9e77\")) +  \n  theme_bw() +\n    theme(panel.grid = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.title.y = element_blank()) + \n  labs(x = \"Estimate and 90% Confidence Interval\",\n       y = \"Study ID\", \n       color = \"Reject Null?\", \n       caption = \"Rug shows distribution of lower bound of confidence interval.\n                  Solid lines shows zero.\n                  Dotted line shows the true effect.\") + \n  # new scales here\n3  scale_x_continuous(expand = expansion(add = c(0.6, 1))) +\n  scale_y_continuous(expand = expansion(add = 2))\n  \n\n# print plot\nprint(gg_exp)\n\n\n\n1\n\nUse dataset with group variable.\n\n2\n\nSet group explicitly.\n\n3\n\nExpand x- and y-axis.\n\n\n\n\n\n\n\n\n\n\n\nNow let’s add the animation. I like the confidence intervals to be shooting toward zero. This gives the feeling that “it might cross!” and makes the fear of a failed study real for each simulation.\n\n\nCode\n# add dyamics to the plot\n1anim &lt;- gg_exp +\n2  transition_states(states = group) +\n  # how points enter\n3  enter_drift(x_mod = 2) +\n4  enter_grow() +\n5  enter_recolor(color = \"black\") +\n6  ease_aes(color = \"exponential-in\") +\n  # how points exit/remain\n7  exit_fade(alpha = 0.3) +\n8  shadow_mark(alpha = 0.3)\n\n# make magic happen!\nanimate(anim, duration = n_studies, fps = 10, \n        height = 6, width = 8, units = \"in\", res = 150)\n\n\n\n1\n\n‘gg’ is created earlier. It’s the plot we want to make dynamic.\n\n2\n\nThe transition_states() function from gganimate creates an animation transitioning between different states of the data. In this case, the argument states is set to group. This makes each group (each row in the data frame animate_data) appear in the plot, one at a time.\n\n3\n\nThe enter_drift() function describes how new data points enter the frame. x_mod = 2 makes new data enter by drifting along the x-axis 2 points from the right of their ending position.\n\n4\n\nThe enter_grow() function describes how new data points enter the frame. In this case, it makes them will grow from a size of 0 to their ending size.\n\n5\n\nThe enter_recolor() function again describes how new data points should enter the frame. Here, it makes the points and CIs change the color from black to their ending color as they appear. I want the final color to be a bit of a surprise, so I start them as black.\n\n6\n\nThe ease_aes() function determines how the aesthetics of the points change over the transitions. color = \"exponential-in\" means the color change will be at an exponential rate at the start of the transition. This makes the color change really fast at the end of the transition to maintain the surprise of the result.\n\n7\n\nThe exit_fade() function describes how data points leave the frame. alpha = 0.3 specifies that points will fade out to 30% transparency when they exit (when the next data point reaches its position).\n\n8\n\nshadow_mark() function keeps past data points in the frame. alpha = 0.3 sets the transparency of the shadow marks to 30% to match the exit_fade() transparency."
  },
  {
    "objectID": "blog/2023-06-01-teaching-confidence-intervals-with-gganimate/index.html#summary",
    "href": "blog/2023-06-01-teaching-confidence-intervals-with-gganimate/index.html#summary",
    "title": "Teaching Confidence Intervals and Hypothesis Testing with gganimate",
    "section": "Summary",
    "text": "Summary\nThe dynamic plot above is a good tool to help students understand that confidence intervals can quite easily include values on the wrong side of zero. Stated differently, it’s easy for a hypothesis test to fail to reject the null (when the null is wrong). Their intuition suggests that the test should tell you the correct answer.\nThe exercises above (appropriately) undermine their trust in hypothesis tests. I want them to feel the riskiness of poorly-powered experiments that consistently nestle confidence intervals right up against zero. I want them ready to work hard to avoid that risk—to make sure they have statistical power. Hopefully, they see that carefully building power into your experiment isn’t a task, it’s the task of experimental design.\nAs a concluding example, here’s the same dynamic plot for a study with about 98% power. Notice how “safe” this study feels compared to the one above with 80% power. I think this plot does a good job a translating probabilities into an appropriate sense of “danger.”\n\n\nCode\n# study parameters\nse &lt;- 0.27  # 1/3.64\n\n# identify effects of interest\neoi &lt;- tribble(\n  ~Effect, ~Description,\n  true_effect, \"True Effect (known in this exercise)\"\n) \n\n# compute quantities of interest regarding power\neoi %&gt;%\n  mutate(Power = 1 - pnorm(1.64*se, Effect, se),\n         Power = scales::percent(Power, accuracy = 1),\n         `Type S` = retrodesign::type_s(Effect, se)$type_s,\n         `Type S` = scales::number(`Type S`, accuracy = 0.01),\n         `Type M` = retrodesign::type_m(Effect, se)$type_m,\n         `Type M` = scales::number(`Type M`, accuracy = 0.01),\n         Effect = scales::number(Effect, accuracy = 0.01)) %&gt;% \n  pivot_longer(cols = Effect:`Type M`) %&gt;%\n  kableExtra::kable(format = \"markdown\", col.names = NULL)\n\n# a data frame of studies \nests &lt;- tibble(study_id = 1:n_studies,\n               est = c(rnorm(n_studies, true_effect, se))) %&gt;%\n  mutate(reject_null = ifelse(est - 1.64*se &gt; 0, \"Yes\", \"No\")) \n\nanimate_data &lt;- bind_rows(\n  tibble(study_id = 1, est = NA),  # study_id = 1, est = NA\n  ests                             # combine dummy row with ests data frame from above\n  ) %&gt;%\n  mutate(group = 1:n())            # group (row index)\n\n# same ggplot, except three annotated changes\ngg_exp &lt;- ggplot(animate_data, aes(x = est,\n                         y = study_id, \n                         xmin = est - 1.64*se, \n                         xmax = est + 1.64*se, \n                         color = reject_null, \n                         group = group)) +\n  geom_vline(xintercept = true_effect, \n             linetype = \"dotted\") + \n  geom_vline(xintercept = 0) + \n  geom_point() + \n  geom_errorbarh(height = 0) + \n  geom_rug(sides = \"b\", \n           aes(x = est - 1.64*se, color = NULL), \n           alpha = 0.5, \n           length = unit(0.025, \"npc\")) + \n  scale_color_manual(values = c(\"No\" = \"#d95f02\", \"Yes\" = \"#1b9e77\")) +  \n  theme_bw() +\n    theme(panel.grid = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        axis.title.y = element_blank()) + \n  labs(x = \"Estimate and 90% Confidence Interval\",\n       y = \"Study ID\", \n       color = \"Reject Null?\", \n       caption = \"Rug shows distribution of lower bound of confidence interval.\n                  Solid line shows zero.\n                  Dotted line shows the true effect.\") + \n  # new scales here\n  scale_x_continuous(expand = expansion(add = c(0.6, 1))) +\n  scale_y_continuous(expand = expansion(add = 2))\n\n# add dyamics to the plot\nanim &lt;- gg_exp +\n  transition_states(states = group) +\n  # how points enter\n  enter_drift(x_mod = 2) +\n  enter_grow() +\n  enter_recolor(color = \"black\") +\n  ease_aes(color = \"exponential-in\") +\n  # how points exit/remain\n  exit_fade(alpha = 0.3) +\n  shadow_mark(alpha = 0.3)\n\n# make magic happen!\nanimate(anim, duration = n_studies, fps = 10, \n        height = 6, width = 8, units = \"in\", res = 150)\n\n\n\n\n\nEffect\n1.00\n\n\nDescription\nTrue Effect (known in this exercise)\n\n\nPower\n98%\n\n\nType S\n0.00\n\n\nType M\n1.02"
  },
  {
    "objectID": "blog/2023-08-11-benchmarking-firth/index.html",
    "href": "blog/2023-08-11-benchmarking-firth/index.html",
    "title": "Benchmarking Firth’s Logit: {brglm2} versus {logistf}",
    "section": "",
    "text": "I like Firth’s logistic regression model (Firth 1993). I talk about that in Rainey and McCaskey (2021) and this Twitter thread. Kosmidis and Firth (2021) offer an excellent, recent follow-up as well.\nI’ll refer you to the papers for a careful discussion of the benefits, but Firth’s penalty reduces the bias and variance of the logit coefficients."
  },
  {
    "objectID": "blog/2023-08-11-benchmarking-firth/index.html#firths-logit",
    "href": "blog/2023-08-11-benchmarking-firth/index.html#firths-logit",
    "title": "Benchmarking Firth’s Logit: {brglm2} versus {logistf}",
    "section": "",
    "text": "I like Firth’s logistic regression model (Firth 1993). I talk about that in Rainey and McCaskey (2021) and this Twitter thread. Kosmidis and Firth (2021) offer an excellent, recent follow-up as well.\nI’ll refer you to the papers for a careful discussion of the benefits, but Firth’s penalty reduces the bias and variance of the logit coefficients."
  },
  {
    "objectID": "blog/2023-08-11-benchmarking-firth/index.html#goals-for-benchmarking",
    "href": "blog/2023-08-11-benchmarking-firth/index.html#goals-for-benchmarking",
    "title": "Benchmarking Firth’s Logit: {brglm2} versus {logistf}",
    "section": "Goals for Benchmarking",
    "text": "Goals for Benchmarking\nIn this post, I want to compare the brglm2 and logistf packages. Which fits logistic regression models with Firth’s penalty the fastest?\nThese packages both fit the models almost instantly, so there is no practical difference when fitting just one model. But large Monte Carlo simulations (or perhaps bootstraps), small differences might add up to a substantial time difference.\nHere, I benchmark the two packages for fitting logistic regression models with Firth’s penalty in a small sample–the results might not generalize to a larger sample. The data set comes from Weisiger (2014) (see ?crdata::weisiger2014). It has only 35 observations.\nYou can find the benchmarking code as a GitHub Gist."
  },
  {
    "objectID": "blog/2023-08-11-benchmarking-firth/index.html#benchmarking",
    "href": "blog/2023-08-11-benchmarking-firth/index.html#benchmarking",
    "title": "Benchmarking Firth’s Logit: {brglm2} versus {logistf}",
    "section": "Benchmarking",
    "text": "Benchmarking\nI benchmark four methods here.\n\nA vanilla glm() logit model.\nA Firth’s logit via brglm2 by supplying method = brglm2::brglmFit to glm().\nA Firth’s logit via logistf via logistf() using the default settings.\nA Firth’s logit via logistf via logistf() with the argument pl = FALSE. This argument is important because it skips hypothesis testing using profile likelihoods, which are computationally costly.\n\n\n# install crdata package to egt weisiger2014 data set\nremotes::install_github(\"carlislerainey/crdata\")\n\n# load packages\nlibrary(tidyverse)\nlibrary(brglm2)\nlibrary(logistf)\nlibrary(microbenchmark)\n\n\n# load data\nweis &lt;- crdata::weisiger2014\n\n# rescale weisiger2014 explanatory variables using arm::rescale()\nrs_weis &lt;- weis %&gt;%\n  mutate(across(polity_conq:coord, arm::rescale)) \n\n# create functions to fit models\nf &lt;- resist ~ polity_conq + lndist + terrain + soldperterr + gdppc2 + coord\nf1 &lt;- function() {\n  glm(f, data = rs_weis, family = \"binomial\")\n}\nf2 &lt;- function() {\n  glm(f, data = rs_weis, family = \"binomial\", method = brglmFit)\n}\nf3 &lt;- function() {\n  logistf(f, data = rs_weis)\n}\nf4 &lt;- function() {\n  logistf(f, data = rs_weis, pl = FALSE)\n}\n\n# do benchmarking\nbm &lt;- microbenchmark(\"regular glm()\" = f1(), \n               \"brglm2\" = f2(), \n               \"logistf (default)\" = f3(),\n               \"logistf (w/ pl = FALSE)\" = f4(),\n               times = 100) \n\nWarning in microbenchmark(`regular glm()` = f1(), brglm2 = f2(), `logistf\n(default)` = f3(), : less accurate nanosecond times to avoid potential integer\noverflows\n\nprint(bm)\n\nUnit: microseconds\n                    expr      min        lq      mean    median        uq\n           regular glm()  500.651  547.0425  591.5304  560.0805  584.4550\n                  brglm2 2469.266 2546.9405 2735.6438 2591.5690 2672.3800\n       logistf (default) 3303.329 3432.6635 3942.9122 3481.6585 3558.1235\n logistf (w/ pl = FALSE)  513.197  553.3155  594.6205  570.8225  608.9115\n       max neval\n  2713.708   100\n  9735.983   100\n 17672.148   100\n  1698.343   100\n\n\nIn short, logistf is slower than brglm2, but only because it computes the profile likelihood p-values by default. Once we skip those calculations using pl = FALSE, logistf is much faster. On average, it’s faster than glm(), because glm() has the occasional really slow computation.\nHere’s a plot showing the computation times of the four fits. Remember that all of these are computed practically instantly, so it only makes a difference when the fits are done thousands of times, like in a Monte Carlo simulation.\n\n# plot times\nbm %&gt;%\n  group_by(expr) %&gt;%\n  summarize(avg_time = mean(time)*10e-5) %&gt;%  # convert to milliseconds\n  ggplot(aes(x = fct_rev(expr), y = avg_time)) + \n  geom_col() + \n  labs(x = \"Method\", \n       y = \"Avg. Time (in milliseconds)\") + \n  coord_flip()"
  },
  {
    "objectID": "blog/2023-08-11-benchmarking-firth/index.html#computer",
    "href": "blog/2023-08-11-benchmarking-firth/index.html#computer",
    "title": "Benchmarking Firth’s Logit: {brglm2} versus {logistf}",
    "section": "Computer",
    "text": "Computer\nHere’s the info on my machine.\n\nsystem(\"sysctl -n machdep.cpu.brand_string\", intern = TRUE)\n\n[1] \"Apple M2 Max\""
  },
  {
    "objectID": "blog/2023-08-18-equivalence-tests/index.html",
    "href": "blog/2023-08-18-equivalence-tests/index.html",
    "title": "Equivalence Tests Using {marginaleffects}",
    "section": "",
    "text": "I remember sitting in a talk while I was a graduate student, and the speaker said something like: “I expect no effect here, and, just as I expected, the difference is not statistically significant.” Of course, that’s not a compelling argument for a null effect. A lack of statistical significance is an absence of evidence for an effect; it is not evidence of an absence of an effect.\nBut I saw this approach taken again and again in published work. (And still do!)\nMy first publication was an AJPS article (Rainey 2014) (Ungated PDF) explaining why this doesn’t work well and how to do it better.\nHere’s what I wrote in that paper:\n\nHypothesis testing is a powerful empirical argument not because it shows that the data are consistent with the research hypothesis, but because it shows that the data are inconsistent with other hypotheses (i.e., the null hypothesis). However, researchers sometimes reverse this logic when arguing for a negligible effect, showing only that the data are consistent with “no effect” and failing to show that the data are inconsistent with meaningful effects. When researchers argue that a variable has “no effect” because its confidence interval contains zero, they take no steps to rule out large, meaningful effects, making the empirical claim considerably less persuasive . (Altman and Bland 1995; Gill 1999; Nickerson 2000)\n\nBut here’s a critical point, it’s impossible to reject every hypothesis except exactly no effect. Instead, the researcher must define a range of substantively “negligible” effects. The researcher can reject the null hypothesis that the effect falls outside this range of negligible effects. However, this requires a substantive judgement about those effects that are negligible and those that are not.\nHere’s what I wrote:\n\nResearchers who wish to argue for a negligible effect must precisely define the set of effects that are deemed “negligible” as well as the set of effects that are “meaningful.” This requires defining the smallest substantively meaningful effect, which I denote as \\(m\\). The definition must be debated by substantive scholars for any given context because the appropriate \\(m\\) varies widely across applications."
  },
  {
    "objectID": "blog/2023-08-18-equivalence-tests/index.html#background-on-arguing-for-a-negligible-effect",
    "href": "blog/2023-08-18-equivalence-tests/index.html#background-on-arguing-for-a-negligible-effect",
    "title": "Equivalence Tests Using {marginaleffects}",
    "section": "",
    "text": "I remember sitting in a talk while I was a graduate student, and the speaker said something like: “I expect no effect here, and, just as I expected, the difference is not statistically significant.” Of course, that’s not a compelling argument for a null effect. A lack of statistical significance is an absence of evidence for an effect; it is not evidence of an absence of an effect.\nBut I saw this approach taken again and again in published work. (And still do!)\nMy first publication was an AJPS article (Rainey 2014) (Ungated PDF) explaining why this doesn’t work well and how to do it better.\nHere’s what I wrote in that paper:\n\nHypothesis testing is a powerful empirical argument not because it shows that the data are consistent with the research hypothesis, but because it shows that the data are inconsistent with other hypotheses (i.e., the null hypothesis). However, researchers sometimes reverse this logic when arguing for a negligible effect, showing only that the data are consistent with “no effect” and failing to show that the data are inconsistent with meaningful effects. When researchers argue that a variable has “no effect” because its confidence interval contains zero, they take no steps to rule out large, meaningful effects, making the empirical claim considerably less persuasive . (Altman and Bland 1995; Gill 1999; Nickerson 2000)\n\nBut here’s a critical point, it’s impossible to reject every hypothesis except exactly no effect. Instead, the researcher must define a range of substantively “negligible” effects. The researcher can reject the null hypothesis that the effect falls outside this range of negligible effects. However, this requires a substantive judgement about those effects that are negligible and those that are not.\nHere’s what I wrote:\n\nResearchers who wish to argue for a negligible effect must precisely define the set of effects that are deemed “negligible” as well as the set of effects that are “meaningful.” This requires defining the smallest substantively meaningful effect, which I denote as \\(m\\). The definition must be debated by substantive scholars for any given context because the appropriate \\(m\\) varies widely across applications."
  },
  {
    "objectID": "blog/2023-08-18-equivalence-tests/index.html#clark-and-golder-2006",
    "href": "blog/2023-08-18-equivalence-tests/index.html#clark-and-golder-2006",
    "title": "Equivalence Tests Using {marginaleffects}",
    "section": "Clark and Golder (2006)",
    "text": "Clark and Golder (2006)\nClark and Golder (2006) offer a nice example of this sort of hypothesis. I’ll refer you there and to Rainey (2014) for a complete discussion of their idea, but I’ll motivate it briefly here.\nExplaining why a country might have only a few (i.e., two) parties, Clark and Golder write:\n\nFirst, it could be the case that the demand for parties is low because there are few social cleavages. In this situation, there would be few parties whether the electoral institutions were permissive or not. Second, it could be the case that the electoral system is not permissive. In this situation, there would be a small number of parties even if the demand for political parties were high. Only a polity characterized by both a high degree of social heterogeneity and a highly permissive electoral system is expected to produce a large number of parties. (p. 683)\n\nThus, they expect that electoral institutions won’t matter in socially homogeneous systems. And they expect that social heterogeneity won’t matter in electoral systems that are not permissive."
  },
  {
    "objectID": "blog/2023-08-18-equivalence-tests/index.html#reproducing-clark-and-golder-2006",
    "href": "blog/2023-08-18-equivalence-tests/index.html#reproducing-clark-and-golder-2006",
    "title": "Equivalence Tests Using {marginaleffects}",
    "section": "Reproducing Clark and Golder (2006)",
    "text": "Reproducing Clark and Golder (2006)\nBefore computing their specific quantities of interest, let’s reproduce their regression model. Here’s their table that we’re trying to reproduce.\n\nAnd here’s a reproduction of their estimates using the cg2006 data from the {crdata} package on GitHub.1\n1 Run ?crdata::cg2006 for detailed documentation of this data set.\n# load packages\nlibrary(tidyverse)\nlibrary(sandwich)\nlibrary(modelsummary)\n\n# install my data packages from github\ndevtools::install_github(\"carlislerainey/crdata\")  # only updates if newer version available\n\n# load clark and golder's data set\ncg &lt;- crdata::cg2006\n\n# reproduce their estimates\nf &lt;- enep ~ eneg*log(average_magnitude) + eneg*upper_tier + en_pres*proximity\nfit &lt;- lm(f, data = cg)\n\n# regression table\nmodelsummary(fit, \n             vcov = ~ country, # cluster-robust SE; multiple observations per country\n             fmt = 2, \n             shape = term ~ model + statistic)\n\n\n\n    \n\n    \n    \n      \n        \n\n \n(1)\n\n        \n              \n                 \n                Est.\n                S.E.\n              \n        \n        \n        \n                \n                  (Intercept)\n                  2.92\n                  0.35\n                \n                \n                  eneg\n                  0.11\n                  0.14\n                \n                \n                  log(average_magnitude)\n                  0.08\n                  0.23\n                \n                \n                  upper_tier\n                  -0.06\n                  0.03\n                \n                \n                  en_pres\n                  0.26\n                  0.15\n                \n                \n                  proximity\n                  -3.10\n                  0.46\n                \n                \n                  eneg × log(average_magnitude)\n                  0.26\n                  0.17\n                \n                \n                  eneg × upper_tier\n                  0.06\n                  0.02\n                \n                \n                  en_pres × proximity\n                  0.68\n                  0.23\n                \n                \n                  Num.Obs.\n                  487\n                  \n                \n                \n                  R2\n                  0.397\n                  \n                \n                \n                  R2 Adj.\n                  0.387\n                  \n                \n                \n                  AIC\n                  1672.5\n                  \n                \n                \n                  BIC\n                  1714.3\n                  \n                \n                \n                  Log.Lik.\n                  -826.229\n                  \n                \n                \n                  RMSE\n                  1.32\n                  \n                \n                \n                  Std.Errors\n                  by: country\n                  \n                \n        \n      \n    \n\n\n\nSuccess!\nThey use averge_magnitude to measure the permissiveness of the electoral system and eneg to measure social heterogeneity."
  },
  {
    "objectID": "blog/2023-08-18-equivalence-tests/index.html#using-comparisons-to-compute-the-effects",
    "href": "blog/2023-08-18-equivalence-tests/index.html#using-comparisons-to-compute-the-effects",
    "title": "Equivalence Tests Using {marginaleffects}",
    "section": "Using comparisons() to compute the effects",
    "text": "Using comparisons() to compute the effects\nNow let’s compute the two quantities of interest. Clark and Golder argue for two negligible effects, which I make really concrete below.\n\nHypothesis 1 Increasing the effective number of ethnic groups from the 10th percentile (1.06) to the 90th percentile (2.48) will not lead to a substantively meaningful change in the effective number of political parties when the district magnitude is one.\nHypothesis 2 Increasing the district magnitude from one to seven will not lead to a substantively meaningful change in the effective number of political parties when the effective number of ethnic groups is one.\n\nAnd comparing the U.S. and the U.K., I argue that the smallest substantively interesting effect is 0.62. In Rainey (2014), I made the plot below. I want to reproduce it with {marginaleffects}.\n\nThese differences (and the 90% CIs) are really easy to compute using {marginaleffects}!2\n2 I’m only doing Clark and Golder’s original results, not any of the robustness checks.\n# load packages\nlibrary(marginaleffects)\n\n# the smallest substantively interesting effect\nm &lt;- 0.62\n\n# a data frame setting the values of the \"other\" variables\nX_c &lt;- data.frame(\n  eneg = 1.06,  # low value\n  average_magnitude = 1,  # low value\n  upper_tier = 0,\n  en_pres = 0, \n  proximity = 0\n)\n\n# compute the comparison for eneg and average magnitude\nc &lt;- comparisons(fit,\n            vcov = ~ country,\n            newdata = X_c, \n            variables = list(\"eneg\" = c(1.06, 2.48),         # low to high value\n                             \"average_magnitude\" = c(1, 7)), # low to high value\n            conf_level = 0.90)\n\nThis c outputted from comparisons() is a data frame.\n\n# default print method\nprint(c)\n\n\n              Term    Contrast Estimate Std. Error     z Pr(&gt;|z|)    S  5.0 %\n average_magnitude 7 - 1          0.696      0.175 3.973   &lt;0.001 13.8  0.408\n eneg              2.48 - 1.06    0.158      0.204 0.779    0.436  1.2 -0.176\n 95.0 %\n  0.984\n  0.493\n\nType:  response \n\n\n\n# quick look at the bare data frame\nglimpse(c)\n\nRows: 2\nColumns: 19\n$ rowid             &lt;int&gt; 1, 1\n$ term              &lt;chr&gt; \"average_magnitude\", \"eneg\"\n$ contrast          &lt;chr&gt; \"7 - 1\", \"2.48 - 1.06\"\n$ estimate          &lt;dbl&gt; 0.6956014, 0.1584771\n$ std.error         &lt;dbl&gt; 0.1750741, 0.2035052\n$ statistic         &lt;dbl&gt; 3.9731830, 0.7787372\n$ p.value           &lt;dbl&gt; 7.091852e-05, 4.361345e-01\n$ s.value           &lt;dbl&gt; 13.783478, 1.197155\n$ conf.low          &lt;dbl&gt; 0.4076302, -0.1762592\n$ conf.high         &lt;dbl&gt; 0.9835727, 0.4932134\n$ predicted_lo      &lt;dbl&gt; 3.034008, 3.034008\n$ predicted_hi      &lt;dbl&gt; 3.729609, 3.192485\n$ predicted         &lt;dbl&gt; 3.034008, 3.034008\n$ eneg              &lt;dbl&gt; 1.06, 1.06\n$ average_magnitude &lt;dbl&gt; 1, 1\n$ upper_tier        &lt;dbl&gt; 0, 0\n$ en_pres           &lt;dbl&gt; 0, 0\n$ proximity         &lt;dbl&gt; 0, 0\n$ enep              &lt;dbl&gt; 5.75, 5.75\n\n\nNow we can just plot the 90% CIs with ggplot() and check whether the entire interval falls inside the bounds.\n\n# bind the comparisons together and plot\nggplot(c, aes(x = estimate,\n                 xmin = conf.low,\n                 xmax = conf.high, \n                 y = term)) + \n  geom_vline(xintercept = c(-m, m), linetype = \"dashed\") + \n  geom_errorbarh() + \n  geom_point() \n\n\n\n\n\n\n\n\nIn this case, we conclude that social heterogeneity (eneg) has a negligible effect because the 90% CI only contains substantively negligible values. However, the 90% CI for district magnitude (average_magnitude) contains substantively negligible and meaningful values, so we cannot reject the null hypothesis of a meaningful effect."
  },
  {
    "objectID": "blog/2023-08-18-equivalence-tests/index.html#computing-the-tost-p-values-using-hypotheses",
    "href": "blog/2023-08-18-equivalence-tests/index.html#computing-the-tost-p-values-using-hypotheses",
    "title": "Equivalence Tests Using {marginaleffects}",
    "section": "Computing the TOST p-values using hypotheses()",
    "text": "Computing the TOST p-values using hypotheses()\nIt’s then almost trivial to use the hypotheses() function to compute the TOST p-values.\n\n# hypothesis tests\nhypotheses(c, equivalence = c(-m, m))\n\n\n              Term Estimate Std. Error     z Pr(&gt;|z|)    S  5.0 % 95.0 %\n average_magnitude    0.696      0.175 3.973   &lt;0.001 13.8  0.408  0.984\n eneg                 0.158      0.204 0.779    0.436  1.2 -0.176  0.493\n p (NonInf) p (NonSup) p (Equiv)\n     &lt;0.001     0.6671    0.6671\n     &lt;0.001     0.0117    0.0117\n\n\nThis doesn’t print super-nicely into this document, so let’s extract the important parts.\n\n# hypothesis tests, extracting the important pieces\nhypotheses(c, equivalence = c(-m, m)) %&gt;%\n  select(term, estimate, conf.low, conf.high, p.value.equiv)\n\n\n              Term Estimate CI low CI high p (Equiv)\n average_magnitude    0.696  0.408   0.984    0.6671\n eneg                 0.158 -0.176   0.493    0.0117\n\n\nChecking that the 90% CIs fall within the bounds created by the smallest substantively-meaningful effect is equivalent to checking whether the TOST p-value (i.e., the p(Equiv) column) is less than 0.05, so our conclusions are (and must be) identical."
  },
  {
    "objectID": "blog/2023-08-18-equivalence-tests/index.html#other-references",
    "href": "blog/2023-08-18-equivalence-tests/index.html#other-references",
    "title": "Equivalence Tests Using {marginaleffects}",
    "section": "Other references",
    "text": "Other references\nFor more on effective arguments for no effect, see the following:\n\nLakens (2017) and Lakens, Scheel, and Isager (2018) offer an accessible introduction to equivalences tests for psychologists.\nFitzgerald (2025) offers and introduction to and argument for equivalence tests for economists.\nKane (2024) offers an excellent summary of design considerations when arguing for no effect.\nMcCaskey and Rainey (2015) (Ungated PDF) argue that researchers should make “claims if and only if those claims hold for the entire confidence interval.” This extends the logic of equivalence testing to a broader collection of possible hypotheses."
  },
  {
    "objectID": "blog/2023-08-18-equivalence-tests/index.html#final-thoughts",
    "href": "blog/2023-08-18-equivalence-tests/index.html#final-thoughts",
    "title": "Equivalence Tests Using {marginaleffects}",
    "section": "Final thoughts",
    "text": "Final thoughts\n\n{marginaleffects} is a great package (Arel-Bundock 2024). I think it’s the first package in which the syntax matches the way I think about computing quantities of interest. That said, this is just my first try at it. But I’m very impressed so far.\nThe {marginaleffects} book Model to Meaning has a whole chapter on equivalence tests. My only caution is that there is a mismatch between 95% confidence intervals and equivalence tests. By default, {marginaleffects} reports a 95% CI, even when producing a p-value for an equivalence test. However, the 90% confidence interval correspondents to a size-5% equivalence test. So if you’re using {marginaleffects} to do equivalence tests, I recommend setting conf_level = 0.90.3\nFor a more recent example, Jares and Malhotra (2024) discusses negligible effects and hypothesis tests in a way that I find clear and compelling. It’s an excellent model to follow. See pp. 12-13. They “show that improved compensation outcomes had negligible impacts on Republican farmers’ midterm turnout and campaign contributions, even though such variation in benefits significantly affected farmers’ propensity to view the intervention as helpful.”\n\n\n\n\n3 I would make a similar point about one-sided tests as well, but that’s less correct, because it should be a one-sided 95% CI."
  },
  {
    "objectID": "blog/2024-06-03-pilot-power/index.html",
    "href": "blog/2024-06-03-pilot-power/index.html",
    "title": "Statistical Power from Pilot Data: Simulations to Illustrate",
    "section": "",
    "text": "We can think of statistical power as determined by the ratio \\(\\frac{\\tau}{SE}\\), where \\(\\tau\\) is the treatment effect and SE is the standard error of the estimate.1 To reason about statistical power, one needs to make assumptions or predictions about the treatment effect and the standard error.\nIn this post, I discuss ways that pilot data should and should not be used as part of a power analysis. I make two points:\nWith a predicted standard error in hand, we can predict for the minimum detectable effect, the statistical power, or the required sample size in the planned study."
  },
  {
    "objectID": "blog/2024-06-03-pilot-power/index.html#pilot-data-should-not-usually-be-used-to-estimate-the-treatment-effect",
    "href": "blog/2024-06-03-pilot-power/index.html#pilot-data-should-not-usually-be-used-to-estimate-the-treatment-effect",
    "title": "Statistical Power from Pilot Data: Simulations to Illustrate",
    "section": "Pilot data should not usually be used to estimate the treatment effect",
    "text": "Pilot data should not usually be used to estimate the treatment effect\nTo compute statistical power, researchers need to make an assumption about the size of the treatment effect. It’s easy to feel lost without any guidance on what effects are reasonable to look for, so we might feel tempted to use a small pilot study to estimate the treatment effect and then use that estimate in our power analysis. This is a bad idea because the estimate of the treatment effect from a pilot study is too uncertain for a power analysis.2 Leon, Davis, and Kraemer (2011) and Albers and Lakens (2018) discuss this problem in more detail.3\n2 The estimate of the treatment effect from a well-powered study might be too noisy as well.3 Perugini, Gallucci, and Costantini (2014) offer a potential solution if it’s important to estimate the treatment effect from pilot data, though their approach is data-hungry and very conservative. Pilot data can estimate/predict the standard deviation in the full study quite precisely even with small pilots, such as 10 respondents per condition.\n\n\n\n\n\nWarning\n\n\n\nDo not use a small pilot study to estimate the treatment effect and then use that estimate as the treatment effect in a power analysis."
  },
  {
    "objectID": "blog/2024-06-03-pilot-power/index.html#pilot-data-can-be-used-to-predict-the-standard-error",
    "href": "blog/2024-06-03-pilot-power/index.html#pilot-data-can-be-used-to-predict-the-standard-error",
    "title": "Statistical Power from Pilot Data: Simulations to Illustrate",
    "section": "Pilot data can be used to predict the standard error",
    "text": "Pilot data can be used to predict the standard error\nWhile pilot data might not be useful for estimating the treatment effect, pilot data are useful for estimating the standard error of the planned study. Given that power is a function of the ratio of the treatment effect and the standard error, it’s important to have a good prediction of the standard error. Further, the noisiness of this estimated standard error is predictable, so it’s easy to nudge the estimate slightly to obtain a conservative prediction.\nIn political science, it’s common to run pilot studies with, say, 100-200 respondents before a full-sized study of, say, 1,000 respondents. It can be very helpful to use these pilot data to confirm any preliminary power calculations.\nHere are two helpful rules:\n\n\n\n\n\n\nPredicting the SE in the planned study using pilot data\n\n\n\nWe can use pilot data to predict the standard error of the estimated treatment effect in a planned study. Conservatively, the standard error will be about \\(\\sqrt{\\frac{n^{pilot}}{n^{planned}}}\\ \\left\\lbrack \\left( \\sqrt{\\frac{1}{n^{pilot}}} + 1 \\right) \\cdot {\\widehat{SE}}_{\\widehat{\\tau}}^{pilot} \\right\\rbrack\\), where \\(n^{pilot}\\) is the number of respondents per condition in the pilot data, \\(SE_{\\widehat{\\tau}}^{pilot}\\) is the estimated standard error using the pilot data, and \\(n^{planned}\\) is the number of respondents per condition in the planned study.\n\n\n\n\n\n\n\n\nPredicting the required sample size in the planned study using pilot data\n\n\n\nWe can use pilot data to conservatively predict the sample size we will need in a planned study. For 80% power to detect the treatment effect \\(\\widetilde{\\tau}\\), we will (conservatively) need about \\(n^{pilot} \\cdot \\left\\lbrack \\frac{2.5}{\\widetilde{\\tau}} \\cdot \\left( \\sqrt{\\frac{1}{n^{pilot}}} + 1 \\right) \\cdot {\\widehat{SE}}_{\\widehat{\\tau}}^{pilot} \\right\\rbrack^{2}\\) respondents per condition, where \\(n^{pilot}\\) is the number of respondents per condition in the pilot data and \\(SE_{\\widehat{\\tau}}^{pilot}\\) is the estimated standard error using the pilot data.\n\n\nNote that the factor \\(\\sqrt{\\frac{1}{n^{pilot}}} + 1\\) nudges the predicted standard error in a conservative direction. See this working paper for more details.\nWe can use the predicted standard error to find the minimum detectable effect (for 80% power) or the power (for a given treatment effect). Or we can use the pilot data to estimate the required sample size (for 80% power to detect a given treatment effect.)\n\nThe setting\nLet’s imagine a setting where a study with 1,000 respondents has 80% power to detect an average treatment effect of 1 unit. I’m imagining that we’re using linear regression with robust standard errors to test the hypothesis that the average treatment effect is positive (aka Welch’s t-test). There’s just one treatment group and one control group with 500 respondents each, for 1,000 respondents total.\n\n# set the treatment effect, SE, and sample size\ntau &lt;- 1  # treatment effect\nse  &lt;- tau/(qnorm(0.95) + qnorm(0.80))  # standard error for 80% power\nn_planned &lt;- 500  # sample size per condition in planned full study\n\n# calculate required standard deviation to yield 80% power given the above\nsigma &lt;- se*sqrt(2*n_planned)/2\n\nLet’s confirm that this setting does indeed give us 80% power.\n\nres_list &lt;- NULL # a container to collect results\nfor (i in 1:10000) {\n      \n  # simulate study\n  y0 &lt;- rnorm(2*n_planned, sd = sigma)\n  y1 &lt;- y0 + tau\n  d &lt;- sample(rep(0:1, length.out = 2*n_planned))\n  y &lt;- ifelse(d == 1, y1, y0)\n  data &lt;- data.frame(y, d)\n  \n  # fit model and get standard error and p-value\n  fit &lt;- lm(y ~ d, data = data)\n  tau_hat &lt;- as.numeric(coef(fit)[\"d\"])\n  se_hat &lt;- as.numeric(sqrt(diag(sandwich::vcovHC(fit, type = \"HC2\")))[\"d\"])\n  p_value &lt;- pnorm(tau_hat/se_hat, lower.tail = FALSE)\n\n  # collect results\n  res_list[[i]] &lt;- data.frame(tau_hat, se_hat, p_value)\n  }\n\n# compute power (and monte carlo error)\nres_list |&gt;\n  bind_rows() |&gt;\n  summarize(power = mean(p_value &lt; 0.05), \n            mc_error = sqrt(power*(1 - power))/sqrt(n()), \n            lwr = power - 2*mc_error, \n            upr = power + 2*mc_error)\n\n   power    mc_error       lwr       upr\n1 0.8023 0.003982646 0.7943347 0.8102653\n\n\nNailed it!\n\n\n\n\n\n\nDeclareDesign Alternative\n\n\n\nAlex Coppock kindly shared how one might run the same simulation using {DeclareDesign}. Code here.\n\n\n\n\nThe Pilot Studies\nNow let’s simulate 1,000 pilot studies with 10, 30, 60, 90, and 150 respondents per condition. I’m going to grab the standard error from each but throw the estimates of the treatment effects right into the trash.\n\n# sample size per condition in pilot study\nn_pilot_values   &lt;- c(10, 30, 60, 90, 150) \n\nres_list &lt;- NULL  # a container to collect results\niter &lt;- 1  # counter to index the collection\nfor (i in 1:10000) {\n  for (j in 1:length(n_pilot_values)) {\n    \n    # set respondents per condition in the pilot study\n    n_pilot &lt;- n_pilot_values[j]\n    \n    # simulate pilot study\n    y0 &lt;- rnorm(2*n_pilot, sd = sigma)\n    y1 &lt;- y0 + tau\n    d &lt;- sample(rep(0:1, length.out = 2*n_pilot))\n    y &lt;- ifelse(d == 1, y1, y0)\n    pilot_data &lt;- data.frame(y, d)\n    \n    # fit model and get standard error\n    fit_pilot &lt;- lm(y ~ d, data = pilot_data)\n    tau_hat &lt;- as.numeric(coef(fit_pilot)[\"d\"])\n    pilot_se_hat &lt;- as.numeric(sqrt(diag(sandwich::vcovHC(fit_pilot, type = \"HC2\")))[\"d\"])\n    \n    # collect standard errors \n    res_list[[iter]] &lt;- data.frame(pilot_se_hat, n_pilot)\n    iter &lt;- iter + 1 # update counter\n  }\n}\n\n# combine collected results in a data frame\nres &lt;- bind_rows(res_list) |&gt;\n  glimpse()\n\nRows: 50,000\nColumns: 2\n$ pilot_se_hat &lt;dbl&gt; 2.7282069, 1.7541542, 1.1426683, 0.9763233, 0.7760267, 3.…\n$ n_pilot      &lt;dbl&gt; 10, 30, 60, 90, 150, 10, 30, 60, 90, 150, 10, 30, 60, 90,…\n\n\nNow let’s take a look a these standard errors from the simulated pilot studies. Notice that the standard errors are all larger than the standard error in the full study. And the smaller the pilot, the larger the standard error. This makes sense.\n\nggplot(res, aes(x = pilot_se_hat)) + \n  geom_histogram() + \n  facet_wrap(vars(n_pilot)) + \n  geom_vline(xintercept = se)\n\n\n\n\n\n\n\n\nHowever, we can translate the standard error from the pilot studies into predictions for the standard errors in the full studies by multiplying the pilot standard error times \\(\\sqrt{\\frac{n^{pilot}}{n^{planned}}}\\).4\n4 In this setting, we’re planning on 500 respondents per condition.\nggplot(res, aes(x = sqrt(n_pilot/n_planned)*pilot_se_hat)) + \n  geom_histogram() + \n  facet_wrap(vars(n_pilot)) + \n  geom_vline(xintercept = se)\n\n\n\n\n\n\n\n\nThat’s spot on! However, notice that we sometimes substantially underestimate the standard error. When we underestimate the standard error, we will overestimate the power (which is bad for us).\nAs a solution, we can gently nudge the standard error from the pilot up by a factor of \\(\\left( \\sqrt{\\frac{1}{n^{pilot}}} + 1 \\right)\\), which will make “almost all” of the standard errors over-estimates or “conservative” (details here).\n\nggplot(res, aes(x = sqrt(n_pilot/n_planned)*(sqrt(1/n_pilot) + 1)*pilot_se_hat)) + \n  geom_histogram() + \n  facet_wrap(vars(n_pilot)) + \n  geom_vline(xintercept = se)\n\n\n\n\n\n\n\n\nThis works super well.\nBut how should we use this predicted standard error to evaluate or choose a sample size?"
  },
  {
    "objectID": "blog/2024-06-03-pilot-power/index.html#how-to-use-the-predicted-standard-error",
    "href": "blog/2024-06-03-pilot-power/index.html#how-to-use-the-predicted-standard-error",
    "title": "Statistical Power from Pilot Data: Simulations to Illustrate",
    "section": "How to use the predicted standard error",
    "text": "How to use the predicted standard error\nWe can use these conservative standard errors to compute any of the following (conservatively, as well):\n\nthe minimum detectable effect with 80% power\nthe statistical power for a given treatment effect\nthe sample size required to obtain 80% power for a given treatment effect\n\n\nThe Minimum Detectable Effect\nFirst, we can compute the minimum detectable effect with 80% power. This is about 2.5 times the standard error.\n\n# compute minimum detectable effect\nmde &lt;- res %&gt;%\n  mutate(pred_se_cons = sqrt(n_pilot/n_planned)*(sqrt(1/n_pilot) + 1)*pilot_se_hat, \n         mde_cons = 2.5*pred_se_cons) %&gt;%\n  glimpse()\n\nRows: 50,000\nColumns: 4\n$ pilot_se_hat &lt;dbl&gt; 2.7282069, 1.7541542, 1.1426683, 0.9763233, 0.7760267, 3.…\n$ n_pilot      &lt;dbl&gt; 10, 30, 60, 90, 150, 10, 30, 60, 90, 150, 10, 30, 60, 90,…\n$ pred_se_cons &lt;dbl&gt; 0.5078358, 0.5081264, 0.4469336, 0.4578814, 0.4597523, 0.…\n$ mde_cons     &lt;dbl&gt; 1.269590, 1.270316, 1.117334, 1.144704, 1.149381, 1.40411…\n\n# plot minimum detectable effect\nggplot(mde, aes(x = mde_cons)) + \n  geom_histogram() + \n  facet_wrap(vars(n_pilot)) + \n  geom_vline(xintercept = tau)\n\n\n\n\n\n\n\n\n\n\nStatistical Power\nSecond, we can compute the statistical power for given treatment effect. Power equals \\(1 - \\Phi_{std}\\left(1.64 - \\frac{\\tau}{SE} \\right)\\), where \\(\\Phi_{std}(z)\\) is pnorm(), \\(SE\\) is the standard error of the estimated treatment effect, and \\(\\tau\\) is the treatment effect.\n\n# compute the power\npwr &lt;- res %&gt;%\n  mutate(pred_se_cons = sqrt(n_pilot/n_planned)*(sqrt(1/n_pilot) + 1)*pilot_se_hat, \n         power_cons = 1 - pnorm(1.64 - tau/pred_se_cons)) %&gt;%\n  glimpse()\n\nRows: 50,000\nColumns: 4\n$ pilot_se_hat &lt;dbl&gt; 2.7282069, 1.7541542, 1.1426683, 0.9763233, 0.7760267, 3.…\n$ n_pilot      &lt;dbl&gt; 10, 30, 60, 90, 150, 10, 30, 60, 90, 150, 10, 30, 60, 90,…\n$ pred_se_cons &lt;dbl&gt; 0.5078358, 0.5081264, 0.4469336, 0.4578814, 0.4597523, 0.…\n$ power_cons   &lt;dbl&gt; 0.6289751, 0.6285495, 0.7249028, 0.7067695, 0.7037043, 0.…\n\n# plot the power\nggplot(pwr, aes(x = power_cons)) + \n  geom_histogram() + \n  facet_wrap(vars(n_pilot)) + \n  geom_vline(xintercept = .8)\n\n\n\n\n\n\n\n\n\n\nRequired Sample Size\nFinally, we can compute the required sample size to obtain 80% power to detect a certain treatment effect.\nAs I described above, for 80% power to detect the treatment effect \\(\\widetilde{\\tau}\\), we will (conservatively) need about \\(n^{pilot} \\cdot \\left\\lbrack \\frac{2.5}{\\widetilde{\\tau}} \\cdot \\left( \\sqrt{\\frac{1}{n^{pilot}}} + 1 \\right) \\cdot {\\widehat{SE}}_{\\widehat{\\tau}}^{pilot} \\right\\rbrack^{2}\\) respondents per condition.\n\n# compute the required sample size\nss &lt;- res %&gt;%\n  mutate(ss_cons = n_pilot*((2.5/tau)*(sqrt(1/n_pilot) + 1)*pilot_se_hat)^2) %&gt;%\n  glimpse()\n\nRows: 50,000\nColumns: 3\n$ pilot_se_hat &lt;dbl&gt; 2.7282069, 1.7541542, 1.1426683, 0.9763233, 0.7760267, 3.…\n$ n_pilot      &lt;dbl&gt; 10, 30, 60, 90, 150, 10, 30, 60, 90, 150, 10, 30, 60, 90,…\n$ ss_cons      &lt;dbl&gt; 805.9289, 806.8515, 624.2176, 655.1731, 660.5380, 985.772…\n\n# plot the required sample size\nggplot(ss, aes(x = ss_cons)) + \n  geom_histogram() + \n  facet_wrap(vars(n_pilot)) + \n  geom_vline(xintercept = n_planned)\n\n\n\n\n\n\n\n\nSample size is an especially helpful metric, because it is the constraint and cost that researchers face most directly. Because these required sample sizes are conservative, they tend to be too large—but by how much? For pilots with 60 respondents per condition, the sample sizes tend to be about 30% too large. This means that researchers could have obtained 80% power with 1,000 respondents but instead used 1,300 respondents.\nIn my view, this 30% waste is not particularly concerning. It’s relatively small and the statistical power will still be less than 90% even if the sample size is increased by 30%.\nBut most importantly, almost all of the sample sizes exceed what we need for 80% power.\n\n# compute features of the sample sizes\nss %&gt;%\n  group_by(n_pilot) %&gt;%\n  mutate(waste = ss_cons/n_planned - 1) %&gt;%\n  summarize(avg_waste = scales::percent(mean(waste), accuracy = 1), \n            pct_too_small = scales::percent(mean(ss_cons &lt; 500), accuracy = 1)) %&gt;%\n  rename(`Respondents per condition in pilot study` = n_pilot,\n         `Average waste (needed 1,000 respondents and used 1,300 means waste is 30%)` = avg_waste, \n         `Percent of sample sizes that produce less than 80% power` = pct_too_small) %&gt;%\n  tinytable::tt()\n\n \n\n  \n    \n    \n    tinytable_sghpfcv67rch2nmchm9v\n    \n    \n    \n    \n  \n\n  \n    \n      \n        \n        \n              \n                Respondents per condition in pilot study\n                Average waste (needed 1,000 respondents and used 1,300 means waste is 30%)\n                Percent of sample sizes that produce less than 80% power\n              \n        \n        \n        \n                \n                   10\n                  75%\n                  7%\n                \n                \n                   30\n                  41%\n                  4%\n                \n                \n                   60\n                  29%\n                  3%\n                \n                \n                   90\n                  24%\n                  3%\n                \n                \n                  150\n                  18%\n                  3%"
  },
  {
    "objectID": "blog/2024-06-03-pilot-power/index.html#summary",
    "href": "blog/2024-06-03-pilot-power/index.html#summary",
    "title": "Statistical Power from Pilot Data: Simulations to Illustrate",
    "section": "Summary",
    "text": "Summary\nWe think of statistical power as determined by the ratio \\(\\frac{\\tau}{SE}\\), where \\(\\tau\\) is the treatment effect and SE is the standard error of the estimate. To reason about statistical power, one needs to make assumptions or predictions about the treatment effect and the standard error.\nI make two points in this post:\n\nPilot data are not usually useful to estimate the treatment effect.\nPilot data can be useful to predict the standard error.\n\nWith a predicted standard error in hand, we can obtain a prediction for the minimum detectable effect, the statistical power, or the required sample size.\nYou can find more details in this paper."
  },
  {
    "objectID": "blog/2024-06-03-pilot-power/index.html#see-also",
    "href": "blog/2024-06-03-pilot-power/index.html#see-also",
    "title": "Statistical Power from Pilot Data: Simulations to Illustrate",
    "section": "See also",
    "text": "See also\n\nSection 21.4 of the DeclareDesign book on piloting.\n“Should a pilot study change your study design decisions?” on the DeclareDesign blog."
  },
  {
    "objectID": "blog/2025-03-19-kane-2025-as-a-power-paper/index.html",
    "href": "blog/2025-03-19-kane-2025-as-a-power-paper/index.html",
    "title": "Kane (2025) as a Power Paper",
    "section": "",
    "text": "It’s helpful to think of statistical power as determined by the ratio \\(\\frac{\\tau}{SE}\\), where \\(\\tau\\) is the treatment effect and SE is the standard error of the estimate.1\nI like this ratio because it makes two distinct tasks clear:\nWhile pitched as a paper on null result, Kane (2025) offers a very helpful (and thorough!) list of ideas to increase statistical power."
  },
  {
    "objectID": "blog/2025-03-19-kane-2025-as-a-power-paper/index.html#power-as-a-ratio",
    "href": "blog/2025-03-19-kane-2025-as-a-power-paper/index.html#power-as-a-ratio",
    "title": "Kane (2025) as a Power Paper",
    "section": "Power as a Ratio",
    "text": "Power as a Ratio\nFor a “well-powered experiment,” the task is to get the ratio \\(\\frac{\\tau}{SE}\\) above at least 2.5 (for 80% power) and ideally to 3.3 (for 95% power).2\n2 Remember that “well-powered” isn’t a given. Most political science research is “greatly underpowered” (Arel-Bundock et al. 2022).Here’s the idea:\n\nPower equals \\(1 - \\Phi_{std}\\left( 1.64 - \\frac{\\tau}{SE} \\right)\\), where \\(\\Phi_{std}(z)\\) is the standard normal CDF (as found in a standard \\(z\\) table), \\(SE\\) is the standard error of the estimated treatment effect, and \\(\\tau\\) is the treatment effect.\n\n\nThis rule allows us to connect statistical power to the familiar \\(z\\) table in the appendices of many statistics textbooks or the pnorm() function in R. But, more importantly, it drives home an important intuition—power is determined by the key ratio \\(\\frac{\\tau}{SE}\\). To estimate power, we must make an informed assumption about the effect of interest and a good prediction of standard error of the estimate. Further, when we consider changes to the experimental design and the consequences for statistical power, it can be helpful to think about the numerator or the denominator of the key ratio \\(\\frac{\\tau}{SE}\\): How do you make the effect as large as possible? And how do you make the estimate as precise as possible?\n\n\n\n\n\n\n\nTwo Key Tasks\n\n\n\n\nMake the effect as large as possible.\nMake the estimate as precise as possible."
  },
  {
    "objectID": "blog/2025-03-19-kane-2025-as-a-power-paper/index.html#kanes-list-and-relevance-to-the-ratio",
    "href": "blog/2025-03-19-kane-2025-as-a-power-paper/index.html#kanes-list-and-relevance-to-the-ratio",
    "title": "Kane (2025) as a Power Paper",
    "section": "Kane’s List, and Relevance to the Ratio",
    "text": "Kane’s List, and Relevance to the Ratio\nKane (2025) offers an excellent list of options to either increase the treatment effect or shrink the standard error.\nHe pitches the list as a way to make a compelling case for null results. However, the list is also helpful for making sure that we’ve thought through the ways to maximize power. It’s a wonderful paper well worth a careful read for folks who run experiments or read experimental work.\nI’m going to borrow his list and connect the action items to statistical power.\nKane arranges his list into seven “alternative explanations” for null results, which we might think of as “potential power leakages.” I recreated a version of Kane’s table (Table 1 in his paper) that contains each action item to consider. I’ve kept Kane’s labeling for the various categories, but taken some liberties in collapsing some of the action items.3\n3 He breaks his suggestions into to columns for the current study and future studies. For my purposes, these can be collapsed.The table below lists all of Kane’s categories and action items and notes the pathway through which this stop might increase statistical power. While Kane doesn’t explicitly pitch the list as “ways to increase” power, we can certainly borrow it for that purpose.\n\n\n\n\nKane’s Categories\nAction Item\nPathway to Increase Power\n\n\n\n\n#1: Inattentiveness\nInclude a measure of attention to the experimental manipulation (after the outcome measure)\nNA\n\n\n\nFeature experiment relatively earlier in survey\nIncrease treatment effect.\n\n\n\nMore salient treatment content\nIncrease treatment effect.\n\n\n\nConsider techniques to improve respondent attentiveness\nIncrease treatment effect.\n\n\n\nUse alternative survey company\nIncrease treatment effect; see Stagnaro et al. (2024) for more.\n\n\n\nIncreased (pre-treatment) screening-out of inattentive respondents\nIncrease treatment effect, though a smaller sample size will increase the standard error.\n\n\n#2: Failure to vary the independent variable\nInclude a manipulation check after the outcome measure\nNA\n\n\n\nMake treatment content more salient (e.g., appear sooner and/or more frequently)\nIncrease treatment effect.\n\n\n\nMake treatment stronger (e.g., more direct, forceful language)\nIncrease treatment effect.\n\n\n#3: Pre-treatment effect\nInclude a measure to gauge how pre-treated a respondent might be (implemented prior to random assignment)\nNA\n\n\n\nMake treatment stronger (assuming no ceiling/floor effect)\nIncrease treatment effect.\n\n\n\nPostpone the study until treatment has lower salience in the real world\nIncrease treatment effect.\n\n\n\nInvestigate using a non-experimental design\nNA\n\n\n#4: Statistical power\nConduct power analyses to determine necessary n size (assuming smallest meaningful effect size)\nDecrease standard error; see Bloom (1995) and this preprint for more.\n\n\n\nChoose sample size cognizant of number of conditions, subgroup analyses, and likely inattentiveness\nDecrease standard error; see Bloom (1995) and this preprint for more.\n\n\n\nUse pre-registered covariates in model, blocking, or a within-subjects design\nDecrease standard error; see Blair et al. (2019) and Blair, Coppock, and Humphreys (2023) for a framework to assess the impact of these design choices; ; see Clifford, Sheagley, and Piston (2021) for a good approach using a pre-post design.\n\n\n\nAim to collect a larger sample\nDecrease standard error.\n\n\n\nChoose sample size cognizant of quantities learned from first study: effect size, SD of Y, level of inattentiveness, and number/size of subgroup analyses\nDecrease standard error; this this preprint presents some ideas about how to connect power analysis to existing studies.\n\n\n\nConsider an alternative design structure (e.g., within-subjects)\nDecrease standard error.\n\n\n#5: Poor measurement of the dependent variable\nCheck Y’s criterion validity\nDecrease standard error; perhaps also increase treatment effect.\n\n\n\nIf possible, use existing (validated) measures of Y\nDecrease standard error; perhaps also increase treatment effect.\n\n\n\nUse an alternative and/or multiple measures of Y to reduce measurement error\nDecrease standard error; perhaps also increase treatment effect.\n\n\n#6: Ceiling/Floor effect\nUse measure of Y that is unlikely to have an extremely high/low mean (e.g., scales with more extreme end-points or multi-item scales)\nIncrease treatment effect.\n\n\n\nAssuming population of interest remains the same, use a measure of Y with a more (conceptually) extreme range\nIncrease treatment effect.\n\n\n\nConsider postponing study if ceiling/floor is due to current context\nIncrease treatment effect.\n\n\n#7: Countervailing treatment effects\nInclude a pre-treatment measure of the moderating variable, across which countervailing effects might occur\nIncrease treatment effect (by changing definition slightly); this will increase the standard error.\n\n\n\nPre-register a hypothesized interaction between treatment and moderator\nIncrease treatment effect (by changing definition slightly); this will increase the standard error.\n\n\n\nInclude best possible (pre-treatment) measure(s) of moderating variable to improve precision\nDecrease standard error.\n\n\n\n\nAnd for completeness, here is Kane’s original table:"
  },
  {
    "objectID": "blog/2025-08-18-for-your-syllabus-power/index.html",
    "href": "blog/2025-08-18-for-your-syllabus-power/index.html",
    "title": "For Your Syllabus: Statistical Power",
    "section": "",
    "text": "As you prepare your courses on quantitative methods, consider including a section on statistical power.\nHere are five readings I like and often recommend."
  },
  {
    "objectID": "blog/2025-08-18-for-your-syllabus-power/index.html#quantitative-political-science-research-is-greatly-underpowered-arel-bundock-et-al.",
    "href": "blog/2025-08-18-for-your-syllabus-power/index.html#quantitative-political-science-research-is-greatly-underpowered-arel-bundock-et-al.",
    "title": "For Your Syllabus: Statistical Power",
    "section": "Quantitative Political Science Research is Greatly Underpowered (Arel-Bundock et al.)",
    "text": "Quantitative Political Science Research is Greatly Underpowered (Arel-Bundock et al.)\nThis paper shows that low power is a real problem—“doing what others have done” does not guarantee a well-powered study for the true effect. Instead, power requires some thinking!\nFrom the abstract:\n&gt; “Only about 1 in 10 tests have at least 80% power to detect the consensus effects reported in the literature.”\n\n🔗 Journal DOI (JOP): https://doi.org/10.1086/734279\n\n🔗 Preprint DOI (OSF): https://doi.org/10.31219/osf.io/7vy2f"
  },
  {
    "objectID": "blog/2025-08-18-for-your-syllabus-power/index.html#minimum-detectable-effects-bloom-1995",
    "href": "blog/2025-08-18-for-your-syllabus-power/index.html#minimum-detectable-effects-bloom-1995",
    "title": "For Your Syllabus: Statistical Power",
    "section": "Minimum Detectable Effects (Bloom 1995)",
    "text": "Minimum Detectable Effects (Bloom 1995)\nInstead of focusing on power curves or abstract probabilities, Bloom asks a concrete question: “What’s the smallest effect this design could reliably detect?”\nThe paper: - Defines the minimum detectable effect (MDE) clearly\n- Provides formulas and tables to compute it\n- Explains how design choices affect statistical power\n\n🔗 Journal DOI: https://doi.org/10.1177/0193841X9501900504\n\n🔗 PDF: https://sites.uci.edu/gduncan/files/2021/03/Bloom-MDES-Eval-Rev-1995-Bloom.pdf"
  },
  {
    "objectID": "blog/2025-08-18-for-your-syllabus-power/index.html#power-rules",
    "href": "blog/2025-08-18-for-your-syllabus-power/index.html#power-rules",
    "title": "For Your Syllabus: Statistical Power",
    "section": "Power Rules",
    "text": "Power Rules\nMy (unpublished) paper updates Bloom’s ideas. Especially for political scientists or others using OLS and robust SEs, this paper may be useful.\nIt: 1. Builds intuition for power\n2. Develops simple rules for back-of-the-envelope power calculations\n3. Describes how to incorporate pilot data into power calculations\n\n🔗 Preprint DOI (OSF): https://doi.org/10.31219/osf.io/5am9q"
  },
  {
    "objectID": "blog/2025-08-18-for-your-syllabus-power/index.html#sample-size-justification-lakens-2022",
    "href": "blog/2025-08-18-for-your-syllabus-power/index.html#sample-size-justification-lakens-2022",
    "title": "For Your Syllabus: Statistical Power",
    "section": "Sample Size Justification (Lakens 2022)",
    "text": "Sample Size Justification (Lakens 2022)\nPower is just one way to justify sample size. This paper offers a broader framework to align your design with your research goals.\nLakens outlines six common justifications: 1. Collecting data from (almost) the entire population\n2. Choosing a sample size based on resource constraints\n3. Performing an a-priori power analysis\n4. Planning for a desired accuracy (e.g., CI width)\n5. Using heuristics (e.g., 30 per group)\n6. Explicitly acknowledging the absence of a justification\n\n🔗 Journal DOI (open): https://doi.org/10.1525/collabra.33267"
  },
  {
    "objectID": "blog/2025-08-18-for-your-syllabus-power/index.html#declaring-and-diagnosing-research-designs-blair-et-al.-2019",
    "href": "blog/2025-08-18-for-your-syllabus-power/index.html#declaring-and-diagnosing-research-designs-blair-et-al.-2019",
    "title": "For Your Syllabus: Statistical Power",
    "section": "Declaring and Diagnosing Research Designs (Blair et al. 2019)",
    "text": "Declaring and Diagnosing Research Designs (Blair et al. 2019)\nThe authors introduce a much more general way to think about designs. They propose the MIDA framework:\n\nModel: Assumptions about how the world works\n\nInquiry: The specific question being asked\n\nData Strategy: The plan for collecting data\n\nAnswer Strategy: The method to answer the inquiry\n🔗 Journal DOI (open): https://doi.org/10.1017/S0003055419000194\n\n📚 Book (open): https://book.declaredesign.org/\n\n💻 R package site: https://declaredesign.org/r/declaredesign/"
  },
  {
    "objectID": "blog/2025-08-18-for-your-syllabus-power/index.html#closing",
    "href": "blog/2025-08-18-for-your-syllabus-power/index.html#closing",
    "title": "For Your Syllabus: Statistical Power",
    "section": "Closing",
    "text": "Closing\nThese five readings complement each other really well. Students have a lot to gain from each.\n\n\n\nPaper\nValue\n\n\n\n\nArel-Bundock et al.\nShows that most political science studies are underpowered; demonstrates the importance of thinking carefully about design.\n\n\nBloom (1995)\nDefines the minimum detectable effect (MDE) and explains how design choices influence power.\n\n\nRainey\nProvides intuitive rules of thumb for power analysis, including using pilot data.\n\n\nLakens (2022)\nBroadens the discussion; outlines six ways to justify sample size beyond traditional power analysis.\n\n\nBlair et al. (2019)\nIntroduces the MIDA framework for formally declaring and diagnosing research designs."
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Papers",
    "section": "",
    "text": "If want to stay up-to-date on my work, follow me on Google Scholar."
  },
  {
    "objectID": "research/index.html#working-papers",
    "href": "research/index.html#working-papers",
    "title": "Papers",
    "section": "Working Papers",
    "text": "Working Papers\n\n\n\n\n\n\n\n  \n    \n      Power Rules: Practical Advice for Computing Power (and Automating with Pilot Data) \n      \n      \n      \n      \n        \n          Mar 5, 2025. Carlisle Rainey. Working Paper.\n        \n      \n      \n      \n      \n      \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n         \n           Preprint \n         \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n        \n          I provide detailed examples with R code in the blog posts Statistical Power from Pilot Data: Simulations to Illustrate and Statistical Power from Pilot Data: An Example     \n        \n      \n      \n      \n    \n  \n    \n      The Data Availability Policies of Political Science Journals \n      \n      \n      \n      \n        \n          Jan 12, 2025. Carlisle Rainey, Harley Roe. Working paper.\n        \n      \n      \n      \n      \n      \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n         \n           Preprint \n         \n      \n      \n      \n      \n      \n         \n           Dataverse \n        \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n    \n      Overt Consequences of Covert Actions: Success, Failure, and Voters’ Preferences for Legislative Oversight \n      \n      \n      \n      \n        \n          Apr 19, 2024. Caroline Robbins, Alessandro Brunelli, José Castro, Ainsley Coty, Andrew Louis, Bryanna Major, María Alemán Martínez, Yadianis Lara Ojeda, Larissa Pontes, Elke Schumacher, Omer Turkomer, Luzmi Valenzuela, Valeria Veras, Carlisle Rainey. Working paper.\n        \n      \n      \n      \n      \n      \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n         \n           Preprint \n         \n      \n      \n      \n      \n      \n      \n      \n         \n           OSF\n        \n      \n      \n      \n      \n      \n         \n           Preregistration \n         \n      \n      \n      \n      \n      \n      \n      \n    \n  \n    \n      The Dissent Score: Using Events Data to Measure Dissent \n      \n      \n      \n      \n        \n          Dec 23, 2023. Carlisle Rainey, Harley Roe, Qing Wang, Nick Dietrich. Working paper.\n        \n      \n      \n      \n      \n      \n      \n\n      \n      \n      \n         \n           Project Website\n        \n      \n      \n      \n      \n      \n      \n      \n      \n      \n         \n           Preprint \n         \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n        \n          The dissent score data set is available on Dataverse.    \n        \n      \n      \n      \n    \n  \n    \n      Generational Differences in Abortion Attitudes in the United States: Will Dobbs v. Jackson Become Starkly Counter-Majoritarian? \n      \n      \n      \n      \n        \n          Oct 13, 2023. Carlisle Rainey, Robert Jackson. Working paper.\n        \n      \n      \n      \n      \n      \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n         \n           Preprint \n         \n      \n      \n      \n      \n      \n         \n           Dataverse \n        \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n    \n      Generation Effects on Americans' Symbolic Ideology and Attitudes Toward the Economic Role of Government \n      \n      \n      \n      \n        \n          Aug 2, 2023. Robert Jackson, Carlisle Rainey. Working paper.\n        \n      \n      \n      \n      \n      \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n         \n           Preprint \n         \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n\n\n\nNo matching items"
  },
  {
    "objectID": "research/index.html#forthcoming",
    "href": "research/index.html#forthcoming",
    "title": "Papers",
    "section": "Forthcoming",
    "text": "Forthcoming\n\n\n\n\n\n\n\n  \n    \n      Use and Misuse of a Fast Approximation: Not a Criticism, but a Caution \n      \n      \n      \n      \n        \n          Feb 20, 2025. Carlisle Rainey. Forthcoming at Meta-Psychology.\n        \n      \n      \n      \n      \n      \n      \n\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n         \n           Preprint \n         \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n        \n          A peer-reviewed commentary on Replication value as a function of citation impact and sample size in Meta-Psychology    \n        \n      \n      \n      \n    \n  \n\n\n\nNo matching items"
  },
  {
    "objectID": "research/index.html#published-papers",
    "href": "research/index.html#published-papers",
    "title": "Papers",
    "section": "Published Papers",
    "text": "Published Papers\n\n\n\n\n\n\n\n  \n    \n      The Limits (and Strengths) of Single-Topic Experiments \n      \n      \n      \n      \n      \n      \n      \n        \n          2025. Scott Clifford, Carlisle Rainey. Political Analysis.\n        \n      \n      \n      \n\n      \n      \n      \n      \n      \n      \n      \n         \n           Open Access\n        \n      \n      \n      \n      \n      \n         \n           Preprint \n         \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n    \n      Data and Code Availability in Political Science Publications from 1995 to 2022 \n      \n      \n      \n      \n      \n      \n      \n        \n          2025. Carlisle Rainey, Harley Roe, Qing Wang, Hao Zhou. PS: Political Science and Politics.\n        \n      \n      \n      \n\n      \n      \n      \n      \n      \n      \n      \n         \n           Open Access\n        \n      \n      \n      \n      \n      \n         \n           Preprint \n         \n      \n      \n      \n      \n      \n         \n           Dataverse \n        \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n    \n      Estimators for Topic-Sampling Designs \n      \n      \n      \n      \n      \n      \n      \n        \n          2024. Scott Clifford, Carlisle Rainey. Political Analysis.\n        \n      \n      \n      \n\n      \n      \n      \n      \n      \n      \n      \n         \n           Open Access\n        \n      \n      \n      \n      \n      \n         \n           Preprint \n         \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n    \n      A Careful Consideration of CLARIFY: Simulation-Induced Bias in Point Estimates of Quantities of Interest \n      \n      \n      \n      \n      \n      \n      \n        \n          2024. Carlisle Rainey. Political Science Research and Methods.\n        \n      \n      \n      \n\n      \n         \n           Errata\n         \n      \n      \n      \n      \n      \n         \n           PDF \n         \n      \n      \n      \n         \n           Open Access\n        \n      \n      \n      \n      \n      \n      \n      \n      \n      \n         \n           Dataverse \n        \n      \n      \n      \n      \n      \n         \n           GitHub \n        \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n    \n      Generalizing Survey Experiments Using Topic Sampling: An Application to Party Cues \n      \n      \n      \n      \n      \n      \n      \n        \n          2024. Scott Clifford, Thomas Leeper, Carlisle Rainey. Political Behavior.\n        \n      \n      \n      \n\n      \n      \n      \n      \n      \n         \n           PDF \n         \n      \n      \n      \n      \n      \n         \n           Publisher's Version \n        \n      \n      \n      \n      \n      \n      \n      \n         \n           Dataverse \n        \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n    \n      Hypothesis Tests Under Separation \n      \n      \n      \n      \n      \n      \n      \n        \n          2024. Carlisle Rainey. Political Analysis.\n        \n      \n      \n      \n\n      \n      \n      \n      \n      \n      \n      \n         \n           Open Access\n        \n      \n      \n      \n      \n      \n         \n           Preprint \n         \n      \n      \n      \n      \n      \n         \n           Dataverse \n        \n      \n      \n      \n         \n           OSF\n        \n      \n      \n      \n      \n      \n      \n      \n      \n      \n        \n          See also Rainey (2016)    \n        \n      \n      \n      \n    \n  \n    \n      Estimating Logit Models with Small Samples \n      \n      \n      \n      \n      \n      \n      \n        \n          2021. Carlisle Rainey, Kelly McCaskey. Political Science Research and Methods.\n        \n      \n      \n      \n\n      \n      \n      \n      \n      \n         \n           PDF \n         \n      \n      \n      \n         \n           Open Access\n        \n      \n      \n      \n      \n      \n      \n      \n      \n      \n         \n           Dataverse \n        \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n    \n      When BLUE Is Not Best: Non-Normal Errors and the Linear Model \n      \n      \n      \n      \n      \n      \n      \n        \n          2020. Daniel Baissa, Carlisle Rainey. Political Science Research and Methods.\n        \n      \n      \n      \n\n      \n      \n      \n      \n      \n         \n           PDF \n         \n      \n      \n      \n      \n      \n         \n           Publisher's Version \n        \n      \n      \n      \n      \n      \n      \n      \n         \n           Dataverse \n        \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n    \n      Unreliable Inferences about Unobservable Processes: A Critique of Partial Observability Models \n      \n      \n      \n      \n      \n      \n      \n        \n          2018. Carlisle Rainey, Robert Jackson. Political Science Research and Methods.\n        \n      \n      \n      \n\n      \n      \n      \n      \n      \n         \n           PDF \n         \n      \n      \n      \n      \n      \n         \n           Publisher's Version \n        \n      \n      \n      \n      \n      \n      \n      \n         \n           Dataverse \n        \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n    \n      Transformation-Induced Bias: Unbiased Coefficients Do Not Imply Unbiased Quantities of Interest \n      \n      \n      \n      \n      \n      \n      \n        \n          2017. Carlisle Rainey. Political Analysis.\n        \n      \n      \n      \n\n      \n      \n      \n      \n      \n         \n           PDF \n         \n      \n      \n      \n      \n      \n         \n           Publisher's Version \n        \n      \n      \n      \n      \n      \n      \n      \n         \n           Dataverse \n        \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n    \n      Compression and Conditional Effects: A Product Term Is Essential When Using Logistic Regression to Test for Interaction \n      \n      \n      \n      \n      \n      \n      \n        \n          2016. Carlisle Rainey. Political Science Research and Methods.\n        \n      \n      \n      \n\n      \n      \n      \n      \n      \n         \n           PDF \n         \n      \n      \n      \n      \n      \n         \n           Publisher's Version \n        \n      \n      \n      \n      \n      \n         \n           Appendix \n        \n      \n      \n      \n         \n           Dataverse \n        \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n    \n      Dealing with Separation in Logistic Regression Models \n      \n      \n      \n      \n      \n      \n      \n        \n          2016. Carlisle Rainey. Political Analysis.\n        \n      \n      \n      \n\n      \n      \n      \n      \n      \n         \n           PDF \n         \n      \n      \n      \n      \n      \n         \n           Publisher's Version \n        \n      \n      \n      \n      \n      \n         \n           Appendix \n        \n      \n      \n      \n         \n           Dataverse \n        \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n    \n      Does District Magnitude Matter: The Case of Taiwan \n      \n      \n      \n      \n      \n      \n      \n        \n          2016. Carlisle Rainey. Electoral Studies.\n        \n      \n      \n      \n\n      \n      \n      \n      \n      \n         \n           PDF \n         \n      \n      \n      \n      \n      \n         \n           Publisher's Version \n        \n      \n      \n      \n      \n      \n      \n      \n         \n           Dataverse \n        \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n    \n      Strategic Mobilization: Why Proportional Representation Decreases Voter Mobilization \n      \n      \n      \n      \n      \n      \n      \n        \n          2015. Carlisle Rainey. Electoral Studies.\n        \n      \n      \n      \n\n      \n      \n      \n      \n      \n         \n           PDF \n         \n      \n      \n      \n      \n      \n         \n           Publisher's Version \n        \n      \n      \n      \n      \n      \n         \n           Appendix \n        \n      \n      \n      \n         \n           Dataverse \n        \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n    \n      Substantive Importance and the Veil of Statistical Significance \n      \n      \n      \n      \n      \n      \n      \n        \n          2015. Kelly McCaskey, Carlisle Rainey. Statistics, Politics, and Policy.\n        \n      \n      \n      \n\n      \n      \n      \n      \n      \n         \n           PDF \n         \n      \n      \n      \n      \n      \n         \n           Publisher's Version \n        \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n         \n           GitHub \n        \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n    \n      Moral Concerns and Policy Attitudes: Investigating the Influence of Elite Rhetoric \n      \n      \n      \n      \n      \n      \n      \n        \n          2015. Scott Clifford, Jennifer Jerit, Carlisle Rainey, Matt Motyl. Political Communication.\n        \n      \n      \n      \n\n      \n      \n      \n      \n      \n         \n           PDF \n         \n      \n      \n      \n      \n      \n         \n           Publisher's Version \n        \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n    \n      The Politics of Need: Examining Governors' Decisions to Oppose the 'Obamacare' Medicaid Expansion \n      \n      \n      \n      \n      \n      \n      \n        \n          2014. Charles Barrillleaux, Carlisle Rainey. State Politics and Policy Quarterly.\n        \n      \n      \n      \n\n      \n      \n      \n      \n      \n         \n           PDF \n         \n      \n      \n      \n      \n      \n         \n           Publisher's Version \n        \n      \n      \n      \n      \n      \n         \n           Appendix \n        \n      \n      \n      \n         \n           Dataverse \n        \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n    \n      The Question(s) of Political Knowledge \n      \n      \n      \n      \n      \n      \n      \n        \n          2014. Jason Barabas, Jennifer Jerit, William Pollock, Carlisle Rainey. American Political Science Review.\n        \n      \n      \n      \n\n      \n      \n      \n      \n      \n         \n           PDF \n         \n      \n      \n      \n      \n      \n         \n           Publisher's Version \n        \n      \n      \n      \n      \n      \n      \n      \n         \n           Dataverse \n        \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n    \n      Arguing for a Negligible Effect \n      \n      \n      \n      \n      \n      \n      \n        \n          2014. Carlisle Rainey. American Journal of Political Science.\n        \n      \n      \n      \n\n      \n      \n      \n      \n      \n         \n           PDF \n         \n      \n      \n      \n      \n      \n         \n           Publisher's Version \n        \n      \n      \n      \n      \n      \n         \n           Appendix \n        \n      \n      \n      \n         \n           Dataverse \n        \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/2024-03-02-topic-sampling/index.html",
    "href": "talks/2024-03-02-topic-sampling/index.html",
    "title": "Topic Sampling @ EPOVB 2024",
    "section": "",
    "text": "Slides are available via Dropbox here.\nThe talk will be an amalgamation of several recent papers, but focus on two in particular:\n\nClifford, Scott and Carlisle Rainey. “The Limits (and Strengths) of Single-Topic Experiments.” [Preprint]\nThorson, Emily and Carlisle Rainey. “The Policy Longevity Bonus: Pre-Analysis Plan.” [Pre-Analysis Plan]\n\nThe two other relevant papers are below.\n\nClifford, Scott, Thomas Leeper, and Carlisle Rainey. “Generalizing Survey Experiments Using Topic Sampling: An Application to Party Cues.” Forthcoming in Political Behavior. [Journal] [Ungated]\nClifford, Scott and Carlisle Rainey. “Estimators for Topic-Sampling Designs.” Conditionally accepted at Political Analysis. [Preprint]"
  },
  {
    "objectID": "talks/2024-10-01-power-rules-uga/index.html",
    "href": "talks/2024-10-01-power-rules-uga/index.html",
    "title": "Power Rules @ UGA",
    "section": "",
    "text": "You can find the paper here.\nThe GitHub repo for the paper is here.\nYou can find the iCloud version of the slides here and a pdf version of the slides here."
  },
  {
    "objectID": "talks/2024-10-01-power-rules-uga/index.html#paper-and-slides",
    "href": "talks/2024-10-01-power-rules-uga/index.html#paper-and-slides",
    "title": "Power Rules @ UGA",
    "section": "",
    "text": "You can find the paper here.\nThe GitHub repo for the paper is here.\nYou can find the iCloud version of the slides here and a pdf version of the slides here."
  },
  {
    "objectID": "talks/2024-10-01-power-rules-uga/index.html#other-things-from-me",
    "href": "talks/2024-10-01-power-rules-uga/index.html#other-things-from-me",
    "title": "Power Rules @ UGA",
    "section": "Other things from me",
    "text": "Other things from me\n\nYou can find several relevant blog posts on the topic here.\n\n“Arguing for a Negligible Effect” [PDF] describes how you can use an equivalence test to argue in favor of “no effect.”\n“Substantive Importance and the Veil of Statistical Significance” [PDF] makes a more general argument about how we should test claims.\n\nAnd the code to compute power is here:\n# mean and sd\ntrue_effect &lt;- 1.00\nse &lt;- 0.4\n\n# compute power\npnorm(1.64*se,             # want fraction above* 1.64 SE\n      mean = true_effect,  # mean of sampling distribution\n      sd = se,             # sd of sampling distribution\n      lower.tail = FALSE)  # fraction above, not below"
  },
  {
    "objectID": "talks/2024-10-01-power-rules-uga/index.html#other-relevant-papers",
    "href": "talks/2024-10-01-power-rules-uga/index.html#other-relevant-papers",
    "title": "Power Rules @ UGA",
    "section": "Other relevant papers",
    "text": "Other relevant papers\nHere are the articles worth reading if you want to learn more:\n\nArel-Bundock et al. (2022) for an argument that political scientists need to think harder about statistical power.\nBloom (1995) on the simple and effective concept of “minimum detectable effects.”\nLakens (2022) for a clear and complete description of how researchers can justify their sample size using power analysis and other arguments.\nBLAIR et al. (2019) for a comprehensive way of thinking about power along with research designs and their implications much more generally."
  },
  {
    "objectID": "talks/2025-01-11-power-rules-spsa/index.html",
    "href": "talks/2025-01-11-power-rules-spsa/index.html",
    "title": "Power Rules @ SPSA",
    "section": "",
    "text": "You can find the paper here.\nThe GitHub repo for the paper is here.\nYou can find a pdf version of the slides here and slides with transitions here."
  },
  {
    "objectID": "talks/2025-01-11-power-rules-spsa/index.html#paper-and-slides",
    "href": "talks/2025-01-11-power-rules-spsa/index.html#paper-and-slides",
    "title": "Power Rules @ SPSA",
    "section": "",
    "text": "You can find the paper here.\nThe GitHub repo for the paper is here.\nYou can find a pdf version of the slides here and slides with transitions here."
  },
  {
    "objectID": "talks/2025-01-11-power-rules-spsa/index.html#other-things-from-me",
    "href": "talks/2025-01-11-power-rules-spsa/index.html#other-things-from-me",
    "title": "Power Rules @ SPSA",
    "section": "Other things from me",
    "text": "Other things from me\n\nYou can find several relevant blog posts on the topic here.\n\n“Arguing for a Negligible Effect” [PDF] describes how you can use an equivalence test to argue in favor of “no effect.”\n“Substantive Importance and the Veil of Statistical Significance” [PDF] makes a more general argument about how we should test claims.\n\nAnd the code to compute power is here:\n# mean and sd\ntrue_effect &lt;- 1.00\nse &lt;- 0.4\n\n# compute power\npnorm(1.64*se,             # want fraction above* 1.64 SE\n      mean = true_effect,  # mean of sampling distribution\n      sd = se,             # sd of sampling distribution\n      lower.tail = FALSE)  # fraction above, not below"
  },
  {
    "objectID": "talks/2025-01-11-power-rules-spsa/index.html#other-relevant-papers",
    "href": "talks/2025-01-11-power-rules-spsa/index.html#other-relevant-papers",
    "title": "Power Rules @ SPSA",
    "section": "Other relevant papers",
    "text": "Other relevant papers\nHere are the articles worth reading if you want to learn more:\n\nArel-Bundock et al. (2022) for an argument that political scientists need to think harder about statistical power.\nBloom (1995) on the simple and effective concept of “minimum detectable effects.”\nLakens (2022) for a clear and complete description of how researchers can justify their sample size using power analysis and other arguments.\nBLAIR et al. (2019) for a comprehensive way of thinking about power along with research designs and their implications much more generally."
  },
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Teaching",
    "section": "",
    "text": "POS 5747: Advanced Quantitative Methods\nPOS 3713: Introduction to Political Science Research Methods (for RIBC)"
  },
  {
    "objectID": "teaching/index.html#current-classes",
    "href": "teaching/index.html#current-classes",
    "title": "Teaching",
    "section": "",
    "text": "POS 5747: Advanced Quantitative Methods\nPOS 3713: Introduction to Political Science Research Methods (for RIBC)"
  },
  {
    "objectID": "teaching/index.html#archived-classes",
    "href": "teaching/index.html#archived-classes",
    "title": "Teaching",
    "section": "Archived Classes",
    "text": "Archived Classes\n\nPOS 5737: Introduction to Data Analysis\nPOLS 209 at TAMU: Research Methods"
  }
]